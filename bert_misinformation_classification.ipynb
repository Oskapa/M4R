{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y38lbrea-wae",
        "outputId": "14a1b1f6-f638-4ae2-aeae-f98c5c83a49f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "!pip install -q -U \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbAh5hWc_eXp",
        "outputId": "1b316849-508a-410e-f016-d82d34843bb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m189.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q tf-models-official==2.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrQ-xhrZ-3nD",
        "outputId": "9810b5c3-8148-4e79-dd3f-d4b2546de8eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.8.4 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M78MSuEHO5Zi",
        "outputId": "fdfcb064-5b2f-4f66-c111-bbdf5893bc22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.8.4\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ees0ZHM7dAsp"
      },
      "source": [
        "We have first preprocessed the collected data using the python package tweet-preprocessor. This allows us to do a basic clean on the data where we remove hashtags, emojis and urls from the tweets. We then divide our data into a training set, a test set as well as a validation set. We will train the model weights using the training set while testing the loss and accuracy on the validation set after each epoch. This will allow us to select the correct number of epochs for which we should train our model to maximize accuracy without overfitting on the training set. We will as a last step also test our model on the test data to check whether we indeed get good results.\n",
        "\n",
        "We load the data using the tf.data API to allow for more efficient training.\n",
        "\n",
        "In order to make the training of our model as efficient as possible we will divide our data into batches so the the computer does not have to store all the data in memory at each epoch which would be expensive as the model will update the weights after each batch (see when and why to use bacthes tds). Furthermore, we will also apply pre-fetching to our data. Without pre-fetching, after every training step we have to wait for the next batch to be read before training. However, with prefetching, while we train on a certain batch, the next batch will start being read so that it is ready for training once the previous batch has been trained. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMOlf6zNGAsc"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# training and validation dataset\n",
        "\n",
        "train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "\n",
        "train_val_data.shuffle(buffer_size=len(list(train_val_data)))\n",
        "\n",
        "train_val_set_size = len(list(train_val_data))\n",
        "val_n = int(0.2*train_val_set_size)\n",
        "\n",
        "train_data = train_val_data.skip(val_n).batch(batch_size)\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_data = train_val_data.take(val_n).batch(batch_size)\n",
        "val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# test dataset\n",
        "\n",
        "test_data = tf.data.experimental.CsvDataset([\"test_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "test_data = test_data.batch(batch_size)\n",
        "test_data = test_data.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "chPE4xFAhOQc"
      },
      "source": [
        "As an initial model we will use the pretrained bert model small_bert/bert_en_uncased_L-6_H-128_A-2 which is a smaller bert. This will allow for shorter fine tuning time compared to larger models like the original BERT-Base. Pre-training means that the initial architecture was trained using a MLM (masked language model) on the content of Wikipedia and Bookscorpus. This is thus a general model for english that we will fine tune for our needs namely language classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weacutFqf-hB",
        "outputId": "f6ecc196-a518-4c3a-c9c9-97016981e432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ]
        }
      ],
      "source": [
        "bert_model_name = 'small_bert/bert_en_uncased_L-6_H-128_A-2'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3XFF0TQiRJIO"
      },
      "source": [
        "preprocess model to tokenize text which we then input into the bert model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6m1_cwagJyD"
      },
      "outputs": [],
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jkm0bIoFgUu3"
      },
      "outputs": [],
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LZFYigPKiwd4"
      },
      "source": [
        "To perform the classification task we will thus use as basis the pre-trained small BERT model to which we add a dropout layer which will help in our quest to avoid overfitting on the training data by randomly dropping data points out of training thus in essence creating slightly dissimilar datasets on which the model is trained. \n",
        "We will then also add a dense layer with only one unit meaning the output will be a classifier.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktkoHpCNgdAV"
      },
      "outputs": [],
      "source": [
        "def build_classifier_model(dropout):\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(dropout)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGceo1F1VMQJ"
      },
      "outputs": [],
      "source": [
        "def build_classifier_model_no_dropout():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  #net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAMbqh88gyrS"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP2P5kfcg3Nm"
      },
      "outputs": [],
      "source": [
        "steps_per_epoch = train_val_set_size - val_n\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xXOj6XKGpcRG"
      },
      "source": [
        "## Grid search for dropout"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3r-dgh6RdGUB"
      },
      "source": [
        "0.1 dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neIUm50nhS6N"
      },
      "outputs": [],
      "source": [
        "classifier_model = build_classifier_model()\n",
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3u3-GqjhmDe",
        "outputId": "dd9b90fa-a50b-44c1-91f3-f9cc6fe1337b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1\n",
            "Epoch 1/15\n",
            "229/229 [==============================] - 382s 2s/step - loss: 0.8502 - binary_accuracy: 0.5092 - val_loss: 0.7109 - val_binary_accuracy: 0.7131\n",
            "Epoch 2/15\n",
            "229/229 [==============================] - 373s 2s/step - loss: 0.6544 - binary_accuracy: 0.6703 - val_loss: 0.5247 - val_binary_accuracy: 0.6885\n",
            "Epoch 3/15\n",
            "229/229 [==============================] - 374s 2s/step - loss: 0.5090 - binary_accuracy: 0.7380 - val_loss: 0.4073 - val_binary_accuracy: 0.7721\n",
            "Epoch 4/15\n",
            "229/229 [==============================] - 364s 2s/step - loss: 0.4106 - binary_accuracy: 0.8049 - val_loss: 0.3217 - val_binary_accuracy: 0.8519\n",
            "Epoch 5/15\n",
            "229/229 [==============================] - 356s 2s/step - loss: 0.3347 - binary_accuracy: 0.8553 - val_loss: 0.2672 - val_binary_accuracy: 0.8820\n",
            "Epoch 6/15\n",
            "229/229 [==============================] - 359s 2s/step - loss: 0.2863 - binary_accuracy: 0.8825 - val_loss: 0.2295 - val_binary_accuracy: 0.9038\n",
            "Epoch 7/15\n",
            "229/229 [==============================] - 354s 2s/step - loss: 0.2399 - binary_accuracy: 0.9058 - val_loss: 0.1913 - val_binary_accuracy: 0.9224\n",
            "Epoch 8/15\n",
            "229/229 [==============================] - 354s 2s/step - loss: 0.2118 - binary_accuracy: 0.9201 - val_loss: 0.1624 - val_binary_accuracy: 0.9366\n",
            "Epoch 9/15\n",
            "229/229 [==============================] - 354s 2s/step - loss: 0.1753 - binary_accuracy: 0.9358 - val_loss: 0.1410 - val_binary_accuracy: 0.9503\n",
            "Epoch 10/15\n",
            "229/229 [==============================] - 353s 2s/step - loss: 0.1585 - binary_accuracy: 0.9436 - val_loss: 0.1232 - val_binary_accuracy: 0.9585\n",
            "Epoch 11/15\n",
            "229/229 [==============================] - 352s 2s/step - loss: 0.1425 - binary_accuracy: 0.9512 - val_loss: 0.1203 - val_binary_accuracy: 0.9607\n",
            "Epoch 12/15\n",
            "229/229 [==============================] - 355s 2s/step - loss: 0.1330 - binary_accuracy: 0.9557 - val_loss: 0.1154 - val_binary_accuracy: 0.9628\n",
            "Epoch 13/15\n",
            "229/229 [==============================] - 352s 2s/step - loss: 0.1205 - binary_accuracy: 0.9600 - val_loss: 0.1063 - val_binary_accuracy: 0.9661\n",
            "Epoch 14/15\n",
            "229/229 [==============================] - 353s 2s/step - loss: 0.1095 - binary_accuracy: 0.9645 - val_loss: 0.1052 - val_binary_accuracy: 0.9694\n",
            "Epoch 15/15\n",
            "229/229 [==============================] - 354s 2s/step - loss: 0.1024 - binary_accuracy: 0.9668 - val_loss: 0.1024 - val_binary_accuracy: 0.9699\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 184). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(x=train_data,\n",
        "                               validation_data=val_data,\n",
        "                               epochs=epochs)\n",
        "!mkdir saved_model\n",
        "classifier_model.save('saved_model/bert_misinf') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Df03m4bbj7H",
        "outputId": "a4e9b03d-c585-4d69-dab8-b9b891077c49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "72/72 [==============================] - 29s 402ms/step - loss: 0.1035 - binary_accuracy: 0.9703\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = classifier_model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "vmOsD_bnb80B",
        "outputId": "9095666d-0301-4c6b-d7e8-7b6fa0bb8766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'binary_accuracy', 'val_loss', 'val_binary_accuracy'])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6bf5584ee0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "\n",
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "# r is for \"solid red line\"\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2gcZES1qchqv"
      },
      "source": [
        "As we can see, the validation accuracy is always higher than those same metrics evaluated on the training set. This seems to indicate that our model is not overfitting and is in fact generalizing quite well. Maybe this is also because of dropout. The accuracy on the training, validation and test set after 5 epochs are 0.9266, 0.9497 and 0.9515 respectively. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "WGhSfJa0y_Ki",
        "outputId": "d5479ece-3379-46de-f1f2-8c2f5c32168e"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_4288e0d7-22e5-4125-a65d-599aa9d89cde\", \"bert_misinf\", 4096)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "files.download('/content/saved_model/bert_misinf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlJqJcTJa0uS"
      },
      "outputs": [],
      "source": [
        "# need compile=False cause otherwise will not load properly cause custom optimizer\n",
        "model_bert = tf.keras.models.load_model('saved_model/bert_misinf', compile=False)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnz6MEQcVXQY"
      },
      "outputs": [],
      "source": [
        "classifier_model_no_dropout = build_classifier_model_no_dropout()\n",
        "classifier_model_no_dropout.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbX8-1NVVjgB",
        "outputId": "1dab187d-9d27-4bfd-ff1f-9a62a494321a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1\n",
            "Epoch 1/15\n",
            "229/229 [==============================] - 378s 2s/step - loss: 0.8841 - binary_accuracy: 0.4264 - val_loss: 0.7533 - val_binary_accuracy: 0.6432\n",
            "Epoch 2/15\n",
            "229/229 [==============================] - 349s 2s/step - loss: 0.6763 - binary_accuracy: 0.6539 - val_loss: 0.5827 - val_binary_accuracy: 0.6727\n",
            "Epoch 3/15\n",
            "229/229 [==============================] - 348s 2s/step - loss: 0.5379 - binary_accuracy: 0.7010 - val_loss: 0.4644 - val_binary_accuracy: 0.7301\n",
            "Epoch 4/15\n",
            "229/229 [==============================] - 350s 2s/step - loss: 0.4400 - binary_accuracy: 0.7793 - val_loss: 0.3761 - val_binary_accuracy: 0.8230\n",
            "Epoch 5/15\n",
            "229/229 [==============================] - 346s 2s/step - loss: 0.3681 - binary_accuracy: 0.8398 - val_loss: 0.3061 - val_binary_accuracy: 0.8710\n",
            "Epoch 6/15\n",
            "229/229 [==============================] - 353s 2s/step - loss: 0.3100 - binary_accuracy: 0.8743 - val_loss: 0.2564 - val_binary_accuracy: 0.8973\n",
            "Epoch 7/15\n",
            "229/229 [==============================] - 352s 2s/step - loss: 0.2649 - binary_accuracy: 0.8944 - val_loss: 0.2175 - val_binary_accuracy: 0.9148\n",
            "Epoch 8/15\n",
            "229/229 [==============================] - 356s 2s/step - loss: 0.2275 - binary_accuracy: 0.9152 - val_loss: 0.1803 - val_binary_accuracy: 0.9306\n",
            "Epoch 9/15\n",
            "229/229 [==============================] - 351s 2s/step - loss: 0.1989 - binary_accuracy: 0.9268 - val_loss: 0.1491 - val_binary_accuracy: 0.9475\n",
            "Epoch 10/15\n",
            "229/229 [==============================] - 352s 2s/step - loss: 0.1732 - binary_accuracy: 0.9384 - val_loss: 0.1400 - val_binary_accuracy: 0.9557\n",
            "Epoch 11/15\n",
            "229/229 [==============================] - 374s 2s/step - loss: 0.1553 - binary_accuracy: 0.9463 - val_loss: 0.1329 - val_binary_accuracy: 0.9579\n",
            "Epoch 12/15\n",
            "229/229 [==============================] - 380s 2s/step - loss: 0.1460 - binary_accuracy: 0.9471 - val_loss: 0.1251 - val_binary_accuracy: 0.9596\n",
            "Epoch 13/15\n",
            "229/229 [==============================] - 383s 2s/step - loss: 0.1252 - binary_accuracy: 0.9582 - val_loss: 0.1102 - val_binary_accuracy: 0.9650\n",
            "Epoch 14/15\n",
            "229/229 [==============================] - 378s 2s/step - loss: 0.1160 - binary_accuracy: 0.9609 - val_loss: 0.1040 - val_binary_accuracy: 0.9650\n",
            "Epoch 15/15\n",
            "229/229 [==============================] - 366s 2s/step - loss: 0.1052 - binary_accuracy: 0.9657 - val_loss: 0.1071 - val_binary_accuracy: 0.9678\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 184). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model_no_dropout.fit(x=train_data, validation_data=val_data, epochs=epochs)\n",
        "!mkdir saved_model_no_dropout\n",
        "classifier_model.save('saved_model/bert_misinf_no_dropout') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "6FR4jh0DVwhB",
        "outputId": "6fda63e4-e85c-410f-8709-405820adbda9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'binary_accuracy', 'val_loss', 'val_binary_accuracy'])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6bdfef8550>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "\n",
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "# r is for \"solid red line\"\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iOvvBN_79Tv0"
      },
      "source": [
        " **FOR CV NEED NEW TRAIN SET AS VALIDATION SET NO LONGER NEEDED**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C4SvJu3B82SZ"
      },
      "source": [
        "We will then apply k-fold cross validation which will allow us to get a better and more realistic estimate of how our model will perform when applied to unseen data. K-fold cross validation works by splitting our training dataset into K groups then at each epoch, we will take a group to hold out as a validation set, train the model on the remaining training data and evaluate on the validation set. We will do this for each of the K-folds and then average the performance obtained on each validation fold. We will initially set the number of folds to 5 to keep the training time low."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J-WNVv7FbKVT"
      },
      "source": [
        "Next steps:\n",
        "- somehow need to find out if we should use dropout maybe find optimal epochs then grid search for dropout then verify using cross-validation\n",
        "- train up to 15 epochs to find optimal number of epochs for training from looking at graph and seeing where validation accuracy gets lower than test accuracy, ie \n",
        "- train CV with/without dropout for 5 folds to evaluate model\n",
        "(say source where we take the value of 5 folds) \n",
        "- go upto 20 epochs\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lCPGyG13idzV"
      },
      "source": [
        "We thus have chosen a BERT model to classify the Tweets as from recent literature BERT has shown to the most accurate NLP model. To this base bert architercture, we need to add a couple extra layers that will allow us to fine tune BERT for the classification problem at hand. Thus the most important added layer is a dense layer that will be used for the classification. We can also add a dropout layer. This is where we will apply hyperparameter tuning techniques that will help us to select the ideal value of the dropout as well as the ideal number of epochs that our model should be trained for in order to get the highest accuracy (see hyperparameter tuning methods ie grid search). We can evaluate our resulting models using k-fold cross validation. For k-fold cross validation, we will need to divide our dataset into a test set and another set used for training that we will divide into k non-intersecting folds. We will then train our model k times, at each iteration, we will keep one fold as a validation set while training the data on the k-1 remaining folds. We can then use the average accuracy over all trained models to evaluate the performance of our model. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "af8Eo9sMlzeu"
      },
      "source": [
        "Explanation from https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/:\n",
        "The k-fold cross-validation procedure divides a limited dataset into k non-overlapping folds. Each of the k folds is given an opportunity to be used as a held back test set whilst all other folds collectively are used as a training dataset. A total of k models are fit and evaluated on the k holdout test sets and the mean performance is reported."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KbBcsjb_lzUW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9-yGzMleEst"
      },
      "outputs": [],
      "source": [
        "split_size = int(0.2*train_val_set_size)\n",
        "\n",
        "cv_1 = train_val_data.take(split_size).batch(batch_size)\n",
        "cv_1 = cv_1.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "m1 = train_val_data.skip(split_size)\n",
        "cv_2 = m1.take(split_size).batch(batch_size)\n",
        "cv_2 = cv_2.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "m2 = m1.skip(split_size)\n",
        "cv_3 = m2.take(split_size).batch(batch_size)\n",
        "cv_3 = cv_3.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "m3 = m2.skip(split_size)\n",
        "cv_4 = m3.take(split_size).batch(batch_size)\n",
        "cv_4 = cv_4.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "cv_5 = m3.skip(split_size).batch(batch_size)\n",
        "cv_5 = cv_5.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GS-kmTALtzs",
        "outputId": "0cc6c3a9-a725-427a-e3c9-f1e5e216e032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "232/232 [==============================] - 365s 2s/step - loss: 0.7240 - binary_accuracy: 0.6710 - val_loss: 0.6148 - val_binary_accuracy: 0.6710\n",
            "Epoch 2/15\n",
            "232/232 [==============================] - 351s 2s/step - loss: 0.5707 - binary_accuracy: 0.6740 - val_loss: 0.4801 - val_binary_accuracy: 0.6902\n",
            "Epoch 3/15\n",
            "232/232 [==============================] - 348s 2s/step - loss: 0.4611 - binary_accuracy: 0.7328 - val_loss: 0.3727 - val_binary_accuracy: 0.7984\n",
            "Epoch 4/15\n",
            "232/232 [==============================] - 348s 2s/step - loss: 0.3756 - binary_accuracy: 0.8138 - val_loss: 0.3094 - val_binary_accuracy: 0.8443\n",
            "Epoch 5/15\n",
            "232/232 [==============================] - 348s 2s/step - loss: 0.3226 - binary_accuracy: 0.8541 - val_loss: 0.2692 - val_binary_accuracy: 0.8770\n",
            "Epoch 6/15\n",
            "232/232 [==============================] - 345s 1s/step - loss: 0.2774 - binary_accuracy: 0.8868 - val_loss: 0.2354 - val_binary_accuracy: 0.8945\n",
            "Epoch 7/15\n",
            "232/232 [==============================] - 347s 1s/step - loss: 0.2464 - binary_accuracy: 0.8978 - val_loss: 0.2053 - val_binary_accuracy: 0.9148\n",
            "Epoch 8/15\n",
            "232/232 [==============================] - 349s 2s/step - loss: 0.2165 - binary_accuracy: 0.9152 - val_loss: 0.1895 - val_binary_accuracy: 0.9208\n",
            "Epoch 9/15\n",
            "232/232 [==============================] - 348s 1s/step - loss: 0.1939 - binary_accuracy: 0.9250 - val_loss: 0.1633 - val_binary_accuracy: 0.9344\n",
            "Epoch 10/15\n",
            "232/232 [==============================] - 350s 2s/step - loss: 0.1728 - binary_accuracy: 0.9340 - val_loss: 0.1429 - val_binary_accuracy: 0.9459\n",
            "Epoch 11/15\n",
            "232/232 [==============================] - 347s 1s/step - loss: 0.1506 - binary_accuracy: 0.9435 - val_loss: 0.1293 - val_binary_accuracy: 0.9514\n",
            "Epoch 12/15\n",
            "232/232 [==============================] - 348s 2s/step - loss: 0.1315 - binary_accuracy: 0.9566 - val_loss: 0.1215 - val_binary_accuracy: 0.9585\n",
            "Epoch 13/15\n",
            "232/232 [==============================] - 349s 2s/step - loss: 0.1261 - binary_accuracy: 0.9559 - val_loss: 0.1111 - val_binary_accuracy: 0.9634\n",
            "Epoch 14/15\n",
            "232/232 [==============================] - 356s 2s/step - loss: 0.1051 - binary_accuracy: 0.9638 - val_loss: 0.1184 - val_binary_accuracy: 0.9645\n",
            "Epoch 15/15\n",
            "232/232 [==============================] - 356s 2s/step - loss: 0.1035 - binary_accuracy: 0.9650 - val_loss: 0.1115 - val_binary_accuracy: 0.9683\n",
            "Epoch 1/15\n",
            "232/232 [==============================] - 366s 2s/step - loss: 0.2604 - binary_accuracy: 0.9036 - val_loss: 0.1599 - val_binary_accuracy: 0.9437\n",
            "Epoch 2/15\n",
            "232/232 [==============================] - 348s 1s/step - loss: 0.1311 - binary_accuracy: 0.9529 - val_loss: 0.1261 - val_binary_accuracy: 0.9585\n",
            "Epoch 3/15\n",
            "232/232 [==============================] - 346s 1s/step - loss: 0.1080 - binary_accuracy: 0.9620 - val_loss: 0.1227 - val_binary_accuracy: 0.9628\n",
            "Epoch 4/15\n",
            "232/232 [==============================] - 350s 2s/step - loss: 0.0904 - binary_accuracy: 0.9698 - val_loss: 0.1324 - val_binary_accuracy: 0.9634\n",
            "Epoch 5/15\n",
            "232/232 [==============================] - 347s 1s/step - loss: 0.0853 - binary_accuracy: 0.9727 - val_loss: 0.1470 - val_binary_accuracy: 0.9634\n",
            "Epoch 6/15\n",
            "232/232 [==============================] - 350s 2s/step - loss: 0.0705 - binary_accuracy: 0.9766 - val_loss: 0.1264 - val_binary_accuracy: 0.9672\n",
            "Epoch 7/15\n",
            "232/232 [==============================] - 346s 1s/step - loss: 0.0725 - binary_accuracy: 0.9780 - val_loss: 0.1500 - val_binary_accuracy: 0.9639\n",
            "Epoch 8/15\n",
            "232/232 [==============================] - 351s 2s/step - loss: 0.0619 - binary_accuracy: 0.9813 - val_loss: 0.1412 - val_binary_accuracy: 0.9656\n",
            "Epoch 9/15\n",
            "232/232 [==============================] - 349s 2s/step - loss: 0.0577 - binary_accuracy: 0.9828 - val_loss: 0.1407 - val_binary_accuracy: 0.9667\n",
            "Epoch 10/15\n",
            "232/232 [==============================] - 345s 1s/step - loss: 0.0538 - binary_accuracy: 0.9840 - val_loss: 0.1446 - val_binary_accuracy: 0.9672\n",
            "Epoch 11/15\n",
            "232/232 [==============================] - 347s 1s/step - loss: 0.0473 - binary_accuracy: 0.9870 - val_loss: 0.1668 - val_binary_accuracy: 0.9645\n",
            "Epoch 12/15\n",
            "232/232 [==============================] - 347s 1s/step - loss: 0.0495 - binary_accuracy: 0.9854 - val_loss: 0.1699 - val_binary_accuracy: 0.9645\n",
            "Epoch 13/15\n",
            "232/232 [==============================] - 348s 2s/step - loss: 0.0419 - binary_accuracy: 0.9884 - val_loss: 0.1568 - val_binary_accuracy: 0.9678\n",
            "Epoch 14/15\n",
            "232/232 [==============================] - 346s 1s/step - loss: 0.0356 - binary_accuracy: 0.9902 - val_loss: 0.1647 - val_binary_accuracy: 0.9694\n",
            "Epoch 15/15\n",
            "232/232 [==============================] - 348s 1s/step - loss: 0.0375 - binary_accuracy: 0.9896 - val_loss: 0.1712 - val_binary_accuracy: 0.9672\n",
            "Epoch 1/15\n",
            "232/232 [==============================] - 381s 2s/step - loss: 0.2091 - binary_accuracy: 0.9248 - val_loss: 0.1193 - val_binary_accuracy: 0.9623\n",
            "Epoch 2/15\n",
            "232/232 [==============================] - 351s 2s/step - loss: 0.1004 - binary_accuracy: 0.9669 - val_loss: 0.0944 - val_binary_accuracy: 0.9705\n",
            "Epoch 3/15\n",
            "232/232 [==============================] - 351s 2s/step - loss: 0.0847 - binary_accuracy: 0.9731 - val_loss: 0.1015 - val_binary_accuracy: 0.9721\n",
            "Epoch 4/15\n",
            "232/232 [==============================] - 347s 1s/step - loss: 0.0717 - binary_accuracy: 0.9787 - val_loss: 0.1104 - val_binary_accuracy: 0.9699\n",
            "Epoch 5/15\n",
            "232/232 [==============================] - 352s 2s/step - loss: 0.0594 - binary_accuracy: 0.9809 - val_loss: 0.1126 - val_binary_accuracy: 0.9749\n",
            "Epoch 6/15\n",
            "232/232 [==============================] - 349s 2s/step - loss: 0.0539 - binary_accuracy: 0.9835 - val_loss: 0.1206 - val_binary_accuracy: 0.9732\n",
            "Epoch 7/15\n",
            "232/232 [==============================] - 351s 2s/step - loss: 0.0440 - binary_accuracy: 0.9874 - val_loss: 0.1329 - val_binary_accuracy: 0.9727\n",
            "Epoch 8/15\n",
            "232/232 [==============================] - 353s 2s/step - loss: 0.0389 - binary_accuracy: 0.9902 - val_loss: 0.1363 - val_binary_accuracy: 0.9732\n",
            "Epoch 9/15\n",
            "232/232 [==============================] - 350s 2s/step - loss: 0.0371 - binary_accuracy: 0.9899 - val_loss: 0.1421 - val_binary_accuracy: 0.9716\n",
            "Epoch 10/15\n",
            "232/232 [==============================] - 347s 1s/step - loss: 0.0338 - binary_accuracy: 0.9910 - val_loss: 0.1453 - val_binary_accuracy: 0.9716\n",
            "Epoch 11/15\n",
            "232/232 [==============================] - 349s 2s/step - loss: 0.0280 - binary_accuracy: 0.9919 - val_loss: 0.1497 - val_binary_accuracy: 0.9710\n",
            "Epoch 12/15\n",
            "232/232 [==============================] - 347s 1s/step - loss: 0.0232 - binary_accuracy: 0.9941 - val_loss: 0.1696 - val_binary_accuracy: 0.9699\n",
            "Epoch 13/15\n",
            "232/232 [==============================] - 359s 2s/step - loss: 0.0246 - binary_accuracy: 0.9926 - val_loss: 0.1643 - val_binary_accuracy: 0.9727\n",
            "Epoch 14/15\n",
            "232/232 [==============================] - 351s 2s/step - loss: 0.0195 - binary_accuracy: 0.9948 - val_loss: 0.1685 - val_binary_accuracy: 0.9727\n",
            "Epoch 15/15\n",
            "232/232 [==============================] - 343s 1s/step - loss: 0.0171 - binary_accuracy: 0.9959 - val_loss: 0.1666 - val_binary_accuracy: 0.9738\n",
            "Epoch 1/15\n",
            "232/232 [==============================] - 351s 1s/step - loss: 0.2059 - binary_accuracy: 0.9297 - val_loss: 0.1167 - val_binary_accuracy: 0.9656\n",
            "Epoch 2/15\n",
            "232/232 [==============================] - 341s 1s/step - loss: 0.0994 - binary_accuracy: 0.9674 - val_loss: 0.1191 - val_binary_accuracy: 0.9689\n",
            "Epoch 3/15\n",
            "232/232 [==============================] - 334s 1s/step - loss: 0.0783 - binary_accuracy: 0.9751 - val_loss: 0.1367 - val_binary_accuracy: 0.9672\n",
            "Epoch 4/15\n",
            "232/232 [==============================] - 337s 1s/step - loss: 0.0683 - binary_accuracy: 0.9799 - val_loss: 0.1210 - val_binary_accuracy: 0.9705\n",
            "Epoch 5/15\n",
            "232/232 [==============================] - 340s 1s/step - loss: 0.0503 - binary_accuracy: 0.9851 - val_loss: 0.1469 - val_binary_accuracy: 0.9694\n",
            "Epoch 6/15\n",
            "232/232 [==============================] - 337s 1s/step - loss: 0.0454 - binary_accuracy: 0.9869 - val_loss: 0.1477 - val_binary_accuracy: 0.9683\n",
            "Epoch 7/15\n",
            "232/232 [==============================] - 332s 1s/step - loss: 0.0430 - binary_accuracy: 0.9885 - val_loss: 0.1407 - val_binary_accuracy: 0.9710\n",
            "Epoch 8/15\n",
            "232/232 [==============================] - 342s 1s/step - loss: 0.0356 - binary_accuracy: 0.9902 - val_loss: 0.1331 - val_binary_accuracy: 0.9710\n",
            "Epoch 9/15\n",
            "232/232 [==============================] - 340s 1s/step - loss: 0.0310 - binary_accuracy: 0.9918 - val_loss: 0.1490 - val_binary_accuracy: 0.9699\n",
            "Epoch 10/15\n",
            "232/232 [==============================] - 335s 1s/step - loss: 0.0275 - binary_accuracy: 0.9934 - val_loss: 0.1604 - val_binary_accuracy: 0.9694\n",
            "Epoch 11/15\n",
            "232/232 [==============================] - 338s 1s/step - loss: 0.0292 - binary_accuracy: 0.9914 - val_loss: 0.1485 - val_binary_accuracy: 0.9710\n",
            "Epoch 12/15\n",
            "232/232 [==============================] - 338s 1s/step - loss: 0.0228 - binary_accuracy: 0.9943 - val_loss: 0.1503 - val_binary_accuracy: 0.9689\n",
            "Epoch 13/15\n",
            "148/232 [==================>...........] - ETA: 1:53 - loss: 0.0185 - binary_accuracy: 0.9949"
          ]
        }
      ],
      "source": [
        "acc_fold = []\n",
        "val_acc_fold = []\n",
        "loss_fold = []\n",
        "val_loss_fold = []\n",
        "\n",
        "# fold 1\n",
        "classifier_model_cv1 = build_classifier_model_no_dropout()\n",
        "classifier_model_cv1.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv1 = classifier_model_cv1.fit(x=cv_2.concatenate(cv_3).concatenate(cv_4).concatenate(cv_5),\n",
        "                               validation_data=cv_1,\n",
        "                               epochs=epochs)\n",
        "history_dict_cv1 = history_cv1.history\n",
        "acc_fold.append(history_dict_cv1['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv1['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv1['loss'])\n",
        "val_loss_fold.append(history_dict_cv1['val_loss'])\n",
        "\n",
        "# fold 2\n",
        "classifier_model_cv2 = build_classifier_model_no_dropout()\n",
        "classifier_model_cv2.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv2 = classifier_model_cv2.fit(x=cv_1.concatenate(cv_3).concatenate(cv_4).concatenate(cv_5),\n",
        "                               validation_data=cv_2,\n",
        "                               epochs=epochs)\n",
        "history_dict_cv2 = history_cv2.history\n",
        "acc_fold.append(history_dict_cv2['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv2['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv2['loss'])\n",
        "val_loss_fold.append(history_dict_cv2['val_loss'])\n",
        "\n",
        "#fold 3\n",
        "classifier_model_cv3 = build_classifier_model_no_dropout()\n",
        "classifier_model_cv3.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv3 = classifier_model_cv3.fit(x=cv_2.concatenate(cv_1).concatenate(cv_4).concatenate(cv_5),\n",
        "                               validation_data=cv_3,\n",
        "                               epochs=epochs)\n",
        "history_dict_cv3 = history_cv3.history\n",
        "acc_fold.append(history_dict_cv3['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv3['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv3['loss'])\n",
        "val_loss_fold.append(history_dict_cv3['val_loss'])\n",
        "\n",
        "# fold 4\n",
        "classifier_model_cv4 = build_classifier_model_no_dropout()\n",
        "classifier_model_cv4.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv4 = classifier_model_cv4.fit(x=cv_2.concatenate(cv_3).concatenate(cv_1).concatenate(cv_5),\n",
        "                               validation_data=cv_4,\n",
        "                               epochs=epochs)\n",
        "history_dict_cv4 = history_cv4.history\n",
        "acc_fold.append(history_dict_cv4['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv4['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv4['loss'])\n",
        "val_loss_fold.append(history_dict_cv4['val_loss'])\n",
        "\n",
        "#fold 5\n",
        "classifier_model_cv5 = build_classifier_model_no_dropout()\n",
        "classifier_model_cv5.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv5 = classifier_model_cv5.fit(x=cv_2.concatenate(cv_3).concatenate(cv_4).concatenate(cv_1),\n",
        "                               validation_data=cv_5,\n",
        "                               epochs=epochs)\n",
        "history_dict_cv5 = history_cv5.history\n",
        "acc_fold.append(history_dict_cv5['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv5['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv5['loss'])\n",
        "val_loss_fold.append(history_dict_cv5['val_loss'])\n",
        "\n",
        "# print\n",
        "print(acc_fold)\n",
        "print(val_acc_fold)\n",
        "print(loss_fold)\n",
        "print(val_loss_fold)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RIvK-LVs49tf"
      },
      "source": [
        "We now use grid search to find the optimal dropout value while training for 15 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "id": "xi-WUlOZ3Q4n",
        "outputId": "49004530-af08-4d86-95d7-857a97a65db2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current dropout is 0\n",
            "Epoch 1/20\n",
            "229/229 [==============================] - 469s 2s/step - loss: 0.7490 - binary_accuracy: 0.6146 - val_loss: 0.6863 - val_binary_accuracy: 0.6798\n",
            "Epoch 2/20\n",
            "229/229 [==============================] - 421s 2s/step - loss: 0.6283 - binary_accuracy: 0.6786 - val_loss: 0.5486 - val_binary_accuracy: 0.6869\n",
            "Epoch 3/20\n",
            "229/229 [==============================] - 422s 2s/step - loss: 0.5167 - binary_accuracy: 0.7103 - val_loss: 0.4321 - val_binary_accuracy: 0.7459\n",
            "Epoch 4/20\n",
            "229/229 [==============================] - 421s 2s/step - loss: 0.4178 - binary_accuracy: 0.7881 - val_loss: 0.3424 - val_binary_accuracy: 0.8344\n",
            "Epoch 5/20\n",
            "229/229 [==============================] - 428s 2s/step - loss: 0.3394 - binary_accuracy: 0.8507 - val_loss: 0.2795 - val_binary_accuracy: 0.8803\n",
            "Epoch 6/20\n",
            "229/229 [==============================] - 421s 2s/step - loss: 0.2898 - binary_accuracy: 0.8827 - val_loss: 0.2356 - val_binary_accuracy: 0.9055\n",
            "Epoch 7/20\n",
            "229/229 [==============================] - 421s 2s/step - loss: 0.2457 - binary_accuracy: 0.9012 - val_loss: 0.1959 - val_binary_accuracy: 0.9246\n",
            "Epoch 8/20\n",
            "229/229 [==============================] - 432s 2s/step - loss: 0.2133 - binary_accuracy: 0.9165 - val_loss: 0.1651 - val_binary_accuracy: 0.9393\n",
            "Epoch 9/20\n",
            "229/229 [==============================] - 435s 2s/step - loss: 0.1778 - binary_accuracy: 0.9336 - val_loss: 0.1443 - val_binary_accuracy: 0.9525\n",
            "Epoch 10/20\n",
            "229/229 [==============================] - 428s 2s/step - loss: 0.1509 - binary_accuracy: 0.9458 - val_loss: 0.1277 - val_binary_accuracy: 0.9579\n",
            "Epoch 11/20\n",
            "229/229 [==============================] - 428s 2s/step - loss: 0.1370 - binary_accuracy: 0.9518 - val_loss: 0.1197 - val_binary_accuracy: 0.9607\n",
            "Epoch 12/20\n",
            "229/229 [==============================] - 423s 2s/step - loss: 0.1239 - binary_accuracy: 0.9568 - val_loss: 0.1143 - val_binary_accuracy: 0.9628\n",
            "Epoch 13/20\n",
            "229/229 [==============================] - 441s 2s/step - loss: 0.1133 - binary_accuracy: 0.9619 - val_loss: 0.1087 - val_binary_accuracy: 0.9667\n",
            "Epoch 14/20\n",
            "229/229 [==============================] - 432s 2s/step - loss: 0.1061 - binary_accuracy: 0.9671 - val_loss: 0.1053 - val_binary_accuracy: 0.9678\n",
            "Epoch 15/20\n",
            "229/229 [==============================] - 431s 2s/step - loss: 0.0971 - binary_accuracy: 0.9680 - val_loss: 0.1036 - val_binary_accuracy: 0.9716\n",
            "Epoch 16/20\n",
            "229/229 [==============================] - 448s 2s/step - loss: 0.0913 - binary_accuracy: 0.9702 - val_loss: 0.1043 - val_binary_accuracy: 0.9699\n",
            "Epoch 17/20\n",
            "229/229 [==============================] - 423s 2s/step - loss: 0.0904 - binary_accuracy: 0.9697 - val_loss: 0.1001 - val_binary_accuracy: 0.9721\n",
            "Epoch 18/20\n",
            "229/229 [==============================] - 425s 2s/step - loss: 0.0799 - binary_accuracy: 0.9743 - val_loss: 0.1065 - val_binary_accuracy: 0.9716\n",
            "Epoch 19/20\n",
            "229/229 [==============================] - 431s 2s/step - loss: 0.0748 - binary_accuracy: 0.9751 - val_loss: 0.1034 - val_binary_accuracy: 0.9727\n",
            "Epoch 20/20\n",
            "229/229 [==============================] - 433s 2s/step - loss: 0.0717 - binary_accuracy: 0.9776 - val_loss: 0.1041 - val_binary_accuracy: 0.9732\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-3d4ada69422a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                           metrics=metrics)\n\u001b[1;32m     15\u001b[0m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'binary_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history_dict' is not defined"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# training and validation dataset\n",
        "\n",
        "train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "\n",
        "train_val_data.shuffle(buffer_size=len(list(train_val_data)))\n",
        "\n",
        "train_val_set_size = len(list(train_val_data))\n",
        "val_n = int(0.2*train_val_set_size)\n",
        "\n",
        "train_data = train_val_data.skip(val_n).batch(batch_size)\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_data = train_val_data.take(val_n).batch(batch_size)\n",
        "val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# test dataset\n",
        "\n",
        "test_data = tf.data.experimental.CsvDataset([\"test_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "test_data = test_data.batch(batch_size)\n",
        "test_data = test_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "epochs = 25\n",
        "list_dropouts = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "\n",
        "for dropout in list_dropouts:\n",
        "  print(f\"Current dropout is {dropout}\")\n",
        "  classifier = build_classifier_model(dropout)\n",
        "  classifier.compile(optimizer=optimizer,\n",
        "                          loss=loss,\n",
        "                          metrics=metrics)\n",
        "  history = classifier.fit(x=train_data, validation_data=val_data, epochs=epochs)\n",
        "  history_dict=history.history\n",
        "  print(f\"For dropout {dropout}\")\n",
        "  print(history_dict['loss'])\n",
        "  print(history_dict['val_loss'])\n",
        "  print(history_dict['binary_accuracy'])\n",
        "  print(history_dict['val_binary_accuracy'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2D3OCMtq8dXn"
      },
      "outputs": [],
      "source": [
        "print(train_loss)\n",
        "print(val_loss)\n",
        "print(train_accuracies)\n",
        "print(val_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPIt7QrvdODX",
        "outputId": "7ce9990a-55ea-42ff-e0d2-a86e19cf8fbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "229/229 [==============================] - 370s 2s/step - loss: 0.7765 - binary_accuracy: 0.5682 - val_loss: 0.6820 - val_binary_accuracy: 0.6885\n",
            "Epoch 2/15\n",
            "229/229 [==============================] - 356s 2s/step - loss: 0.6396 - binary_accuracy: 0.6764 - val_loss: 0.5261 - val_binary_accuracy: 0.7027\n",
            "Epoch 3/15\n",
            "229/229 [==============================] - 351s 2s/step - loss: 0.5227 - binary_accuracy: 0.7275 - val_loss: 0.4291 - val_binary_accuracy: 0.7689\n",
            "Epoch 4/15\n",
            "229/229 [==============================] - 352s 2s/step - loss: 0.4353 - binary_accuracy: 0.7887 - val_loss: 0.3547 - val_binary_accuracy: 0.8301\n",
            "Epoch 5/15\n",
            "229/229 [==============================] - 352s 2s/step - loss: 0.3624 - binary_accuracy: 0.8433 - val_loss: 0.2874 - val_binary_accuracy: 0.8770\n",
            "Epoch 6/15\n",
            "229/229 [==============================] - 354s 2s/step - loss: 0.3055 - binary_accuracy: 0.8735 - val_loss: 0.2416 - val_binary_accuracy: 0.9005\n",
            "Epoch 7/15\n",
            "229/229 [==============================] - 350s 2s/step - loss: 0.2551 - binary_accuracy: 0.8991 - val_loss: 0.2020 - val_binary_accuracy: 0.9142\n",
            "Epoch 8/15\n",
            "229/229 [==============================] - 346s 2s/step - loss: 0.2206 - binary_accuracy: 0.9165 - val_loss: 0.1647 - val_binary_accuracy: 0.9372\n",
            "Epoch 9/15\n",
            "229/229 [==============================] - 346s 2s/step - loss: 0.1889 - binary_accuracy: 0.9303 - val_loss: 0.1482 - val_binary_accuracy: 0.9475\n",
            "Epoch 10/15\n",
            "229/229 [==============================] - 349s 2s/step - loss: 0.1701 - binary_accuracy: 0.9411 - val_loss: 0.1375 - val_binary_accuracy: 0.9536\n",
            "Epoch 11/15\n",
            "229/229 [==============================] - 356s 2s/step - loss: 0.1467 - binary_accuracy: 0.9481 - val_loss: 0.1170 - val_binary_accuracy: 0.9601\n",
            "Epoch 12/15\n",
            "229/229 [==============================] - 352s 2s/step - loss: 0.1297 - binary_accuracy: 0.9563 - val_loss: 0.1097 - val_binary_accuracy: 0.9628\n",
            "Epoch 13/15\n",
            "229/229 [==============================] - 350s 2s/step - loss: 0.1148 - binary_accuracy: 0.9613 - val_loss: 0.1006 - val_binary_accuracy: 0.9667\n",
            "Epoch 14/15\n",
            "229/229 [==============================] - 352s 2s/step - loss: 0.1098 - binary_accuracy: 0.9645 - val_loss: 0.1008 - val_binary_accuracy: 0.9689\n",
            "Epoch 15/15\n",
            "229/229 [==============================] - 358s 2s/step - loss: 0.1014 - binary_accuracy: 0.9686 - val_loss: 0.1057 - val_binary_accuracy: 0.9705\n"
          ]
        }
      ],
      "source": [
        "classifier_drop_20 = build_classifier_model(0.2)\n",
        "classifier_drop_20.compile(optimizer=optimizer,\n",
        "                        loss=loss,\n",
        "                        metrics=metrics)\n",
        "history_drop_20 = classifier_drop_20.fit(x=train_data, validation_data=val_data, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LBtDYUyBDrF"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# training and validation dataset\n",
        "\n",
        "train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "\n",
        "train_val_data.shuffle(buffer_size=len(list(train_val_data)))\n",
        "\n",
        "train_val_set_size = len(list(train_val_data))\n",
        "val_n = int(0.2*train_val_set_size)\n",
        "\n",
        "train_data = train_val_data.skip(val_n).batch(batch_size)\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_data = train_val_data.take(val_n).batch(batch_size)\n",
        "val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# test dataset\n",
        "\n",
        "test_data = tf.data.experimental.CsvDataset([\"test_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "test_data = test_data.batch(batch_size)\n",
        "test_data = test_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "epochs = 25\n",
        "list_dropouts = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "\n",
        "for dropout in list_dropouts:\n",
        "  print(f\"Current dropout is {dropout}\")\n",
        "  classifier = build_classifier_model(dropout)\n",
        "  classifier.compile(optimizer=optimizer,\n",
        "                          loss=loss,\n",
        "                          metrics=metrics)\n",
        "  history = classifier.fit(x=train_data, validation_data=val_data, epochs=epochs)\n",
        "  history_dict=history.history\n",
        "  print(f\"For dropout {dropout}\")\n",
        "  print(history_dict['loss'])\n",
        "  print(history_dict['val_loss'])\n",
        "  print(history_dict['binary_accuracy'])\n",
        "  print(history_dict['val_binary_accuracy'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aXfboR9ov4Sl"
      },
      "source": [
        "## Grid search for batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDS23jj1v36V",
        "outputId": "fb02d89e-339c-484d-bd31-a625bf26131d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "458/458 [==============================] - 480s 1s/step - loss: 1.1269 - binary_accuracy: 0.7002 - val_loss: 0.5100 - val_binary_accuracy: 0.6940\n",
            "Epoch 2/25\n",
            "458/458 [==============================] - 461s 1s/step - loss: 0.4109 - binary_accuracy: 0.8007 - val_loss: 0.2879 - val_binary_accuracy: 0.8672\n",
            "Epoch 3/25\n",
            "458/458 [==============================] - 461s 1s/step - loss: 0.2668 - binary_accuracy: 0.8842 - val_loss: 0.1939 - val_binary_accuracy: 0.9148\n",
            "Epoch 4/25\n",
            "458/458 [==============================] - 467s 1s/step - loss: 0.1898 - binary_accuracy: 0.9256 - val_loss: 0.1479 - val_binary_accuracy: 0.9503\n",
            "Epoch 5/25\n",
            "458/458 [==============================] - 464s 1s/step - loss: 0.1586 - binary_accuracy: 0.9463 - val_loss: 0.1628 - val_binary_accuracy: 0.9497\n",
            "Epoch 6/25\n",
            "458/458 [==============================] - 466s 1s/step - loss: 0.1246 - binary_accuracy: 0.9578 - val_loss: 0.1305 - val_binary_accuracy: 0.9634\n",
            "Epoch 7/25\n",
            "458/458 [==============================] - 469s 1s/step - loss: 0.1044 - binary_accuracy: 0.9683 - val_loss: 0.1327 - val_binary_accuracy: 0.9678\n",
            "Epoch 8/25\n",
            "458/458 [==============================] - 472s 1s/step - loss: 0.0992 - binary_accuracy: 0.9712 - val_loss: 0.1234 - val_binary_accuracy: 0.9705\n",
            "Epoch 9/25\n",
            "458/458 [==============================] - 467s 1s/step - loss: 0.0862 - binary_accuracy: 0.9746 - val_loss: 0.1176 - val_binary_accuracy: 0.9721\n",
            "Epoch 10/25\n",
            "458/458 [==============================] - 466s 1s/step - loss: 0.0746 - binary_accuracy: 0.9807 - val_loss: 0.1157 - val_binary_accuracy: 0.9749\n",
            "Epoch 11/25\n",
            "458/458 [==============================] - 469s 1s/step - loss: 0.0684 - binary_accuracy: 0.9818 - val_loss: 0.1189 - val_binary_accuracy: 0.9754\n",
            "Epoch 12/25\n",
            "458/458 [==============================] - 467s 1s/step - loss: 0.0583 - binary_accuracy: 0.9850 - val_loss: 0.1464 - val_binary_accuracy: 0.9721\n",
            "Epoch 13/25\n",
            "458/458 [==============================] - 467s 1s/step - loss: 0.0469 - binary_accuracy: 0.9877 - val_loss: 0.1250 - val_binary_accuracy: 0.9743\n",
            "Epoch 14/25\n",
            "458/458 [==============================] - 460s 1s/step - loss: 0.0403 - binary_accuracy: 0.9904 - val_loss: 0.1338 - val_binary_accuracy: 0.9749\n",
            "Epoch 15/25\n",
            "458/458 [==============================] - 461s 1s/step - loss: 0.0348 - binary_accuracy: 0.9911 - val_loss: 0.1473 - val_binary_accuracy: 0.9754\n",
            "Epoch 16/25\n",
            "458/458 [==============================] - 456s 996ms/step - loss: 0.0364 - binary_accuracy: 0.9918 - val_loss: 0.1611 - val_binary_accuracy: 0.9749\n",
            "Epoch 17/25\n",
            "458/458 [==============================] - 457s 998ms/step - loss: 0.0276 - binary_accuracy: 0.9930 - val_loss: 0.1458 - val_binary_accuracy: 0.9765\n",
            "Epoch 18/25\n",
            "458/458 [==============================] - 458s 1s/step - loss: 0.0227 - binary_accuracy: 0.9944 - val_loss: 0.1585 - val_binary_accuracy: 0.9754\n",
            "Epoch 19/25\n",
            "458/458 [==============================] - 459s 1s/step - loss: 0.0233 - binary_accuracy: 0.9948 - val_loss: 0.1558 - val_binary_accuracy: 0.9754\n",
            "Epoch 20/25\n",
            "458/458 [==============================] - 451s 986ms/step - loss: 0.0258 - binary_accuracy: 0.9937 - val_loss: 0.1505 - val_binary_accuracy: 0.9787\n",
            "Epoch 21/25\n",
            "458/458 [==============================] - 451s 984ms/step - loss: 0.0290 - binary_accuracy: 0.9936 - val_loss: 0.1552 - val_binary_accuracy: 0.9732\n",
            "Epoch 22/25\n",
            "458/458 [==============================] - 450s 982ms/step - loss: 0.0267 - binary_accuracy: 0.9939 - val_loss: 0.1760 - val_binary_accuracy: 0.9754\n",
            "Epoch 23/25\n",
            "458/458 [==============================] - 452s 987ms/step - loss: 0.0121 - binary_accuracy: 0.9971 - val_loss: 0.1827 - val_binary_accuracy: 0.9738\n",
            "Epoch 24/25\n",
            "458/458 [==============================] - 448s 979ms/step - loss: 0.0190 - binary_accuracy: 0.9958 - val_loss: 0.1883 - val_binary_accuracy: 0.9776\n",
            "Epoch 25/25\n",
            "458/458 [==============================] - 451s 985ms/step - loss: 0.0175 - binary_accuracy: 0.9960 - val_loss: 0.2083 - val_binary_accuracy: 0.9721\n",
            "[1.1268876791000366, 0.41091328859329224, 0.26682642102241516, 0.18981273472309113, 0.1585627794265747, 0.12462582439184189, 0.10439462214708328, 0.09923368692398071, 0.0861586406826973, 0.074553482234478, 0.06842844933271408, 0.05831009894609451, 0.04693516343832016, 0.04028552398085594, 0.03476426377892494, 0.03636551648378372, 0.027629371732473373, 0.02270563319325447, 0.023278960958123207, 0.02576974220573902, 0.028956500813364983, 0.026662178337574005, 0.01205827109515667, 0.01900443248450756, 0.017517095431685448]\n",
            "[0.5100361108779907, 0.28793448209762573, 0.19390785694122314, 0.1478610336780548, 0.16275732219219208, 0.1304551512002945, 0.13267238438129425, 0.12343831360340118, 0.11761533468961716, 0.11570245027542114, 0.11886848509311676, 0.14636698365211487, 0.12495041638612747, 0.13380475342273712, 0.14728564023971558, 0.16113406419754028, 0.14583949744701385, 0.15852795541286469, 0.15578582882881165, 0.15047205984592438, 0.15522581338882446, 0.17601686716079712, 0.18271781504154205, 0.1883322149515152, 0.2083217203617096]\n",
            "[0.7001768946647644, 0.8007102608680725, 0.884168803691864, 0.9255566000938416, 0.9463188052177429, 0.9577926397323608, 0.9683103561401367, 0.9711788296699524, 0.9745936393737793, 0.9807403087615967, 0.9818331003189087, 0.98497474193573, 0.9877066016197205, 0.9904384613037109, 0.9911214113235474, 0.9918044209480286, 0.993033766746521, 0.9943996667861938, 0.9948094487190247, 0.9937167167663574, 0.9935801029205322, 0.9938532710075378, 0.9971315264701843, 0.9957656264305115, 0.9960387945175171]\n",
            "[0.693989098072052, 0.8672131299972534, 0.9147540926933289, 0.9502732157707214, 0.9497267603874207, 0.9633879661560059, 0.9677595496177673, 0.9704918265342712, 0.9721311330795288, 0.9748634099960327, 0.9754098653793335, 0.9721311330795288, 0.9743169546127319, 0.9748634099960327, 0.9754098653793335, 0.9748634099960327, 0.9765027165412903, 0.9754098653793335, 0.9754098653793335, 0.9786885380744934, 0.9732240438461304, 0.9754098653793335, 0.9737704992294312, 0.9775956273078918, 0.9721311330795288]\n"
          ]
        }
      ],
      "source": [
        "epochs = 25\n",
        "batch_size = 16\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# training and validation dataset\n",
        "\n",
        "train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "\n",
        "train_val_data.shuffle(buffer_size=len(list(train_val_data)))\n",
        "\n",
        "train_val_set_size = len(list(train_val_data))\n",
        "val_n = int(0.2*train_val_set_size)\n",
        "\n",
        "train_data = train_val_data.skip(val_n).batch(batch_size)\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_data = train_val_data.take(val_n).batch(batch_size)\n",
        "val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# test dataset\n",
        "\n",
        "test_data = tf.data.experimental.CsvDataset([\"test_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "test_data = test_data.batch(batch_size)\n",
        "test_data = test_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "steps_per_epoch = train_val_set_size - val_n\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "init_lr = 0.0001\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "classifier = build_classifier_model(0)\n",
        "classifier.compile(optimizer=optimizer,\n",
        "                        loss=loss,\n",
        "                        metrics=metrics)\n",
        "history = classifier.fit(x=train_data, validation_data=val_data, epochs=epochs)\n",
        "history_dict=history.history\n",
        "train_loss.append(history_dict['loss'])\n",
        "val_loss.append(history_dict['val_loss'])\n",
        "train_accuracies.append(history_dict['binary_accuracy'])\n",
        "val_accuracies.append(history_dict['val_binary_accuracy'])\n",
        "print(history_dict['loss'])\n",
        "print(history_dict['val_loss'])\n",
        "print(history_dict['binary_accuracy'])\n",
        "print(history_dict['val_binary_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "HmCeBpXNusL1",
        "outputId": "1731b6fa-2923-4064-ddc6-7601611d8411"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1910553a30>]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABxlUlEQVR4nO2dd3gU1deA30klofcOCaG3QGgiSolUQREUEBBQAf19ir03XKqKHUSkqVTpvffea+iQUBMCoQRCetnz/XETTSVbswmZ93n2ITvl3rPD7py5p2oigo6Ojo5O/sPJ0QLo6Ojo6DgGXQHo6Ojo5FN0BaCjo6OTT9EVgI6Ojk4+RVcAOjo6OvkUF0cLYA6lSpUSLy8vR4uho6Ojk6c4fPjwbREpnX57nlIAXl5eHDp0yNFi6Ojo6OQpNE27ktl23QSko6Ojk0/RFYCOjo5OPkVXADo6Ojr5FF0B6Ojo6ORTdAWgo6Ojk0/RFYCOjo5OPkVXADo6Ojr5FF0B6OjkAk6FnWLthbWOFkMnn5GnEsF0dB5FRIRBywZx9vZZwj8Jx9XZ1dEi6eQT9BWAjo6D2Re8j8Ohh4lKiOLg9YOOFkcnH6ErAB0dBzPhwAQKuxVGQ2PLpS2OFkcnH6ErAB0dB3L9wXUWnl7I4MaDaVSuka4AdHIU3Qego+NAJh+aTJIxiTebv4mT5sTEgxOJTYylgEsBR4umkw/QVwA6Og4iLjGOPw7/wdM1nqZ6ier4e/sTlxTH3mt7HS2aTj5BVwA6Og5i4emFhEWF8XaLtwF4suqTOGvOuhlIJ8fQFYCOjoMYv388tUrWon219gAUcS9C0wpN2XJZVwA6OYOuAHR0HMD+4P0cvH6Qt5q/hZP238/Q39ufAyEHiIyPdKB0OvkFXQHo6DiA8QfGU9itMAN9B6bZ7u/tT6IxkV1XdzlIMp38hK4AdHRymNAHoSw8tZBXG79KYffCafY9Xvlx3JzddD+ATo6gKwAdnRxm8uHJJBoTGdZ8WIZ9nq6ePFbpMbZe3uoAyXTyG7oC0NHJQeKT4vnj0B90qdGF6iWqZ3qMv5c/R0KPEB4TnsPS6eQ3dAWgo5ODLDy1kJtRN3m7+dtZHuPv7Y9RjOy4siMHJdPJj5ikADRN66xp2jlN0wI1Tfs0k/1VNU3brGlagKZp2zRNq5S8vZ2macdSvWI1TXsued9TmqYdSd6+S9O0zB+HdHQeISYcmEDNkjXp4NMhy2OaV2yOh4uHbgbSsTvZKgBN05yBiUAXoC7QV9O0uukO+wGYKSINgZHANwAislVEGolII8AfiAY2JJ8zCeifvG8u8KXVn0ZHJxdzIOQA+0P2Zwj9TI+7iztPVHlCdwTr2B1TVgDNgUARuSgi8cA8oHu6Y+oCKd/WrZnsB3gBWCsi0cnvBSiS/HdR4Lo5guvo5DVSqn4O8h2U7bH+3v6cCDvBrahbOSCZTn7FFAVQEbiW6n1w8rbUHAd6Jv/dAyisaVrJdMe8CPyT6v0QYI2macHAAODbzCbXNO01TdMOaZp26NYt/cegkze5EXmD+Sfn80qjVzKEfmZGO692AGy7vM3OkunkZ2zlBP4QaKNp2lGgDRACJKXs1DStPNAAWJ/qnPeAp0WkEvAX8FNmA4vIFBFpKiJNS5cubSNxdXRylimHp5BgTODN5m+adHyTCk0o7FZYNwPp2BVTykGHAJVTva+UvO1fROQ6ySsATdMKAc+LyL1Uh/QGlopIQvIxpQFfEdmfvH8+sM6SD6Cjk9uJT4pn0qFJdKnehZola5p0jouTC2282uh1gXTsiikrgINADU3TvDVNc0OZclakPkDTtFKa9q9X6zPgz3Rj9CWt+SccKKppWsqvoQNwxlzhdXTyAotPL+ZG5A3eav6WWee182rH+TvnCYkIyf5gHR0LyFYBiEgiMAxlvjkDLBCRU5qmjdQ07dnkw9oC5zRNOw+UBcaknK9pmhdqBbE93ZhDgcWaph1H+QA+ssUH0tHJbYw/MJ4aJWrQqXons87z9/YH0MNBdeyGSR3BRGQNsCbdtuGp/l4ELMri3MtkdBojIkuBpWbIqqOT5zgYcpB9wfv4tfOvDw39zIyGZRtSwqMEWy5t4aWGL9lJQp38jJ4JrKNjRyYcmEAht0K83Ohls8910pxo59VOdwTr2A1dAejo2ImbkTeZf2o+L/u+TBH3ItmfkAntvNpx5f4VLoVfsrF0Ojq6AtDRsRtTDk8hPik+06qfppLiB9BXAQ4i3PEF+aITojkVdsouY+sKQEfHDiQkJTDp0CQ6+XSiVqlaFo9Tu1RtyhUqp4eD5iQPHsDUqdCiBZQoAevXZ3+OHVlyZgn1J9XnYMhBm4+tKwAdHTuw+MxiQiND/234bimaptHOqx1bL21FRGwknU4GRGDPHhg8GMqXh9deg6gocHGBbdscKtqsgFl4FfOiSYUmNh9bVwA6OnZgwoEJVC9Rnc7VO1s9lr+3P6GRoZy7c84Gkumk4dYt+OknqFcPWrWC+fOhb1/YuxdOnIAGDeDwYYeJFxIRwqaLm3ipwUtmR5GZgq4AdHRszOHrh9lzbQ/Dmg2zyY9W9wPYmKQkZdbp1QsqVoQPPoCiRWHaNAgNVeafxx4DTYMmTZQCcNDqa+6JuRjFyADfAXYZX1cAOjo2ZsKBCRR0LWhR6GdmeBfzpkrRKroCsJarV2HECKhWDTp3hq1bYdgwOHlSPfEPHgyF0xXqa9IE7t5V5+YwIsKM4zN4rNJjJpcQMReTEsF0dHRMIywqjH9O/sNQv6EULVDUJmNqmoa/tz8rz63EKEa7mAIeWeLjYcUK9XS/IbkVSYcO8P330L07uLs//Hw/P/Xv4cNQtap9ZU3HsRvHOHXrFL8//bvd5tC/STo6NmTq4alWh35mhr+XP3di7nDi5gmbjvvIcvq0Mu1UrKhMPadOwVdfwcWLyvzTu3f2N3+Ahg3B2RmOHLG/zOmYeXwmrk6u9Knfx25z6CsAHR0bkRL62dGnI7VL1bbp2O28VX+ALZe24FvO16ZjPzJERsLCheppf88eFcHTvbsy7XTsqG7k5lKggHIQ57AjONGYyNyTc3mm1jOU8Chht3n0FYCOjo1YenYpIQ9CzK76aQqVilSiRokaemG49IjAgQMqbLN8eXj1VbhzR5l4goNh0SLo0sWym38KDnAEbwjaQFhUGAMbDrTrPLoC0NGxERMOTMCnuA9P13jaLuP7e/uz/cp2Eo2Jdhk/T3HnDvz6K/j6qoSt2bPhhRdg5044cwY+/BDKlrXNXH5+Klw0JOfKcs88PpOSHiXpUqOLXefRFYCOjg04GnqUXVd38WazN+3mpPX39iciLoIjoTlvj84VGI2webOK069QAd59V9nx//hDhW/+9Rc88YQK37QlTZITsHLID3Av9h7Lzi7jxfov4ubsZte5dAWgo2MDUkI/X2n8it3maOvVFoCtl/KZGSg4GEaPhurVoX17WLcOXn8djh2DgwfV30VtE3GVKb6+4OSUY36ARacXEZcUx0Bf+5p/QFcAOjpWcyvqFnNPzGWg70CKFShmt3nKFCxD/TL180ddoIQEWLoUunVT4ZdffQXe3jBnDly/DuPHqxtzTuDpCXXq5NgKYObxmdQqWYtmFZrZfS49CkhHx0qmHZlGXFKczUM/M8Pfy5+pR1Soqb3NAw4jJkaZXc6cUY7dTz9Vzl0fH8fJ5OcHmzbZfZqL4RfZeXUnY/zHoNnalJUJ+gpAR8cKEo2J/H7od9pXa0/d0nXtPl8773bEJMawP3i/3edyGJMmqZv/X3+pDNwxYxx78welkEJD1cuOzA6YDZBjHeB0BaCjYwXLzi4jOCKYt5tbV/XTVNpUbYOG9uiGg0ZGwjffKFv/yy+rWP7cQEpGsB3NQCLCrIBZtPNqR5WiVew2T2p0BaCjYwXj94/Hu5i33UI/01Pcozh+5f0e3bpAEybA7dswapSjJUlLo0YqusiOjuB9wfsIvBvIgIb2KfyWGboC0NGxkGM3jrHz6k6GNR+Gs5MViUZm0s6rHXuD9xKTEJNjc+YI9++rBK6uXVU1ztxE4cJQs6ZdVwAzj8/Ew8WD5+s+b7c50qMrAB0dC5mwfwKerp682vjVHJ3X39uf+KR49lzbk6Pz2p2ff1YtGEeOdLQkmZOSEWwH4hLjmHdqHj3q9LC4f7Ql6ApAR8cCbkffZu7JuQxsaN/Qz8x4osoTuDi5PFpmoDt3lALo2fM/e3tuo0kTlZMQFmbzoVedX8W92Ht2L/2QHl0B6OhYwLQj04hNjM2R0M/0FHYvTPOKzR+tfIAfflC9eEeMcLQkWWNHR/DMgJmUL1Sep6o9ZfOxH4auAHR0zCTRmMjvB3/nKe+nqFemnkNkaOfVjoMhB3kQ98Ah89uUsDCV2NWnD9Sv72hpsqZxY/WvjRXArahbrLmwhv4N+uPilLNRT7oC0NExk+Vnl3Mt4ppdqn6air+3P0mSxM6rOx0mg8347juIjQWDwdGSPJyiRVU5Chv7Aeafmk+iMTFHSj+kR1cAOjpmMuHABLyKedGtZjeHydCyUkvcnd3zvh/g+nX4/XcYMABq1XK0NNnTpInNVwAzj8/Et6wvDco2sOm4pqArAB0dMwi4GcD2K9t5s9mbORr6mR4PVw9aVm6Z9xPCxo6FxEQYPtzRkpiGnx9cvqyc1jbgzK0zHLx+0CFP/6ArAB0ds0gJ/RzceLCjRcHfy5+joUe5G3PX0aJYxpUrMGWKqvNTrZqjpTGNlNLQR4/aZLhZAbNw0pzo16CfTcYzF10B5DPuRN+h18JezD0x19Gi5DnuRN9h9onZvNTgJYp7FHe0OPh7+yMI2y9vd7QoljF6tMqu/fJLR0tiOimOYBv4AYxiZFbALDr5dKJcoXJWj2cJugLIR8QlxtFzQU8WnV5E/yX9eWvNW8QnxTtarDzD9KPTiU2M5a0WjnP+pqZZxWZ4unrmTT9AYKAq9vb661C5sqOlMZ0SJVRZahv4AbZd3kZwRLDDzD+gK4B8g4gwdOVQdlzZwYznZvBByw/47eBvtP27LSEROdfqLq+SaExk4sGJtPNqR/0yuSNU0c3ZjSerPJk3/QAjR4KrK3z2maMlMR8/P5usAGYFzKKIexG61+puA6EsQ1cA+YTRO0YzK2AWI9qOYKDvQH7o+AMLey3kRNgJ/Kb45b8uU2ay8txKrt6/ytstcqbqp6n4e/tz6tYpbkbedLQopnPmjGrs8uabqt5/XqNJEwgKgnv3LB4iKj6KRacX0atuLzxcPWwnm5noCiAf8M+Jfxi+bTgDGg7gq9Zf/bv9hbovcGDIAUp6lKT9rPZ8v/t7RMSBkuZexh8YT9WiVXmm5jOOFiUN/t7+AHlrFWAwgIcHfPKJoyWxjJSMYCscwcvOLiMyPjJHK39mhq4AHnF2X93Ny8tfpnXV1kx9ZmqGLkN1Stdh/5D9PF/neT7e9DEvLHyBiLgIB0mbOzlx8wTbLm9zeOhnZjQu15ii7kXzzgouIAAWLIB33oHSpR0tjWWkKAArzEAzA2ZStWhVnqz6pI2EsgxdATzCBN0N4rn5z1G1aFWW9F6Cu4t7pscVdi/M/Bfm82PHH1l+djnNpjbjVNipHJY29zLhwAQ8XDwY7Of40M/0ODs508arTd6pC/T11yqj9sMPHS2J5ZQurRzXFjqCQyJC2HRxEwMaDsBJc+wtWFcAjyjhMeF0ndsVoxhZ3W81JT1LPvR4TdN4v+X7bBm0hfux92kxrQXzT87PIWlzL3dj7jI7YDYvNXyJEh4lHC1Opvh7+RN4N5Br9685WpSHc+gQLFsG778PxR0fRmsVVpSGnntiLkYxMsDXseYf0BXAI0l8Ujw9F/TkYvhFlvZZSo2SNUw+t3XV1hx5/QiNyjXixcUv8t6690hISrCjtLmb6UemE5MY49C6P9nRzrsdkAf8AMOHqzDKd991tCTW06QJnD8PEeaZS0WEmQEzeazSY9QsWdNOwpmOrgAeMUSE11e9zrbL2/iz+5+0rtra7DEqFK7A1kFbebfFu/yy/xf8Z/oT+sC+zbBzI0nGJCYenEhbr7YOqdNiKvXL1KeUZ6ncnQ+wZw+sXQsffwxFcq7hid1I8QMcO2bWacdvHudk2Mkcr/ufFSYpAE3TOmuadk7TtEBN0z7NZH9VTdM2a5oWoGnaNk3TKiVvb6dp2rFUr1hN055L3qdpmjZG07Tzmqad0TQtd8XX5VG+2fUNfx/7m+Gth/NSw5csHsfV2ZWfO//MP8//w9HQozSe3JgdV3bYUNLcz8rzK7ly/0qufvoHcNKcaOfVji2XtuTeKK6vvoIyZWBYzvdPsAspJSHM9APMPD4TVydX+tTvYwehLEBEHvoCnIEgoBrgBhwH6qY7ZiEwKPlvf2BWJuOUAO4CnsnvXwFmAk7J78tkJ0uTJk1EJ2vmnZgnGJB+i/uJ0Wi02bgnb56UmhNqivMIZ/lxz482HTs34z/DX6r8XEUSkhIcLUq2/H7gd8GAXLhzwdGiZGTLFhEQ+flnR0tiWypUEHnpJZMPT0hKkDLfl5Ee83rYUajMAQ5JJvdUU7oPNAcCReQigKZp84DuwOlUx9QF3k/+eyuwLJNxXgDWikh08vv/A/qJiDFZEdm+z1o+Yu+1vQxaNohWlVsx/dnpGcI9raFemXocHHqQV5a/wgcbPmBf8D6mPzudwu6FbTZHbuNk2Em2XNrCt099m+NNOizh33yAS1upXqK6xeMcnnIYz1Ke1OlZxzaCiain/woV4H//s82YOURibCJr315LbHhs5gdIL1geCb0WmjReaGQoba624fHKj7NwkWnnpKbTL50oUtG25jNTvtkVgdThBcFAi3THHAd6Ar8CPYDCmqaVFJHUNVNfBH5K9d4H6KNpWg/gFvC2iFxIP7mmaa8BrwFUqVLFBHHzHxfDL9J9XncqFanEsheXUcClgM3nKOJehEW9FvHDnh/4dPOnnAw7yeLei6lT2kY3ilzGbwd+o4BLAYb4DXG0KCZRs2RNKhSuwJbLWxjaZKhFYwTvC2bV/1ZRskZJ2ymADRtg926YOBEK2P57aU8C1wdyZOoRSlQvgbNbJvkficXgQQKcvAlO2VvTb0TcoFx8OVxx5Ra3zJYnKS7J7HOyw1aPNh8Cv2ma9jKwAwgB/pVW07TyQANgfapz3IFYEWmqaVpP4E8gQ1aEiEwBpgA0bdo0lxo4Hce92Ht0nduVRGMiq/utppRnKbvNpWkaH7X6iKYVmtJnUR+aT2vOX93/4oW6L9htTkcQHhPOrIBZ9G/QP9vw2dyCpmm082rHpoubEBGzV4BJ8UmsGLICBO6cv0P4xXCKV7MyVDPl6b9qVRic+3IosiNwXSBuhdx449QbmSuAlSvh2Wdh+m54/PGHjnUv9h7lfijHEL8hvPn0m3aS2HxMcQKHAKnL9VVK3vYvInJdRHqKSGPgi+Rt91Id0htYKiKp4wmDgSXJfy8FGponuk5CUgIvLHiBoLtBLOmzhFqlcqajUjvvdhx5/Qj1y9Sn18JefLjhQxKNiTkyd07w59E/iU6IzvXO3/T4e/tzM+omZ26fMfvc3eN2c+vULTr80AFQT79Ws3IlHDyolIB75kmIuRURIWhdEN5PeWd+8wezMoIXnV5EXFKcQyt/ZoYpCuAgUEPTNG9N09xQppwVqQ/QNK2Upv2b0vYZ6mk+NX2Bf9JtWwa0S/67DXDeDLnzPSLC/63+PzZf2szUZ6bS1qttjs5fqUgltr+8nWHNhvHj3h9pP7M9NyJv5KgM9iDJmMRvB3+jddXW+JbzdbQ4ZpHiBzA3HPT2udvsGLWDur3q0vL9lhTzKkbQuiDrhDEaVdx/9eowMHfd9Ezhzvk73Lt8j+qdH+JPqVABypY1SQHMCphFrZK1aFahmQ2ltJ5sFYCIJALDUOabM8ACETmladpITdOeTT6sLXBO07TzQFlgTMr5mqZ5oVYQ6btWfAs8r2naCeAbIG8YW3MJ43aPY/rR6Xzx5BcMajTIITK4Obsx4ekJzO4xmwMhB/Cb7Mfuq7sdIoutWH1hNZfvXebt5nkvKtmrmBfexbzNUgBiFFa9tgpXT1e6jO+Cpmn4dPbh0pZLJMVbYXNevBiOH1elH1xdLR/HQQStVwrQp5NP1gdpmloFZBMKein8Ejuu7GCg70CbBmfYApPyAERkjYjUFBEfERmTvG24iKxI/nuRiNRIPmaIiMSlOveyiFRMifZJtf2eiHQVkQYi0lJEjtvygz3KLDq9iE83f0qfen0Y2W6ko8Whf8P+7Buyj4JuBWk7oy3j94/PvfHo2TB+/3gqF6lM99qOq9FuDe282rHt8jaMaX9uWXJk+hGu7LhChx86UKhcIQCqd65OfGQ81/ZYWFoiKUnd+OvUgb59LRvDwQSuC6RkrZIU987GD9KkCZw+DTExWR4yO2A2AP0b9LeliDZBzwTOY+wP3s+ApQNoWaklfz/3t8OLSaXQsGxDDg49SNcaXXln3Tv0X9KfyPhIR4tlFqdvnWbzpc280eyNPBH6mRn+3v6Ex4Zz/Eb2z1MPQh+w8aONeLX1ovGrjf/d7u3vjZOLE4HrLPQDzJunav6PGAHOuat6qikkxCRwedvlhz/9p+DnpxReQECmuyW59ENbr7ZULVbVxpJaT+64e+iYxOV7l3l23rOUL1Se5S8ut0u4pzUUK1CMJX2WMNZ/LPNPzeexaY9x/k7ece1M2D8Bd2f3PBP6mRkpdYFMMQOtfWstibGJdJvSLY1pwr2wO1WeqGKZAkhMVPX+GzaE5583//xcwNWdV0mMSXy4/T+FlIzgLPwA+4L3EXg3MNeUfkiPrgDyCPdj79N1blfiEuNY3W81pQvmzlrqTpoTnz35GetfWs/NqJs0ndKUpWeWOlqsbLkXe4+ZATPp36C/XUNp7U2FwhWoVbJWtuWhzy4/y5nFZ2gzvA0la2QMdfXp5MPN4zd5EPrAPAFmzlT9fkeONCk2PjcSuC4QZ3dnvNp4ZX9w5cpQsmSWfoCZx2fi4eLB83VzpzLMm/9D+YyEpAR6LezF+TvnWdJnSZ5IvmpfrT1HXjtCndJ16LmgJ59u+jRXh4r+G/qZSxq+W4O/tz87ruzIsoprXEQca95cQ5kGZXj8o8zj11OefoM2mBENFB+vbvxNm6r4+DxK0PogvNp44eppgvNa07IsDR2XGMf8U/PpUacHRdxzZwE8XQHkckSEYWuGsfHiRiZ3m/xvqF9eoHLRyux4eQf/a/I/vtv9HR1ndSQsKvdV/Eip+vlklSdpVK6Ro8WxGn9vfyLjIzkcmrlZYtNnm3hw/QHPTnsWZ9fMbfRlfctSqFwh88JBp0+HK1dg1Ch1Y8yD3L96n1unb+HT2QT7fwpNmsDJkxAXl2bz6gurCY8Nz7XmH9AVQK7nx70/MuXIFD5t9SmvNn7V0eKYjbuLO5O6TeLv7n+zN3gvfpP92Be8z9FipWHNhTVcDL+Y5xK/siIlJyQzP8C1Pdc4NOkQLd5uQcXmFbMcQ9M0fDr5ELQhCGOSCRFFMTEwejS0agWdOlkqusNJSYCr3smMekp+fsr3ceJEms0zj8+kfKHyPFXtKVuKaFN0BZCLWXpmKR9v/JhedXsx5qkx2Z+QixnUaBB7B+/F3cWd1n+1ZuKBibkmVHTCgQlUKlKJ52o/52hRbEIpz1I0LNswQ4OYxLhEVgxZQdHKRfEfnf1Ksnrn6sTcjeH6oevZTzp5Mly/nqef/gGC1gVRpHIRStUxww+USWno29G3WX1hNf0a9MvVEWW5VzIbsuzsMkSEbjW74ersuKSUXZNOULFeMbxbV8722IMhB+m/pD/NKzZnxnMzck24pzU0KteIQ0MPMWDpAIatHcaOqztoWamlQ2WKTohm48WNjPEf49Dvhq3x9/Lnj8N/EJcY928v6F3f7uL2mdv0W90Pt0Ju2Y5RrUM10JRTtFKLSlkfGBUF33wD7dqplw3YdHET9UrXo3zh8jYZzxSSEpK4uOki9frUMy9hy8tLtbhM5QeYd3IeicbEXFf6IT35QgFMODCBLZe2UKZgGQb5DmJw48E5VjcnhRMbb9DujdqUdrrD/j3XqdyiQpbHXr1/lWfnPUvZQmVZ/uJyPFw9clBS+1Lcozgr+q5g7M6xGLYZWHBqgaNFoliBYgz1s6yCZm7F39ufX/b/wr7gfbTxasOtM7fYNXYX9V+sT42nTWsR6lnSk4rNKhK0Poi2X7fN+sCJEyEsDJYsyfoYM9h2eRsdZnWgZsma7B28N8d6MYfsDyEuIs608M/UZJIRPPP4THzL+tKwbO4ucZYvFMD6l9azLnAd049O56e9P/H9nu95ssqTDG48mF71euHp6mnX+ZOSYMhLMRTlPlFGD7q1vcGuoAcUrpCxnn5EXARd53YlOiGazQM3U7ZQWbvK5gicNCe+bP0l77d8n/ikeEeLg4eLx79PyY8Krau2xklzYuvlrbSu0pqVQ1fiVsiNzr92Nmscn84+7By9k5i7MXiUyORBJCICvvsOOndW9n8riUmI4bWVr1GxcEUu37tMz/k92TBgA27O2a9YrCVwXSCas4b3U97mn+znB7/+CvHxnLkfxMHrB/mx44+2F9LG5AsF4OLkQrea3ehWsxs3Im8w49gMph2dxsvLX+btdW/Tr34/hvgNwa+8n11qdUww3OFAmDdzO/xJyXa+PP25Ly82Osryq41xKfDff0GiMZE+i/pw5tYZ1vZfS93SdW0uS27C09XT7so3v1K0QFGalG/Clktb6Ha2G9d2X6P7X90pWKagWeNU71ydHSN3KNNI73oZD/j1V7h7V9n+bcDI7SO5cPcCmwduJvRBKC8tfYmhK4fyd/e/7V5HJ3BdIJVbVqZAUQsSLJs0UWGwp08z684CnDQn+jXoZ3shbU1mbcJy68uWLSGNRqNsv7xdBiwZIAVGFxAMiO8kX5mwf4Lcjb5rs3kuXRLxdImVp53WiPFasIiI/NFvu4DIsAbb0sjzf6v+TzAgUw5Nsdn8OvmXTzZ+IiU+LCFji4yVGU/NsKiVZ1JCknxb/FtZ9sqyjDvv3hUpWlSke3erZRURORp6VJxHOMury179d5thq0EwIKO2j7LJHFkReTNSDBhk++jtlg1w/rwISNLUKVL5p8rSZXYX2wpoJWTREtLhN3VzXvbqCRweEy4TD0wUv8l+ggFxH+Uu/Rf3ly0Xt0iSMcnicY1GkU5PREpBHsiVwSPS7PugyVYBkV97KiXw056fBAPy8YaPrfosOjoprLuwTvrU7iMj3UfKnQt3LB5nQa8F8mOFTHpBf/GFuoUcP26lpKpfbpPJTaTs92XlTvR/shqNRnlpyUuCAZkbMNfqebLi+KzjYsAgIYdCLBsgKUmkSBEJ7v+sYED+OfGPbQW0El0BmMjh64fljVVvSNFvigoGxOdXHxm7Y6yERJj/xZg1S13h8a7vi9y8mWZfYlyiPFd+rziRKN//30zRDJo8P/95qxSOjk5qjsw/IgYM8sXgL6wbZ7oa50bAjf82hoWJFCok0ru3lVIqftj9g2BAFpxckGFfbEKsPPnnk+I+yl12X91tk/nSs+SlJTKu9DgxJpm/SvqXNm0ksGYpKTy2sETHR9tOOBuQlQLI+7GFNsavvB8Tu07k+gfXmdVjFpWKVOLzLZ9T5ecqdJ/XnZXnVppU0uDWLXj3rURasoc33nOHMmXS7Hd2c2b2sQb4epzBMKkHXYJ7MrPHzEci3FPH8cTei2XLu1t4UOUBm5pusmqslKqYaYrDjRsH0dGq8JuVBN0N4qutX9G9VvdM24u6u7iztM9SKhetTPd53Qm6a2WzmnSIUQhcH0j1TtXRnCz3MyQ0akiFi7fpU+v5PBO5p99tssDT1ZOXGr7Etpe3cW7YOT58/EP2B+/n2XnPUuXnKny++XMC72ZdLfHddyHivjDV812cP/4g02Puut8l7n99Kard5difEwg/ft9On0Ynv7Hp001E3YzC83NPDoYd5H6s5d+tIhWLUKZBmf/KQty4oUI/+/dXNf+tQER4fdXruDq7MvHpiVk6ekt6lmR1v9UYxUjXuV0Jjwm3at7UhB4NJfpWtGnlnx/CgbKJeCTC656tbSSZ/dEVgAnULFmTb9t/y7X3rrGszzKaVGjCd7u/o8aEGvjP8GdOwBxiEv5rCLFmDcydC5/LGOp92EVVC0zHg7gHdPunG8FlrvLHlHM8MBbkmdb3iLyRt2ro6+Q+ruy8wuHJh2nxbgv8O/ljFCM7r+60akyfTj5c2XmF+Mh4lfQVH69aPlrJjOMz2HxpM9+1/46KRbIuTQHqd7i0z1Iuhl/k+QXP2yyEOGVl49PROgXwl7PqCeAXmjsy3E0iM7tQbn3lhA/AVILvB8vo7aPF+xdvwYAU+7aYDFs9THZfCJDKlUXqFLoiscXKioSHZzg3ISlBnp7ztDiPcJb1getFRGTNiAPiRKI8U3afJMYl5vCn0XlUSIhJkN9q/ya/eP0icZFxEpMQIwVGF5D31r1n1bhBm4LEgEHO/blLxM1NZPBgq2W98eCGFP+2uDz555Nm+b5mHJshGJBXlr1iUWRTev584k+Z3GSyVWOERISIy9eaxBZwFXnrLatlsjXoPgDbUrFIRb5o/QWBbweyeeBmulTvwpQjU2j10hauXTMyPbIP8t5rUKxYhnPfW/ceay6sYeLTE+no0xGALsObMaHPLlbebMEHLXbl8KfReVTYOXYnt8/epusfXXEr6EYBlwI8XvlxsxvFp6fKE1Vw9XQl8MflIAJffWW1rG+ve5uohCimPjPVLN/XQN+BfNX6K/469hff7vrWKhli78dybe8187N/0zH3xFwSNSGpUUOTmsTnFnQFYCVOmhP+3v7MfX4uy1uFoR14m+eLTaG65z4qJ37Py8teZueVnSrkCtVz9reDv/FByw94venracZ6Y14b3m28nV+PtWFi7+2O+Dg6eZiwU2Hs+nYXDV9qmKaapb+XP8dvHudO9B2Lx3Zxd8H7sTIEnoqDoUOhqnXtDVecW8GCUwsY3nq4RWVZRrQdQd/6ffl8y+dWlRO5tPkSkiRWKQARYcbxGTxW6TE8WzwBx46p9P88gK4AbER8PHwwrCgVS8Xz172PiPngHXo2G8iSM0to/Xdrak+szbvr3uW99e/xXO3n+K79d5mO88O+J3i23H7eXvgEa0YczOFPkf948ADOnXO0FNYjRmHl0JW4F3Gn408d0+xL6SGx7fI2E0YKA+5muscnKoBwSnC33zCrZI2Ii+CN1W/QoEwDPmr1kUVjaJrGn93/pFXlVgxcOtDiEuOB6wJxL+JOxRYP9z88jOM3j3My7CQDGg5QJSGio/PMl0pXADbi22/h9Gn4o8xwCpcrRJVPxzL5mcmEfhDKX93/orRnaX7d/yuNyzVmdo/ZODtl3ojD2c2ZOUfr4etxnj6G2hxfkDe+SHkJEdi3D4YMgfLloW5d2LvX0VJZx8FJBwneG0ynnztRsHTacg9NKzSlkFshE8xA8cBjQE0gXejo+fNUPzAXgMCjZraJTMdnmz4jNDKUac9Os6rGTwGXAizts5SKRSry7D/Pcin8klnniwiB6wKp1r5alo1xTGHm8Zm4OrnSp16fTEtD52oycwzk1lducgKn5tQpEVdXkb7tQlXm14QJmR4XdDdIImIjTBoz+OB1qeh0XSo5h0jI4VBbiptvuXVL5KefROrWVf9Nnp4ir74qUrmy2hYX52gJLeP+tfsytvBYmdlhZpZO0S6zu0id3+pkM9Lvon5qlUTESUTGiEiyc7ZvXxFPT/nV6yeZ283yjNydV3YKBqx2SqfmzK0zUuzbYlLntzoSHhNu8nlhp8LEgEEOTTlk8dwJSQlS5vsy0mNej+QNCSIeHiLvvmvxmPYA3QlsH4xGZRItXFj4JeJV1SR6aOalhasVr0Zh94wVQDOjYtPyrJobQXhSEZ55IpyosChbip1vMBphwwbo0wcqVID334fChWHqVBXOPn06TJqkVm/fWudPdAgiwuo3VmNMNNLtj25ZxtH7e/tz5vYZQh+EZjFSDDAaaAWcAfoAXwDPwek9MG8evP021bvW4tKWSyTGmd/fOTYxlqErh+JVzItR7WxTPA6gdqnaLOm9hAt3L9BrYa8seyGnx6LuX+nYELSBsKiw/+r+u7hAo0b6CsAer9y4AvjtN/U0OeO9o+qPKbYt5LbqaxUe2r2cHh5qDleviowYIVK1qvpvKVFCPZSdOJH58X37qlXcqVM5KqbVnFxwUgwYZPf3Dy+RcPj6YcGAzAmYk8URP4v6mW1Nfm8UkV9FxEXkekGRxzxFbt+WcyvPiQGDBG0KMlvWr7Z8JRiQdRfWmX2uKfx19C/BgAxdMdSk8NBZHWfJb3V+s2rOFxe9KCW+KyFxiamWj2++KVK4sKoPlEtArwVke65eVeVQOnY0irFRY5Fq1UTi420+z/jntwmoAnI6WRMXJ7JokUjnziKapr7d7duLzJsnEhv78HNv3lRK4vHHc9Xv9qFE342W78t+L5P9JktSwsOFTkxKlGLfFpPByzOL348UkTIi4p9x17k/RUIQiXMRkVkS9yBORrmNkvUfrjdL1oAbAeIy0kUGLBlg1nnm8vmmzwUDMm7XuIceFx8VL6PcR8m69yxXRvdi7kmB0QXkzdVvpt3x55/qy3f2rMVj25qsFIBuArIQEXjjDWVi+OO59WjHjqrMSFfbtxV8a1Eb3mq4nR8Pt2Vyfz08ND1nz8JHH0GlSvDCC6o395dfwsWLsHGjMv+4Z9PvpUwZ+Pln2LNHmYTyAhs/3kj07WiemfYMTi4P/yk7OznT1qtthj7Bit9Q0T+ZmGU+WAL+RcGpBTAAt0LvUeWJSv+VhTCBJGMSQ1cOpXiB4vzc6WeTz7OEUf6j6FOvD59s+oQlZ7LuUHZ5+2WS4pKsCv9cdHoRsYmxGds++vmpf/OCGSgzrZBbX7lpBTBvnlLyP/2YJFK/vkitWsoBZCcS4xKla5n94kyCrBt90G7z5BUiI0X++kukVSv1/+DiItKzp8jq1SKJFlrKjEaRjh3Vqu7qVZuKa3MubbskBgyy4aMNJp8zft94wYBcCr+Uaut9ESkhIpnUr9+7V13cMWNEJEFEPhQRZNe4F8WAQe4H3zdp3l/2/mL3cs6piY6PlsemPSYeoz3kQPCBTI9Z8/YaGe0xWhJiLP/Ntv6rtdScUDOjuSk+XsTdXeSDDywe29agm4Bsx+3bIqVLizRrJpI4J1kT/GP/+t8RIRHiW+CsFOa+nFh8zu7z5TaMRpEDB0Ree02ZWEGkZk2RceNEbtzI/nxTuHhRRQd166bmy40kxCTIhJoT5Ndqv0p8lOkmxxM3TwgG5M8jf6baOkLUzyuTh4oOHURKlRJ58CDVxoVyI8BLDBjkyPSs/An/cSn8khQcU1CenvO0Tco2mMrNyJvi/Yu3lP2+rFwOv5xh/4RaE2R2l9kWj3/x7kXBgIzePjrzA5o1E2nXzuLxbU1WCkA3AVnABx9AeDhM+yMR5xHDoX596N3b7vMWrlCYVTuKUNgpiq69PbkREGb3OXMDd+/ChAkquKJ5c5g1C3r2hJ07/zP/lLVR62Rvbxg9GlatgvnzbTOmrdkxegd3zt+h6x9dcfU03eRYr3Q9SnuWTmUGugv8CHQHmqabZIeyn336KRQqlGrHC5Spv5rCFaIJXLcMGAdkXvxMRPjfqv+haRqTuk6ye0vH1JQpWIbV/VYTmxhLt3+6pamGGn4pnDvn7lhV/XN2wGwAXmr4UuYHNGmiTECSywvDZaYVcusrN6wANmxQT55ffCEif/+t3ixZkqMyHJ59WjyJlGYFT0rUragcnTunSEoS2bxZRee4u6vL3LSpyKRJIvfu2XfuxET1AFe6tFrt5SZuHL8hI11GytJBSy06v8/CPlLxx4rJT+Ofi/pppevoZTSKtG4tUq6cSFTm369lry6Ub4t9JUkJTiLSQ5QpKS2zj88WDMiE/ZnnxeQEm4I2ictIF+k0q5MkJClzz8FJB8WAQW6dvWXRmEajUaqPry5t/26b9UFTpqgvbWCgRXPYGnQTkPVERop4eyuzQ0xEvHrj5+cQW8Hyz/eJRpI8X2FPthEgeYngYJHRo1VAFYgUKyYybJjI0aM5K8fx48qvMGhQzs77MJISk2Rq86kyrvQ4ibptmeL/4+AfggEJvLNXRAqKSCYdvTZulIclNIqInJyvwk+v7h4nIs4iUlNETv67PywyTEp+V1JaTmspiUmODV+edniaYED+t/J/YjQa5Z/u/8gvXr9YbJLae21vJqa0dBw+rK7h/PkWSm1bdAVgA95/X12x7dtFZPJk9Wb1aofJ8/NzKjz04+ZbHCaDLYiPF1m2TNndnZzUZW3XTmTOHJFoB3bWS2l5u8F0P6td2ffrPjFgkIA5ARaPcf72ecGAHAvtICrb93TaA4xGkRYtVHr0Q2Jno+9EywinEbJl+BYR2S4iZUXEU0SUo7f/4v7iOtJVTt48meUYOcknGz8RDMgP23+QsYXGysr/rbR4rP9b9X/iMdpD7sc+xAkeF6cSSz75xOJ5bImuAKzkwAF1c3r9dRGJiRGpVEnksccc6ik0JhnljfpKCUwZsN1hcljK+fPq91G2rPomli8v8tlnIhcuOFoyRUyMWu15eanVnyO5d+WejCk4RmZ3nm2VM9VoNIrf5PISl+gsIpnE5K9aJaYmNE5rOU2mNp+a/C5ERFqJCHLp7rPiOhL5euvXFstpa5KMSfLCghfEa5ByYJ9ZdsaicWITYqX4t8Wl3+J+2R/s56cSUXIBugKwgvh4kYYNRSpUSLY/jx+vLt2mTWaNs2vcLgncYFubYEJMgnQudUCcSZCN31pe0ySniIoSmTlTpE0bdQmdnUWefVZkxQq7RtFazPbtSs7333ecDEajUeZ0nSNjPMdI+KVwq8dbH1hbEpIQo/F8+onUTcvEhMZtI7aJQTOk8kPFS1zimyKCHArxkNiEi1bLakui46NlQJcB8pXTV7L37F6Lxlh8erFgQNZeWJv9wUOHquzCXBBOlpUC0KOATOCHHyAgAH7/HYq6RsPYsdCmDfj7mzzGuZXn2PTxJjZ+uNGmsrkUcGH+0VrULXCRFz714fSKrPsUO5KjR+HNN1U9noEDIThYXcZr12D5cnjmGVVGJbfRujX873/wyy9w0EHVuU8tOMWF1RdoN7odxbyKWTnaNZ7yDuSvY3DqVlzaXUuXqsgVExMaq3euDgJBG1OSwlz5aIMzfRdBo3Lg7tISyD2Jix6uHjQPaU6Ydxg9V/Xk2v1rZo8x8/hMyhUqR/tq7bM/2M9PhbBduWKBtDmDrgCy4fx5GDFCZZh2745KE71xA0aNAhPD2uIi4ljzxhqc3Zy5GXCTG8dv2FTGIpWKsGpLQTyc4uja052bJ2/ZdHxLuXdPKc0mTdRv4c8/oVs32LpVXdfPPlPlmHM7334L5cqp8tEJptUZsxkxd2NY9/Y6KjSrQIu3W9hgxNE4aRqjd5C2PHRSkrrx16qlmr2bQPkm5fEo4fFvVvC+4H1MODCBUp7DcHY6CBQFnkKFmjo+HPJB6APuBNyhw4sdiEqIots/3XgQZ3pp69vRt1l9YTX9G/THxcmEp5U8UBpaVwAPIaXSp4eHikPnwQN1N+jYEZ580uRxNn+xmYiQCPos7YOTqxPHZxy3uaxVWlZk5V93uJlUku6P3STmbkz2J9kBERVCPnCgurm/+aa6t/z2G1y/DrNnQ9u24JSHvnlFiypFFhAA33+fs3Nv+HAD0XeieWbqMzg5W3vRLgJ/ommv4erkk1YBLFgAp06BwWDyUszJ2Qmfjj4Erg8kLiGOISuGULFIRcY+NRaoBxxE5Rh8CPQGrOsjYC1BG5SiavVCKxb1WsSpsFP0WdSHRKNplU3nnZxHojExY+mHrGjQQF3L3NwiMjO7UG595bQPICXQZ9q05A1jxqgN+/ebPMbVPVfFoBlkzVtrRERkXo958n2Z7+0Wurnk472ikSS9Ku3O0fDQ0FCRb78VqVFDXaIiRUT+7/9EDh3KFSZQm/DCCyonIadqfF3cfFEMGGTjpxttNOIgESkgIiEyZPkQKfpNURWimZCgvN3165tdCe/o30fFgEFGTB0hGJBV51alO8IoIuNERRzVlgxRRznIohcXyQ/lfvjXiT750GTBgLy5+k2THOvNpzYX30m+5k3q6yvSqZP5wtoYrHECA52Bc0Ag8Gkm+6sCm4EAYBtQKXl7O+BYqlcs8Fy6c8cDkabIkZMKICRE3cT8/ZNvYOHhKii9WzeTx0iMS5SJ9SbKT5V/ktgIFVJ3ZukZMWCQc6vsV8rh+65bBUQ+a2nf8NCEBBU08txzypkLIk8+KTJjRpb5Q3ma0FD1FWjd2v4VQ+Oj4+VXn19lfPXxEh9tiwqzZ0XdhFUjlrkBcwUDcijkkFUJjRHXI8SAQdp0aCN9F/V9yJFbRFUcLSQiC8wX30qSEpPku5LfydKBS9Ns/3D9h4IB+WXvLw89/8ytM4IB+XHPj+ZN/MorKqPQwU9BFisAwBkIAqoBbsBxoG66YxYCg5L/9gdmZTJOCVTuuWeqbU2BWblRAfToIVKgQKqQxOHD1eU6csTkMbaP2p7hZp8YlyjflfxOFvZeaGOJ/8OYZJTXam8XEPnzlR02Hz8oSMXIV6igLkmZMiIff5yrqt/ajenT1WeePNm+82z8dKMYMMjFzbaKpOkrKk7/poiIhD4IVXHx276xKqExyZgk71d+X4ZUGyI3I29mc3SwiLQU9XN+X0RsXzo9y5n3B6scirlpcyiSjEnSY14P0QyarDi7IsvzP9/0uTiNcJLQB2Z250tpGOLg6oLWKICWwPpU7z8DPkt3zCmgcvLfGhCRyTivAXNSvXcGtgLlc5sCWLRIXZnvvkvecPu2qj72/PMmj3HrzC0Z5TZKFvbJeKNfPWy1jHIfJdF37ZflFB8VLx1KHBIX4mXz94etHi8mRmTuXJGnnlLXxslJ5Omn1UOjHVog5FqMRrUqLFJErRLtQejRUBnhPEKWvbrMRiOeEBFNRD5Ns7XOb3Xk11friTUJjRMPTJT2rdqLwcUgsfezabogIiJxIjJM1E/6SRHJmXanGUNW/yMqPkqaTmkqBccUlCPXMz7gJRmTpPJPlaXz7M7mT7xnj7q+y2z1f2kZWSkAU7xKFYHU8VLBydtScxzomfx3D6Cwpmkl0x3zIvBPqvfDgBUiklWPOgA0TXtN07RDmqYdunXL/tEt4eEwbBg0bqzaBwLK8xcZqcKBTECMwsrXVuJa0JXOv3bOsL/RoEYkxSVxeuFpG0qeFldPVxYerU4t98s8/1E1zq42vX57ak6cgHfeUeGb/fpBUJAKgLpyBVavhh497NICIdeiaTB5MsTHKwe32Di4xZhkZOXQlXiW9KTj9x1tNOrXQGHgozRbO1ZsTc/FpzG2aAFdupg96rX71/h006cUa1MMEuHSVlOasrsBE4DZwCHAD9hl9tzmErgukIrNKuJZyjPDPk9XT1a8uIISHiXo9k83giOC0+zffnk71yKuMbChic7f1Pj6qoiHXOoItlUsxodAG03TjgJtgBAgKWWnpmnlgQbA+uT3FYBeqG/CQxGRKSLSVESali5d2kbiZs3HH8OtWzBtWnIwxM2bKgSob1+oV8+kMY5MO8LVnVfp+ENHCpUtlGF/+SblKVWnlF2igVJTtEpRVm3ywE1L4OnnXLl12jQFGhEBU6ZAixbQsCH88YcKfNq4USmAL79UzVfyK9Wrw8iRsGwZLMm654hF7B+/n+uHrtN5fGc8SnjYYMQjwBLgPZQV9j9ePphIpfvCmXf6mRzSnIKI8MaaN0iSJL55+xvcCrkRuM6cHJT+wH6gIMpV+Cv2ChWNCY8hZH8IPp2zrv5ZvnB5VvdbzYO4BzzzzzNExkf+u29mwEwKuxWme+3u5k/u6Ql16uTeUNDMlgUi5pmA0h1fCAhOt+0dYEqq912BG8Dl5JcRCMxOFnubgLZsUau1jz9OtfHdd5W945xpTtuI6xHyTdFv5O92fz80smDntzvFgEHuXLhjpdTZs2/aCSlAtDxe+LjEhMdkeozRKLJ7t/JZeXqq61C/vsgvv+S+ipi5gYQEkcaNVcHMu3dtM2b4pXAZ4zlG5nSdY8Pa+V1FpLiI3Eu7OSpKksqWka1VkVHbRpo96rwT8wQD8tOen0RErCiwdk9Euov6ifcRkQcPPdoSUvomX92dvR1+7YW14jzCWbrN7SaJSYkSFR8lhcYWkleXvWq5AAMHqi+KA8EKH4ALKoDYm/+cwPXSHVMKcEr+ewwwMt3+fUC7h8zhcB9AdLRI9eoiPj6pIliCg1Xc3yuvmDzO/Ofny+gCo+X2+YffNe9fuy8GzSBbvsqZQm4L398jIPJilV1pwkPDwkR++EGkdm31bShUSGWw79vn8MCFXM/hwyr6acgQ68cyGo0yu/NsGVtorNy7ci/7E0xir6ifztiMu77/XgTklQ+qi/+MTHoBP4TbUbel9LjS0mxKs38rfVpXYjkpWUYnEakrKmLJdix7dZl8W+xbk8Oifz/wu2BA3l7ztswJmCMYkG2XtlkuwC+/qB/X9euWj2ElWSmAbDM+RCRR07RhKPONM/CniJzSNG1k8qArgLbAN5qmCbADeDPlfE3TvIDK5Kac8EwYMQICA2HzZrVqA1StgqQk+Oork8Y4u+wsZxaf4alvnqJkjfQukLQUqVSEau2rETArgLaGtmhO9m2W8cKPLfn29DY+XdeWam220Xp4W6ZNU2UYEhKgZUuYPl31tSmU0Wr1yHL77G32/bqPpPik7A/OhM/qQsA0+CPEuqzmuHtxBK4LpPP4zhStUtTygdLwFVAaeCvt5gcP4LvvoGNHineoz9yDE4lNjKWASwGTRv1w44eEx4az6dlNODs5A/zbXCVofRClapUyU04nlGGhOcpV2Az4m//cipYjIgStC6Jah2rZ9k1O4f+a/R8X7l7g530/U+ZUGaoWrcqTVU1P/MxASkbw4cMqFT4XYVLKn4isAdak2zY81d+LgEVZnHuZjE7j9Mc49JZz9Kiq9zN4cKryPleuwNSpKv/f2zvbMWLvx7LmzTWU9S1Lyw9amjSv70Bflg5YypWdV/Bq42X5BzCRj1e34UKdnYzd05axnaFkkQTeGubC4CEadevaffpcR1RYFLM7zyYqLArPkhmdg6ZQXKCWM1zcANHlzDalp6H+i/Vp9kYzywdIw3ZgE6oMQ7qf14QJcPs2jBqFf7Fb/LTvJ/Ze20s773bZjroxaCN/H/ubL578goZlG/67vbh3cUrWLEngukArSlY8BRwhIeEFXF2fZ82aHsyZM5Rt2zpiNDpbNGKJhDB633nAxPXVedssBf0j7rFfEiYRdB18DCfNCndpo0bqi3HkSN5UAI8yiYnqHl+qVLo0/1GjlPf+iy9MGmfzZ5uJvBFJn2V9cHY17ctau0dt3Aq5cXzm8RxRAJqTxqSjj1G1/VpqH5/HsxHzcF9ZBcoMhuKD8kZhHhuRFJ/EgucXEBUWxSs7X6FCkwoWj7V1q3pw+OQlVSnE8Qjq6b888H9pd927p77ozzwDzZvzZFwEzpozWy5tyVYBRMVH8fqq16lZsiZftv4yw36fzj4cmXqEhJgEXD0sCw2Li6vM00/voH37Ubz55hSefnopd+9WYs+eV9iz51Xu3PEya7yCxwPhDtR5xodaBc05UyMhqTBbD0Sz+qfnGHxPlTPxsMQvX6iQqrGUGyOBMrML5daXPXwAyaZQWZg6XP/CBWXcffttk8a4suuKGDDI2ndNKBGbjmUvL5Oxhcea1dzbJqTUZW7dWvJEXWYbYjQaZfng5WLAICfmnbDJmIMHq0toRp6gHdkg6ieTSUevlITGVC3WWkxtIa2mt8p21A/WfyAYkO2XM+89cX7NeTFgkMD1lpU8NxpFXnpJiTd3rojKGVgkIp1F5TFoItJBROaLiCk5ByIznpohv9f/3SJ5RFR70K++UjI1bixy0dK8vP79VQ8RB4HeDyAjgYEiHh4i3bunc3i+9JLaEZp9kkpCbIL8Vuc3+bnqzxL3IM5sGS5tvWR1lyerOXcuY2eWzz/PNf1Mbc3eX/aKAYNs/nKzzca8e1cFevj5OVp/GkWkhYhUlgw3ySwSGj/b9Jm4jHSRB3FZR+AcDDkoTiOc5PWVr2d5THxUvIxyHyXr3ltnkeQjRqiv38hMg5KuiIhBRKqIuh2UFJF3RSW5ZU7cgzgZ5TZK1n+43iJ5UrNypSoDUry4yJo1Fgzw44/qw93MLlvaPugKIB2pMzqDg1PtOHVKRNNEPvrIpHG2GraKAYOcX3M++4MzkyPJKD9X/VlmdZpl0fk25WG9GWMyDx/NawSuD5QRTiNkXo95YkyybZhTSgb5uHE2HdZMVor6uWTS0euTT9R3+2TaNo0bAjc8tMlJfGK8NJzUUCr8WEHuxdx76OwzO8yUiXUnmi31nDnq2g0cmF30WaKIrBeRXiLiKuqzPiYi0yR9COm5lefEgEGCNgWZLU9mBAaq2m6aJmIwmFkPats29QEt0h7WoyuAdPz5p/r0f/yRbkevXioW8lb24Wxhp8JkpOtIWdxvsVWybP5ys4xwGiERIRFWjWNTgoNV9dOU7uzFi4u89ZbIsWOOlsxibp+7Ld8W+1YmNZxk0WotO4xGVRgvTQ2pHMUoIo1FpJpkqLNz44ZK8OiXsZVhVHyUuI50lY83fJxhn4jI2B1jBQOy7MyybCXY8+MeMWAwK5R1504RNzdljXxIG+JMCBORn0SFjiKq0NxgUeGvRln95moZ4zlGEmJttySLihIZNEj9JLp0EbljahrPvXvqpNGjbSaLOegKIBWhoep+9uST6bT4sWPqknz5ZbZjGJOMMv3x6fJdie8k8qZ1DWNvn7stBgyya9wuq8axC0lJIps3i/Ttq3IiQKRpU6U57z+kKXYuI/putEyoOUHGlRpnk7aKWREcnK6KbI6yWNRPZUbGXe++q5wUWSQ0tv6rtTSd0jTD9nO3z4n7KHd5YcELJkkQdipMDBjk0BTT2pNeuCBSsqQqI255wqFRRPaIyKsiUlDUNagnv/qMlLnd/rZ00KxnM6qvv6ur6hdtst+nRg1VZdIBZKUA8lBbDtvxzjsQHa2iPNM0Jvn6a9X9498iQFlzaPIhru25RsefOlKwjFnhBRkoWbMklR6rxPEZx5VWzk04OakQl7lzVUeX8eMhLk71SSxfHl5+GXbtsn1RHBtiTDSy+MXFhF8Kp/eS3jZoq5g1FSvCuHGwZQv89ZfdpsmEJGA4UAtVZiEVISGqk93AgVCzZqZnt/Nqx5HQI9yLvffvNqMYeW3la3i4ejChS7ZVWwAoVacURSoX+bdL2MO4exe6dlVfndWroeTDU2cegoYqWDAdCAWmcjewDOFBRnw6TQb6ABtRBQesR9Pg9ddh504VRdiypYn/135+ua8kRGZaIbe+bLECWL5cPcSOGZNux4EDaseoUdmOcT/4vowtPFZmtp9ps5T9lEzK64cdly1oMkajul6vv66ciiBSq5YKqXKQk+thrH13rRgwyOFp1ldFNYWkJGXOKFbMpDgCGzFX1M9kXsZdb7wh4uLy0BCW7Ze3CwZk+dnl/25LaZgy7fC0LM/LjOVDlss3Rb6RxPjELI+JixNp21Y9RW/PPKjIKvb/tj+51Mr7IlJC1LWpKiIjRMR2pZnDwv6rkDt0aDausnHj1IEOqK2CbgJSZrgKFUQaNMikhHHnzmotGpG9HX7ec/NktMdouRNouzo+0XeiZZTbKFnztmOcRBYTGSny118irVqpr5OLi0jPnsrZlZj1DSCnODL9iArRfcf8EF1rOHtWWcx69cqJ2RJEpIaINBBVViEVly+ru+z//vfQEWITYsVjtIe8s/YdEREJiQiRIt8UEf8Z/mY/5JxadEoMGOTKziuZ7jca/7Ojz7JT7MPcbnPlV59fk9/FilKM7UXdSjQR6SLKZGa9LygxUeSzz/6zjl6+nMWBmzapgzZssHpOc9EVgKgWhU5OmXR03LVLTA3fOL34tN3s9QteWCDjSo176JNTrubMGZEPP1QdkEDFPX/1lcilSw4R58rOKzLSdaTM7DAzR9tjppDSQdT+peD/EvUTWZpx1+DBShNdu5btKB1mdpAGvzcQEZEe83pIgdEF5MId873ZMeExMsJ5hGz+IvMw29Gj1XUZPtzsoU0iITZBxniOkdVvZtbj4KKIfCUiFUVds9Ii8qGInLF63mXLlP+nRAmR9ZlFnt69qz74N99YPZe55HsFsHOn+rTvvZfJznbtVAx8Nn0MY8Jj5IdyP8gfjf+wyw3l7IqzYsAgZ5fn8dZacXEqJrJLFxUzp2kiHTqIzJ9vbpiHxYRfDpdxpcfJhJoT7Np452HEx4s0bKhWnffu2WuWOBHxFpEmopyhqUhJaHznHZNGSon2+ePgH4IB+W7Xd9mflAV/PvGnTG6SsW3avHnqd9ivn/2c5EGbglQnvpUPq+CbKCKrRaSniLiIusU8ISJ/i4jlQR3nz6squpqmrMkZQkW9vXNqWZiGfK0AYmKUidrLS1ks0rB5s7oMv/yS7TgrXlshI5xGSMgh+7SCSoxPlHGlx8n85+fbZXyHcOWKyvCpWlVd55IlVUTKCdtk4GZG3IM4meQ7Sb4p+o2F1Sltx4EDatWZjQXGCv4Q9fPI5GnXjIRGEZF91/YJBsRphJM0/qOxJCRZHj65fbRqh5o6Qm7PHrUYadXKvmklGz7aICNdR5oR6ntDVOP6mqKuZWEReV1EDkoGpWoCkZEq8RdUSk14eKqdL7ygQqtzmHytAL78Un3SDMsyo1Hk8cdFKlbM9ht5eftlMWCQ9R9Yn1X4MNa+s1ZGuY2S6DuOeWq1G0lJyvbZu7eySZdFZGVpkbO2VXbGJKPM6zFPRjiNkAvrHBKMn4H331ffvx02b88cIyKVRPXZTXejMjOhUUQkISlBCo8tLM4jnOXwdesc5iEHQ8SAQY7POi4iqo906dKq3LoJKTZW8XuD32WGfyahsNliFJGdIjJIRDxE3XZ8RcT8/zijUbUDdnFR9/t/02fGjlVfBls1kTCRrBRAvggDjYlR0Yod03fYW78e9uxRLa4KZF0KNzE2kZWvraSYVzHajmhrR0nBd5AvSfFJnJx/0q7z5DhOTtChA8yfr8JJ1z8O3W5BlT6w6zWbTbPNsI2zS8/S4YcOVO9U3WbjWsPIkeDlBUOHQmysLUeegurQOgoVCpkKgwEKFlQt7kzExcmFL578gp87/YxfeT+rJCvvVx7PUp4ErQ/i3j0V7pmYqMI9S5lbLdoMIkIiCDsR9tDuX1mjAU+gSlGHApOASFTHsl8wp2OZpqmWoTt2qP/zxx6DmTP5rzT00aMWyGcHMtMKufVljQ8ggy3OaFQuey8vZbN+CFu+2mJVkStzMBqN8nv932XaY+aF3uUtLoqIq0h0T5GjRUUEke31RGKtSyw7OV91flr26jIbdtSyDevXqwe/L76w1YhRIlJWRNpKhqd/MxIa7cni/otlXOlx8pS/UVxdRbZutf+cKVFfNwJu2GjEeyLynKhbkGUdy27cUCGvIPJ/r0RLLG4qZDoHIT+vACBdwhfAihVw6BAMHw5ublmeF3YyjF3f7KLhgIb4dLTkqcI8NE3Dd5AvwfuCuX3utt3ncwyjACfwGA/1b8C25tD6FFyoANf3WzRi6JFQlr28jMqtKtP1965o1hTmtwMdO8KgQaoPS0CALUb8HbhJpk//w4dDsWLwwQe2mMhifDpVJ/pWNKe3hDJ1KrRta/85A9cFUrhCYcrUL2OjEYuieip/CywEWgBnzRqhbFnVT/ujj2DSXx60dtvHtR2XbCSfdeQbBZAGo1H9SGrUgAEDsj4sycjKoSspUKwAnX7qlGPiNejfAM1JI2CWTe4UuYzzwEzgDaAiuBSAtvth34dQNQrcWsKR77MZIy0PQh8wr/s8CpYuSJ8lfXBxz51tLn78EYoXV/0nkixrQJbMA9QNqSPKZJGKgwfVw80HHygl4EDWnlcPTEPaBDJokP3nMyYaubjpIj6dfGz8AKABnwAbgFuojmWLzRrBxUVliC9aBGeMtfBbM4rNm20oooXkTwWwaJF6DDMY1P9MFhyadIjgfcF0+rkTnqUs6xhlCYXLF8anow8BswIQY+4tsWAZIwB34NO0mx/7Hm6vgfuu4PsxbOsMkn3qfmJsIvN7zCfmbgwvLn/R6rIc9qRkSVVJ4+BB1ZTLcsYDd1BP/+kYPlxN9M471kxgNYsWwWejCxJTvDxeSdmXhbAFIQdDiA2PpXpne/l+ngIOA/WAF4CPgESzRnj+eTj4xt+USQqlY0fh228dW0Ul/ymApCR1469bF/r0yfKw+9fus/mzzfh08qFB/wY5J18yDQc25P7V+1zefjnH57Yfp4B/UD1qM1mie3eBsldgfxVoux72V4T7V7McTURY+dpKQvaH0GNWD8o1KmcvwW1Gnz7KIfrFF3DJIivAPeAH4BlUD91U7N4N69bBJ59A4cLWimox+/erhXXLluA/1IfgvdeIvW9T73emBK4LRHPSqNa+mh1nSWlv/gbq/6EDyhRnOrU6e7OfFvRqc4vPPoMePeD+fdtLagr5TwH88w+cOaNCM5wzb90oIqx5Yw1iFLpOcow9ufZztXEv4s7xGcdzfG778TWqP+1HWR9SqBy0vAQ7ekCTG3C3OpzPfLm954c9BMwKoO2IttTpWcceAtscTVN12ZycVD0985/+fkIpgZEZd331lTI4v/mm1XJayuXL8Oyzqk7g8uVQq1t1JEm4tNn+Nu+gdUFUbFERjxKW9G00B3dgIsqUuR/wA/aYfrqfH4WI4p9uc/jlFxUZ1bQpnDhhD1kfTv5SAAkJ6um/USOldrPg9MLTnF91nnaj2lHcu3iOiZcaVw9X6vaqy+lFp4mPineIDLblGMpu+h6QTdlHzQlaL4Ezv4FnElR8AXanvamdX32eTZ9sol7verT+qrWdZLYPlSur3sEbNsDs2eaceRsVjvgC0Cjtri1bVHPizz8Hz5wzV6bm/n21uomLUze10qWh0mOVcC/iTuC6QLvOHX07mpCDIXY0/2TGAGAv4AG0AX7DpFDRsmWhYkW0I4d55x313xYZqUJF58yxq8AZyF8KYOZMCApST/8ZwoIUMeExrH17LeWblKfF2y1yWMC0+A7yJSEqgTNLzjhUDtswHCiOUgAm0vBN0I5AYBFo9Tts84X4SG6dvsXivosp37g83f/qnusifkzh//5PmUjefRfCwkw963tUXLoh7WYR9fRfqRK8ZrucCnNISIBeveD8eViyBOokL8icXZ2p1r4agesCVeapnbi46SII+HSyf6ReWnyBQ0AXlGnzJSAq+9NSlYZ+4gn1Z5Mm8NJL8NZbEJ9Dz3z5RwHExakbf/Pm0K1blodt/Ggj0bejeXbaszi5OPbyVGlVhWLexQiYmdejgfYDK4EPgWLmnVrGF+qGwnY/aBtA9H5v/un6N66ervRZ1gdXT1fbi5sDODnBtGnqye89k3TiDWAC0A/lhExFSkLjF188NKHRXojAsGEq1HHyZNU+IjU+nXyIuBbB7TP2C2sOXBeIRwkPKjStYLc5sqYYsAwYjfJxPQZcePgpTZrA2bPqC4AymW3erL4Lv/2mQmZDQuwocjL5RwFMnw5Xr8KoUcoQmwmXt13m6PSjtPygZa5wKGpOGr4Dfbm4+SL3rznIS2QThgOlgLctO93VE9ocJmnnuyz8sisR1yN4cRwUrVzUlkLmOHXrqnv23LnKZPJwvgXiUX6UVKQ8/Xt5wauv2kXO7PjxR5gyBT79NHMRUp7KA9fbxwwkRiFwXSA+HX1wcnbULc0J+AJYh8oibgosz/pwPz/1f3f8Px+fqyv89JNKlg8IUIds22ZXofOJAoiJgTFj1FqrQ4dMD0mISWDlayspXq04bb9um7PyPYSGAxqCwIk5DvAQ2YSdqPjpT1EOYMtZN689l3d488yYNVTq9xFs62ZSqGhu5tNPoV49ZRJ68CCro4KBP4CBQI20u0xMaLQXS5eqahO9eqmfWGYUq1qMUnVKmdQlzBJuBtwk6maUheUfbE1HVKhoTeA54DMyDRVNKQlx+HCGXb17w4EDUKIEtG8P339vv1DR/KEA/vhD1Z8ZPTrLp/8do3dw98Jduk3uZkezwhLAPHNOCZ8SVHmiSu5sF5ktAnwFlAP+z6qRDv1xiEO/H6Llhy3xfX0lHKwIbVfDvsoQEWwLYR2Cm5tqTRocrKKCbmYaUTgW1c5weNrNJiY02ouDB6F/f2VVnTEjS7caANU7V+fy9sskRCfYXI4UB3NOZOqbRlXUg89rqJVbZ1QCWSrKl1fO4CxaRNatq5RAjx5Kwb7wAkRE2F7S/KEAdu6Ep56CNm0y3X0z4CZ7xu2h0cuN7BhDvBsVvdEVMO9/suHAhtw+e5vrh67bQzA7sgUVM/05YHlkyuVtl1n71lpqPF2D9t+2h8IV4LGrsP0ZaHYdbvtA4ApbCZ3jtGwJn32mTEGVKkHPnrBmTUq28GVgGjAY8Ep7ookJjfbg6lUV7lm2rAr39Mgm8rJ65+okxSXZJa8laH0QZX3LUri843IfMlIAmAz8ifrt+6F8YclomloFZLICSKFwYViwAH74ATZtgmvX7CBmZgWCcuvL4mJwRmOWHTmSEpNkSrMpMq70OIm6/fCGMJYTKyJ1RBXv0kTkDbPOjrkXI6PcR2XR4Si3YhSRx0SksqjPbxl3g+7KdyW+k9/q/CYx9zIp2X30F5EwJ5EHiOx+2+J5cgNnzqjqzSkN1SpWFDl06FVJSnIXkXQdvRITRWrXFqlbN8dbb96/r5qeFCmiqk6bQnx0vIwuMNrmLU9jI2JlpMtI2fjJRpuOa1uOiGra4yoik+Tf4n1ffqmaRWTTiEpE5I6V3WfJ18XgNA2KZu4wPPDbAa4fvE7nXzvjWdJe8dNjgTPAX8A7qEJeu00+u0DRAtR+rjYn/zlJUrxVRWRykLXAPuBLVOKM+cRFxPHPM/8gIvRd0ZcCRTOJcGn0DiQdhIuF4fHxKlooIdoawR1G7dqqXkxwMCxeDJ07X8DXdwbjx/+PDh0qMW9eqnLSc+eqKJKHJDTag8RElc185oxagNSta9p5rh6ueLX1Imi9bf0Al7ZcwphozOH4f3NpjAoV7YAyhb4MRKsVgNFoUnXAEiXsI1n+UABZcO/KPbZ8sYXqXapT/8X6dprlJPAN0B8VKzwKZSMcAsSZPIrvQF9i7sZwYU024WW5ghTbfzXgFYtGMCYZWdJ/CbfP3ab3ot6UqP6QX0A5P6h9Hbb7QpujcLo83Dxm0by5ATc3ZQaaNm0ETk5uGI2fcuEC9O0LFSvCu28nceLzf7JNaLQ1IvD226raxKRJWcZTZIlPZx/unLtD+KVwm8kUuC4Qt0JuVH68ss3GtA8lUKHQI4BZQEtonpwQ+RAzkL3JtwpAkss9AHYs95CEutEXRWVwgoqEmYwqKTvW5JF8OvpQsGzBPFIaYhlwBOW0tMyhvuWLLZxfdZ7Ov3bG2987+xPcCkGbY7BnGFSPAJrA8d8smjt3cBqYi5PTW7z/fjkuXlSZwx06qJtvw+A1PBazhWl/Oj0kesi2/PKLmvujj1RzG3NJeUq31SpARAhaF4T3U944u+XcKshynFC/iTXANSj/DPQtkqUjOKckypecnHeSC2su4D/Gn2JVi9lplokox88vqDj4FDqh0si/Qa0QssfJxYkG/RtwfvV5om/nZhNHSrRKTdSqx3wCZgew+7vdNHm9Cc3eaGbeyY9PgOtLINoZ6r0F257Lo6GiXwMFSamblNJQbd6MOELKNuHnKj8T6VKMoUNVQMngwbB3r/3CBZcvVxWme/ZUZSwsoWTNkhTzKmazshB3L9zl3uV7Dsj+tZbOwGHQfGBuBLRYhXpYzHnypQKIvhPNunfWUbF5RZoPa579CRZxBRX90hmVvZmen1ArgyGY+p/faFAjjAlGTs7Lze0iF6CU2gjA/MiU4P3BrBiygqptqtJlQhfLVmY1ekDJIDhUHtouh71e8CDU/HEcxjFgEapsRrr+idOnUyrkOO9OrceJExr79inT0Pz58PjjUL++Sia6dSvjqJZy+DD066cKls2a9fBwz4ehaRo+nXy4tOWSTXxZKYokt7T+NA9vYDcc9IXXwiCpM6rEd86SLxXAxg83EhseyzNTn7FT5qDwX9z7H2To2ASoH/avqBXCRJNGLduwLGV9y3J8Zm41AyWi6tTUB3qbfXZEcATzn5tP4QqF6b2oN86uVizri1SGFsGwrQu0uAZhXnBxjeXj5Shfo8oLvJ92c0pC45NPQocOaBq0aKHyCEJDVWmJokXVk3rFiiqhaMMG5We0lGvX4JlnVB/fFSusrzNXvXN14h/Ec22v9TGNgesCKVmzJMWrOaZgo/UUgKtfqWdAbQcqVPRQjkqQ7xTAxc0XOfb3MR7/6HHKNixrp1n+QUXBjEU5fLOiL/A0aqVwxaSRfQf5cv3gdW6dseEjns2YA5xDlSo276uVEJ3AvOfmER8ZT98VfW3TgEdzgrZrIOAHKJIApbvC3g+tH9euHABWAB+QoW5SSkJjJuVMChdWZqA9e+DkSVWbZ8sW6NQJvL1hxAgVu28ODx6oslmRkapURTkbVEfx9vfGycXJajNQQkwCl7ddziXZv1bg5wfTgaUfoB4UW6HyPnKGfKUAEmISWPX6KkpUL2HHEsK3UaGeLYDs6rJrwKTkv/+HKaVkG/RrgOas5cJVQALqxu+HSoE3HRFhxeAVhB4Jpefcnjbs55pM4w8gcT9cKQQtf4RtzSDR/g1KLGM4qlx2uo5ekZHK+N6+fZYJjSnUq6fMQCEhyjRUu7ZSAF5e0KWLCt/MrtpkYiK8+CKcOgULFyrTki1wL+JO5VaVrS4LcXXnVRJjEnN5+KcJeHmpPqHrb6NKSLQDhqIS/2LsPn2+UgDbR2wnPCicblO64ephr3IP76EadkwDTDFhVEE5g9cBc7M9ulDZQlTvXJ2AWQEYk3KTc/Nv4CJKCZhnt985dicn553kqbFPUeuZWnaQDSjfDGpehx31oe0hOFkOwnJbfaVdwHpU/9l0Wa2//abqRo/KpA1kFri7KzPQ+vVw8aKqGXfypKrbU6kSfPihiufPjPfeU9nIv/2mVhG2xKeTDzeO3SDyRqTFYwSuD8TZ3RmvNl62E8wRaFqq0tAlgdWoEOo/UasBOzfSySw7LLe+LM4EFpHrR67LCOcRsnzwcovHyJ61okT9yszzEkVlzZYUkbBsjz45/6QYMEjQxiCzJbQPsaIyfh+Tf7McTeTM0jNiwCCL+y8Wo9G8cy1m1+siUYhcdxI5Piln5jSJtqKyxdNlht6/L1KihMjTT1s9Q2KiyNq1Is8/L+LiojKOW7US+esvkchIdcyvv6rt779v9XSZcv3IdTFgkGMzjlk8xsS6E2Vmh5k2lMqBfPyxiJubSFxcqo0rRaSYiBQXEeuzp8kiEzhnC4g4CGOikZVDV+JZypMO35uZvWIykSgzTm1UWVhzcEatGBqjHH+zHnp0rWdr4V5UtYu0b/9TU5kKXEM9tZj+9H8z4CZLXlpChWYVeGbqMznX2KXVH3C+Pbj3hTr/B3uWQNPeDqmm+R8hwDZUYEA6/8cvv8Dduyrr10qcnaFzZ/UKC1M9kqZPh1deUUleTz+tTD7du6usZHtQzrccBcsWJHBdIL4Dfc0+//7V+9w6fYvGgxvbQToH4OenbHKnTkHjlM/UDWUS6omqH/Y1amVgY6NNZloh/QsVy3gOCAQ+zWR/VWAzqtTlNqBS8vZ2qJi2lFcs8FzyvhSP4UnUncM1OzksXQHs+XGPGDDIyfknLTrfNN4RJeYuK8b4OnmMtdkeueK1FTLGc4zERlheZ8c2RIlIORFpLeY8/UfejJSfq/4sP1b4USJCIuwl3MO5d1lkXznJBYvb5FdVEUlX7+jOHVV0p0cPm3/8FIxGkV27RF5+WcTTU6Rp0/9WA/Zi6cCl8l3J7yQpMcnscw9NOSQGDBJ2KvvVcp7gwgW15Jo6NZOdUSIySFQNsf0WT4GltYA0TXNGxSl2AeoCfTVNS18B5Adgpog0RBmBv0lWLltFpJGINAL8gWhUcfgUBVAbaIBqqjnEFIVlCTeO3aBmt5rU7WVi4RKz2Q+MB95A2e0s5TOgDvA6akWRNY0GNSIhOoEzix3dLnISqlvVKEx9+o+PiuefZ/4hKiyKPsv6ULiCg6o4Fq0KzUNg1xx49xmo7aqqVzzXAOaMhIhjQFAOvk6gqkim4scfVTjOiBF2ugjKDN2qFfz1l8of2LULCha023SAKgsRcyeG0MPm52cErQuiSOUilKpTKvuD8wLVqkGRrDKCPVE1xA4BdshZykwrpH4BLYH1qd5/BnyW7phTQOXkvzUgIpNxXgPmZDHHe8CY7GSxxgeQEJNg8bkPJ05E6otIJRG5b4PxdovS9u889Cij0Sjjq4+Xv9v9bYM5LeWBiJQSkQ4mn5GUmCT/PPuPGDSDnFl2xm6SWcTt28oA3qCBeiLz8BAZNEhk5071mJzThIWJFCwo8uKLOT+3nYm6FSUGzSDbRmwz67zE+ET5psg3snyIPX15DqBtW5Hmze02PFZUA62IMvCmEJy8LTXHUcYqgB5AYU3TSqY75kVUgHwaNE1zRdVFWGeCLBbjUsBe7o5xKCvWJKCIDcZ7HLWSGE+a+uHp0DSNhgMbcnnrZe5duWeDeS1hPCrs1bTIFBFh3TvrOLfiHF3Gd6F299p2lc5sSpZUhvDjx1U3jgEDVFnOJ59UXc6//z6rji324bvvVPKXwZBzc+YQnqU8qdC0gtl1gUL2hxAXEZf3wz/T4+envncJtm+Y8zBs5VH4EGijadpRoA3Ko/VvrremaeVRpp71mZz7O7BDRHZmNrCmaa9pmnZI07RDt2yZ324TzqJufn1QThtb8Q1Kxw5B9YHNHN8ByoEWMNsRTePvAd+jPncLk87Y++NeDk48SMsPWtqxBIcN0DRo1kx1OA8NhT//VKmwH3+s4ieffx7Wrk3p2GIfQkNh4kSlhGrZKTTWwVTvXJ3gfcHEhJse7x64LhDNWaPaU7kh+MGGNGkCcXFZx+XaCVMUQAiQutZqpeRt/yIi10Wkp4g0JjkERkTupTqkN7BURNKoN03TvgZKkyHnPc3YU0SkqYg0LV26tAni5hRGVMJGQVTkhi0pjFpRnAS+y/KoYl7FqNqmKgEzA1JMaTnIzyglYFpkyqkFp9j40Ubq9qpLh3H2isSyA4UKqRCZXbvg9Gl45x3YsUOFy3h5qZaMly/bft6xY1U21vDh2R+bR6neuTpiFC5uumjyOYHrAqncsjIFimXSGyIv85AewfbEFAVwEKihaZq3pmluKFNOmv57mqaV0jQtZazPUFE9qelLOvOPpmlDUGUx+4rkxXKNk1GJOz8B9igp0Q11qUejmslkju9AX+6cv0PI/pAsj7E9d1AK4HlU6OrDubLzCksHLKVyq8r0mNkDzSmHwj1tTZ06qj9fSIhKp61fX/WZrlYNOnZUabdxpvd4yJKrV2HKFHj1VTX2I0rF5hUpUKyAyWUhosKiCD0cmvfLP2RGjRrqYSOHS0NnqwBEJBEYhjLfnAEWiMgpTdNGapr2bPJhbYFzmqadR90Nx6Scr2maF2oFsT3d0H8kH7tX07RjmqbloUedYFS2ZntgkB3n+RXVP2AoasWRkbov1MXFwyWHS0P8gIpSyj4y5fbZ28zrPo9i3sV4cfmLdvTF5CBubv+ZgS5fhq+/hnPnVO2EihVVGu1JKyq2jkn++Xz5pU3Eza04uThRrX01gtYFmbSCDdqo/AV5s/pnNjg5qRyAXLgCQFQqWk0R8RGRMcnbhovIiuS/F4lIjeRjhohIXKpzL4tIxfRP+SLikjxeo+SX9VkuOYKgnLSJqFWAPZ9my6CetHej9GVG3Iu4U6dHHU7OO0liXKIdZUnhJsr52xeo99AjI29GMqfLHJxdnem/pr8dW246kCpVlAK4eFHVXPD3V7b7Bg3gscdUiU5zOrZcvKh8Dq+9BpVze5cr6/Hp7MOD6w+4dSp7/17QuiA8S3lS3q98DkjmAPz84Ngx+/qW0pGvagHZhoWo1m6jUEHj9mYAqpfop6QNxvoP30G+xIbHcn7V+RyQ5ztUPt/XDz0qPiqef7qpWP++q/rm4ZK9JuLsrMxACxYoE9FPP6kbf0rHliFDYN++7Du2jBwJLi7w+ec5I7eDSXmaz84MJEYhcH0gPp188q4JMTuaNFFRX2fP5tiUugIwi7vAW0ATMlRrtBsaaqWRhFp5ZLyBeD/lTeEKhXOgXWQIKmhrIKrjV+YYE40sfnExoUdCeX7e81Rslj5q+BGndOn/zEB79qgu6vPmQcuWym/w889w+3bG886eVR1X3nxTKY18QJFKRShTv0y2CiD0aCjRt6IfvfDP1Pj5qX9z0A+gKwCz+BDlAJ2OJd2uLMcbteJYhVqBpMXJ2YkGLzUgcG0gUWFRdpRjLEoRZe2uERHWvLWG86vO02VCF/tV98wLaJq66U+frsI6p05Vhfvffx8qVFCKIXXHlhEjwMMDPvnEsXLnMD6dfbi68yrxkVmHPKcoCJ+Oj6ADOIXatdX/fw76AXQFYDKbUCnZHwPmF7CynreBpqgVyN0Me30H+GJMNHLiH3uVOL6CKvo2GKWQMmf3uN0c/uMwj3/8uPn9fB9lChf+zwx04oR6yt+0SdVarlZNtfGaP1+FmeaqcGf7U71TdZLik7i87XKWxwStD6K8X3kKlrFzjQpH4uwMjRrpK4DcRzSqPk8NVEU+R+CCqhh6F9UtKi1l6pehvF95AmbaKylsFOrrknVkyol/TrD5083Uf7E+7b9pbyc5HgFSzEDXryvTUI0aymdQpIhSBPmMKk9UwdXTlcD1mZuBYu/Hcm3PtUcz/DM9TZrA0aPW9fE0A10BmMTXqGYnU1F16xyFL2oF8jdqRZJu7yBfQo+EEnYyzMbzBibP+ToqDzAjl7dfZvnLy6nauird/+7+6DrqbIm7uzIDbdwIly7BwYNQooSjpcpxXAq44NXOK8suYZc2X0KS5NG2/6fg56e6v124kCPT6QogWw6jkr1eQ1W5cDRfoRywr6FWJv9Rv299nFyc7JATMAJwQ+X4ZeTW6VvMf24+xX2K02dZH1zcH4FY/5zGy0utBPIp1TtX527gXe4GZjRvBq4LxL2IO5Uey/zh45EihzOCdQXwUBJQ9XjK8rCSDDlLAdRK5BLpQzELli5IjadrEDA7AGOirZaQp1GVu4cBGbuCPwh9wJyn5+DsrmL9PYo7coWkk1dJebpPbwYSEQLXBVKtfTWcXU1psZrHqVNHrQxzyA+gK4CH8iOqj81EoJhDJUlLa5Q55idUnfD/aDiwIZGhkVzcbHp9lYdjQNU7+jjDnvhIFesffTuafqv7UcyrmI3m1MlvlKheguLVimcwA90+e5uIaxH4dMoH9n8AV1fw9dVXAI7nAurm1xNV4Tq38R1qZTIEtVJR1OxWkwLFC9jIGXwcFXb6DpC2+YYx0cjC3gu5cewGvRb0okKTCjaYTyc/49PZh0tbL6XJaP83/DO/KAD4r0l8DjiCdQWQKSmVPgsAvzlYlqwoikrKOo5aqShc3F2o/2J9ziw9Q1yEtYXJvk6eJ21kioiw+o3VBK4NpOukrtR4Ov/arnVsR/XO1UmISuDa7v8y3oPWBVGqTimKVS3mOMFymiZNICJClQWxM7oCyJQ/UbXrfgByc0bmc6iKnAbUikXhO8iXxJhETi86bcXYB4HlqJt/2jIOu77ZxZGpR3ji8ydo8loTK+bQ0fkP73beOLk6/fvUnxCdwOXtl/NH9E9qctARrCuADFxHZfy2RSU95XZ+Q4Wm/lcxtGLzipSsWdLK0hDDgZKkL3kRMDuALV9soUG/BviP9rdifB2dtLgVcqPqk1X/VQCXt18mKS4p/ymAevVUxdkccATrCiADbwFxwBTsW+nTVpRDrVS2o0pUqHaRvoN8ubLjCuGXwi0YczeqQ+fHpG5zeWnLJZa/uhyvtl48++ezaFpeuD46eQmfTj6EnQgjIiSCwHWBuBRwocqTVRwtVs7i5qaqyeorgJxmSfLLgMr6zSu8CrQDPkKtYKDhSw1Bg4BZljiDv0KVon7z3y1hJ8OY33M+JWuUpM9SPdZfxz6kPO0HbQgiaH0QXm29cPVwdbBUDiDFEWznTn+6AviXe6hY90Y8pENlLiWlYmgcagUDRasUxbudN8dnHjezXeQWYCvwOSr8Ex5cV7H+rp6u9F/b/9Frx6eTayjToAyFyhfi8B+HuXPuTv4o/5AZTZpAeLh92o2mQlcA//IJqtnJNCAvPnHUQK1cUlYxKicgPCica3sy7yOQEUE9/VdE5RlA3IM45nadS2x4LP1W96NolaK2FlxH5180TaN65+qEHFAtTvOd/T+FHCoNrSsAQNnPp6Ce/PNyVMsHqB69bwL3qPt8XVw9Xc0oDbEe2IMq+FaApIQkFvZayM0TN+m1sBflG+fmiCidR4WUm34xr2KUrFnSwdI4iAYNVGMgO/sBdAVADCqCphqm9LjN3aRUDL0FfIxbITfqPF+HU/NPkRCTkM25KU//XsCriAir/reKoPVBdJvcLf8+ienkONXaV0Nz1vDp7JN/Aw0KFFDRQPoKwN6MQsXQTwEehZ61fqiVzFRgG76DfIm7H8f5ldm1i1yBKivxFeDGjtE7OPbnMVp/1Rq/wX52lllH5z88SngwaOsg2o1s52hRHEuTJmoFYEdHcD5XAMeBccArwFMOlsWWGAAfYChebctSpFKRbHICjKi4/+rAQI7NOMa24dtoOKAhbUe0tbu0OjrpqfpkVQqWfoSbv5iCn59qHRocbLcp8rECSEQlepVCxdE/SniiVjSBODmPpuGAhgSuDyTyRmQWxy8CAgADFzddZeWQlXg/5c2z0/RYfx0dh5EDGcH5WAH8iqr1PwF4FJtw+KPyA77Hd6AbkiScmJtZu8gkVM2futwMaMf8nvMpVbsUvRf3xtktH5Tf1dHJrTRsCE5OugKwPRdRtu5ngRccLIs9+R4oRana71GxeYUsooHmAmeJCP6SOU/Pw72IO/3W9KNAUT3WX0fHoXh6Qt26dnUE50MFIKgYdxdUnf9H2cRRArXCOUzDgbe5efwmN47fSLU/ARhBXEQT5na9R1xEnIr1r6zH+uvo5Ar8/OzqCM6HCmAGqp/ud2TV3/bR4gWgO/Vf/B4n1/TtImeSlHCJBc/349bp2/Re1Jtyvhm7funo6DiIJk3g5k0IDbXL8PlMAdxEhUg+QUqm66OPBkzEs2QSNbuFcWLOieR2kXGIjGTl0Fe5uOkBz0x9Bp+O+TTtXkcnt2JnR3A+UwDvAFGoGPn89NErAt/hO3ATUTejCNoQBExn+4hqHJ9RiTaGNjR6uZGDZdTR0cmAry9omt38APmopONKYD4q8au2g2VxBK9R4+m5eJSM4fjMPUTeXMj2EW1p9Eoj2gxv42jhdHR0MqNQIahdW18BWEcE8AZQn8yam+cPnHB2m0L9vqc4uzSIVa89SbUOxek2uZse66+jk5tJKQ1tB/KJAvgMCEE1THFzsCyOpDaNBrUgKd6Z0nVj6L3odZxd9Vh/HZ1cTZMmEBKinME2Jp8oAG9UuefmjhbE4VRo+jF9VyYxYOMA3Iu4O1ocHR2d7HjiCejdG6KjbT60Zl6zEMfStGlTOXTokKPF0NHR0clTaJp2WESapt+eT1YAOjo6Ojrp0RWAjo6OTj5FVwA6Ojo6+RRdAejo6OjkU3QFoKOjo5NP0RWAjo6OTj5FVwA6Ojo6+RRdAejo6OjkU/JUIpimabeAKxaeXgq4bUNx8jr69fgP/VqkRb8eaXkUrkdVESmdfmOeUgDWoGnaocwy4fIr+vX4D/1apEW/Hml5lK+HbgLS0dHRyafoCkBHR0cnn5KfFMAURwuQy9Cvx3/o1yIt+vVIyyN7PfKND0BHR0dHJy35aQWgo6Ojo5MKXQHo6Ojo5FPyhQLQNK2zpmnnNE0L1DTtU0fL4yg0TausadpWTdNOa5p2StO0dxwtU25A0zRnTdOOapq2ytGyOBpN04ppmrZI07Szmqad0TStpaNlchSapr2X/Ds5qWnaP5qmFXC0TLbmkVcAmqY5AxOBLkBdoK+maXUdK5XDSAQ+EJG6wGPAm/n4WqTmHeCMo4XIJfwKrBOR2oAv+fS6aJpWEXgbaCoi9QFn4EXHSmV7HnkFgGoEHCgiF0UkHpgHdHewTA5BREJF5Ejy3w9QP+6KjpXKsWiaVgnoCkxztCyORtO0okBrYDqAiMSLyD2HCuVYXAAPTdNcAE/guoPlsTn5QQFUBK6leh9MPr/pAWia5gU0BvY7WBRH8wvwMWB0sBy5AW/gFvBXsklsmqZpBR0tlCMQkRDgB+AqEArcF5ENjpXK9uQHBaCTDk3TCgGLgXdFJMLR8jgKTdO6AWEictjRsuQSXAA/YJKINAaigHzpM9M0rTjKUuANVAAKapr2kmOlsj35QQGEAJVTva+UvC1fommaK+rmP0dEljhaHgfTCnhW07TLKNOgv6Zpsx0rkkMJBoJFJGVVuAilEPIj7YFLInJLRBKAJcDjDpbJ5uQHBXAQqKFpmremaW4oR84KB8vkEDRN01D23TMi8pOj5XE0IvKZiFQSES/U92KLiDxyT3mmIiI3gGuaptVK3vQUcNqBIjmSq8BjmqZ5Jv9unuIRdIi7OFoAeyMiiZqmDQPWozz5f4rIKQeL5ShaAQOAE5qmHUve9rmIrHGcSDq5jLeAOckPSxeBVxwsj0MQkf2api0CjqCi547yCJaE0EtB6Ojo6ORT8oMJSEdHR0cnE3QFoKOjo5NP0RWAjo6OTj5FVwA6Ojo6+RRdAejo6OjkU3QFoKOjo5NP0RWAjo6OTj7l/wEbFM7f9qYFhQAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "val_accuracy_16 = [0.693989098072052, 0.8672131299972534, 0.9147540926933289, 0.9502732157707214, 0.9497267603874207, 0.9633879661560059, 0.9677595496177673, 0.9704918265342712, 0.9721311330795288, 0.9748634099960327, 0.9754098653793335, 0.9721311330795288, 0.9743169546127319, 0.9748634099960327, 0.9754098653793335, 0.9748634099960327, 0.9765027165412903, 0.9754098653793335, 0.9754098653793335, 0.9786885380744934, 0.9732240438461304, 0.9754098653793335, 0.9737704992294312, 0.9775956273078918, 0.9721311330795288]\n",
        "val_accuracy_32 = [0.6907103657722473, 0.8098360896110535, 0.8748633861541748, 0.9147540926933289, 0.9442622661590576, 0.9595628380775452, 0.9683060050010681, 0.9710382223129272, 0.9721311330795288, 0.9721311330795288, 0.9726775884628296, 0.971584677696228, 0.9743169546127319, 0.9732240438461304, 0.9726775884628296, 0.9754098653793335, 0.9737704992294312, 0.9732240438461304, 0.9721311330795288, 0.971584677696228, 0.9743169546127319, 0.9770491719245911, 0.9775956273078918, 0.9704918265342712, 0.9721311330795288]\n",
        "val_accuracy_64 = [0.6710382699966431, 0.7273223996162415, 0.848633885383606, 0.8868852257728577, 0.922950804233551, 0.9437158703804016, 0.9551912546157837, 0.9639344215393066, 0.9677595496177673, 0.9704918265342712, 0.971584677696228, 0.9732240438461304, 0.9721311330795288, 0.9737704992294312, 0.9732240438461304, 0.9754098653793335, 0.9737704992294312, 0.9743169546127319, 0.9748634099960327, 0.9726775884628296, 0.9721311330795288, 0.9737704992294312, 0.9737704992294312, 0.9726775884628296, 0.9721311330795288]\n",
        "val_accuracy_128 = [0.3540983498096466, 0.6715847253799438, 0.7759562730789185, 0.8409836292266846, 0.897267758846283, 0.9409835934638977, 0.960109293460846, 0.965573787689209, 0.9704918265342712, 0.9699453711509705, 0.9693989157676697, 0.9688524603843689, 0.9693989157676697, 0.9683060050010681, 0.9704918265342712, 0.9726775884628296, 0.9704918265342712, 0.9732240438461304, 0.9721311330795288, 0.9721311330795288, 0.9748634099960327, 0.9737704992294312, 0.9732240438461304, 0.9721311330795288, 0.971584677696228]\n",
        "val_accuracy_256 = [0.7055, 0.7049, 0.7880,0.8508, 0.8885, 0.9115, 0.9279, 0.9464, 0.9541,0.9628,0.9634,0.9678,0.9672,0.9683,0.9721,0.9743, 0.9705,0.9721,0.9738,0.9738,0.9760,0.9710,0.9760,0.9760]\n",
        "plt.plot(range(10), val_accuracy_16[14:24], color='green')\n",
        "plt.plot(range(10), val_accuracy_32[14:24], color='red')\n",
        "plt.plot(range(10), val_accuracy_64[14:24], color='blue')\n",
        "plt.plot(range(10), val_accuracy_128[14:24], color='yellow')\n",
        "plt.plot(range(10), val_accuracy_256[14:24], color='purple')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt8ktA9Lx4wX"
      },
      "source": [
        "From the zoomed in graph above we see that the validation accuracy of 16 is mostly above the others so we will choose 16 as our batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aY-571GwAY6"
      },
      "outputs": [],
      "source": [
        "plot generalization error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "HPS7cATnQZo7",
        "outputId": "cf30fb4e-198d-42cc-c283-cd65ec166d06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.017517095431685448\n",
            "0.02278626710176468\n",
            "0.014285420998930931\n",
            "0.024105535820126534\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_list_16 = [1.1268876791000366, 0.41091328859329224, 0.26682642102241516, 0.18981273472309113, 0.1585627794265747, 0.12462582439184189, 0.10439462214708328, 0.09923368692398071, 0.0861586406826973, 0.074553482234478, 0.06842844933271408, 0.05831009894609451, 0.04693516343832016, 0.04028552398085594, 0.03476426377892494, 0.03636551648378372, 0.027629371732473373, 0.02270563319325447, 0.023278960958123207, 0.02576974220573902, 0.028956500813364983, 0.026662178337574005, 0.01205827109515667, 0.01900443248450756, 0.017517095431685448]\n",
        "loss_list_32 = [0.5778000950813293, 0.45349517464637756, 0.3365524709224701, 0.24430455267429352, 0.17990316450595856, 0.14272713661193848, 0.11536013334989548, 0.09248892962932587, 0.0844212993979454, 0.07556392252445221, 0.06297740340232849, 0.05454753339290619, 0.04521776735782623, 0.041906196624040604, 0.038973744958639145, 0.03417503088712692, 0.029203122481703758, 0.028163660317659378, 0.022288044914603233, 0.028432272374629974, 0.01426108367741108, 0.019080979749560356, 0.009802976623177528, 0.019739139825105667, 0.02278626710176468]\n",
        "loss_list_64 = [0.7336003184318542, 0.5498955845832825, 0.3969078063964844, 0.29476746916770935, 0.22255413234233856, 0.16873212158679962, 0.13528117537498474, 0.10923530161380768, 0.09243155270814896, 0.08499571681022644, 0.07158724963665009, 0.06311731785535812, 0.056249313056468964, 0.048009131103754044, 0.04591318592429161, 0.041753821074962616, 0.03545118495821953, 0.03043154627084732, 0.03077448345720768, 0.029365094378590584, 0.031014489009976387, 0.02227487601339817, 0.020090047270059586, 0.018755009397864342, 0.014285420998930931]\n",
        "loss_list_128 = [1.1001505851745605, 0.7088683247566223, 0.4939412474632263, 0.3607051968574524, 0.2672146260738373, 0.2029889076948166, 0.15222841501235962, 0.1293368935585022, 0.11396116018295288, 0.10045761615037918, 0.09055810421705246, 0.08380580693483353, 0.07322715222835541, 0.06939279288053513, 0.05898356810212135, 0.05217868089675903, 0.04889925196766853, 0.04909573867917061, 0.045657578855752945, 0.04147873446345329, 0.0313851535320282, 0.03186754882335663, 0.02699873223900795, 0.024873340502381325, 0.024105535820126534]\n",
        "loss_list_256 = []\n",
        "\n",
        "plt.plot(range(25), loss_list_16, color='green')\n",
        "plt.plot(range(25), loss_list_32, color='red')\n",
        "plt.plot(range(25), loss_list_64, color='blue')\n",
        "plt.plot(range(25), loss_list_128, color='black')\n",
        "print(loss_list_16[-1])\n",
        "print(loss_list_32[-1])\n",
        "print(loss_list_64[-1])\n",
        "print(loss_list_128[-1])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a-yWpfn5yORR"
      },
      "source": [
        "Looking at the graph of losses we will pick 10 epochs for our model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MGFKSnFtwc1c"
      },
      "source": [
        "start doing some inference on own tweets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o6CIl4OOxWxU"
      },
      "source": [
        "## Grid search for learning rate and batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e93EpiS2Ghe9"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "init_lr = 0.0001\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "# training and validation dataset\n",
        "\n",
        "train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "\n",
        "train_val_data.shuffle(buffer_size=len(list(train_val_data)))\n",
        "\n",
        "train_val_set_size = len(list(train_val_data))\n",
        "val_n = int(0.2*train_val_set_size)\n",
        "\n",
        "train_data = train_val_data.skip(val_n).batch(batch_size)\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_data = train_val_data.take(val_n).batch(batch_size)\n",
        "val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# test dataset\n",
        "\n",
        "test_data = tf.data.experimental.CsvDataset([\"test_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "test_data = test_data.batch(batch_size)\n",
        "test_data = test_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "epochs = 25\n",
        "list_dropouts = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "\n",
        "for dropout in list_dropouts:\n",
        "  print(f\"Current dropout is {dropout}\")\n",
        "  classifier = build_classifier_model(dropout)\n",
        "  classifier.compile(optimizer=optimizer,\n",
        "                          loss=loss,\n",
        "                          metrics=metrics)\n",
        "  history = classifier.fit(x=train_data, validation_data=val_data, epochs=epochs)\n",
        "  history_dict=history.history\n",
        "  print(f\"For dropout {dropout}\")\n",
        "  print(history_dict['loss'])\n",
        "  print(history_dict['val_loss'])\n",
        "  print(history_dict['binary_accuracy'])\n",
        "  print(history_dict['val_binary_accuracy'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KTQH5qIuI6Dx"
      },
      "source": [
        "Learning rate of 0.01 is way too high"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hUomh-NzxWUw",
        "outputId": "80476ec3-a4ef-4f8a-b60b-c203d7cb645e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "458/458 [==============================] - 28s 40ms/step - loss: 0.3924 - binary_accuracy: 0.8356 - val_loss: 0.1873 - val_binary_accuracy: 0.9175\n",
            "Epoch 2/25\n",
            "458/458 [==============================] - 18s 38ms/step - loss: 0.1679 - binary_accuracy: 0.9409 - val_loss: 0.1225 - val_binary_accuracy: 0.9617\n",
            "Epoch 3/25\n",
            "458/458 [==============================] - 18s 38ms/step - loss: 0.1129 - binary_accuracy: 0.9676 - val_loss: 0.1068 - val_binary_accuracy: 0.9721\n",
            "Epoch 4/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0920 - binary_accuracy: 0.9762 - val_loss: 0.1134 - val_binary_accuracy: 0.9765\n",
            "Epoch 5/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0777 - binary_accuracy: 0.9783 - val_loss: 0.1236 - val_binary_accuracy: 0.9760\n",
            "Epoch 6/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0638 - binary_accuracy: 0.9843 - val_loss: 0.1311 - val_binary_accuracy: 0.9732\n",
            "Epoch 7/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0525 - binary_accuracy: 0.9874 - val_loss: 0.1843 - val_binary_accuracy: 0.9639\n",
            "Epoch 8/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0477 - binary_accuracy: 0.9884 - val_loss: 0.1155 - val_binary_accuracy: 0.9743\n",
            "Epoch 9/25\n",
            "458/458 [==============================] - 18s 38ms/step - loss: 0.0394 - binary_accuracy: 0.9908 - val_loss: 0.2817 - val_binary_accuracy: 0.9563\n",
            "Epoch 10/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0360 - binary_accuracy: 0.9930 - val_loss: 0.2374 - val_binary_accuracy: 0.9650\n",
            "Epoch 11/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0406 - binary_accuracy: 0.9918 - val_loss: 0.1835 - val_binary_accuracy: 0.9661\n",
            "Epoch 12/25\n",
            "458/458 [==============================] - 18s 38ms/step - loss: 0.0329 - binary_accuracy: 0.9933 - val_loss: 0.2195 - val_binary_accuracy: 0.9672\n",
            "Epoch 13/25\n",
            "458/458 [==============================] - 18s 38ms/step - loss: 0.0322 - binary_accuracy: 0.9939 - val_loss: 0.1683 - val_binary_accuracy: 0.9678\n",
            "Epoch 14/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0216 - binary_accuracy: 0.9952 - val_loss: 0.3175 - val_binary_accuracy: 0.9590\n",
            "Epoch 15/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0265 - binary_accuracy: 0.9952 - val_loss: 0.2264 - val_binary_accuracy: 0.9721\n",
            "Epoch 16/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0288 - binary_accuracy: 0.9951 - val_loss: 0.2572 - val_binary_accuracy: 0.9634\n",
            "Epoch 17/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0532 - binary_accuracy: 0.9887 - val_loss: 0.2631 - val_binary_accuracy: 0.9568\n",
            "Epoch 18/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0753 - binary_accuracy: 0.9843 - val_loss: 0.2794 - val_binary_accuracy: 0.9514\n",
            "Epoch 19/25\n",
            "458/458 [==============================] - 18s 38ms/step - loss: 0.0922 - binary_accuracy: 0.9805 - val_loss: 0.1667 - val_binary_accuracy: 0.9568\n",
            "Epoch 20/25\n",
            "458/458 [==============================] - 18s 38ms/step - loss: 0.0871 - binary_accuracy: 0.9822 - val_loss: 0.1861 - val_binary_accuracy: 0.9617\n",
            "Epoch 21/25\n",
            "458/458 [==============================] - 18s 38ms/step - loss: 0.1217 - binary_accuracy: 0.9724 - val_loss: 0.2104 - val_binary_accuracy: 0.9410\n",
            "Epoch 22/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.1594 - binary_accuracy: 0.9622 - val_loss: 0.2021 - val_binary_accuracy: 0.9448\n",
            "Epoch 23/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.5803 - binary_accuracy: 0.7080 - val_loss: 0.6422 - val_binary_accuracy: 0.6710\n",
            "Epoch 24/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.6370 - binary_accuracy: 0.6704 - val_loss: 0.6384 - val_binary_accuracy: 0.6710\n",
            "Epoch 25/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.6367 - binary_accuracy: 0.6704 - val_loss: 0.6386 - val_binary_accuracy: 0.6710\n",
            "Learning rate 0.001, Batch size 16\n",
            "[0.39243805408477783, 0.1679077297449112, 0.11293626576662064, 0.09199552237987518, 0.07770299166440964, 0.06382960826158524, 0.05251798778772354, 0.047663573175668716, 0.03940749913454056, 0.036012202501297, 0.040574993938207626, 0.03288101777434349, 0.03219420835375786, 0.021573271602392197, 0.026504654437303543, 0.028827741742134094, 0.05315963551402092, 0.07528258115053177, 0.09219229221343994, 0.08708250522613525, 0.12170228362083435, 0.15940319001674652, 0.5802868604660034, 0.6369670629501343, 0.6366502642631531]\n",
            "[0.18733622133731842, 0.12247389554977417, 0.10678765177726746, 0.11344149708747864, 0.12364129722118378, 0.13109628856182098, 0.18430940806865692, 0.11548615992069244, 0.28171104192733765, 0.237406924366951, 0.1834666132926941, 0.21952536702156067, 0.16831380128860474, 0.3174852430820465, 0.2264299839735031, 0.25719544291496277, 0.26307210326194763, 0.2793731987476349, 0.16668149828910828, 0.1861148327589035, 0.2104155719280243, 0.2021135687828064, 0.6421863436698914, 0.6384482383728027, 0.6385961771011353]\n",
            "[0.8355738520622253, 0.940855085849762, 0.9676273465156555, 0.9762327671051025, 0.9782816767692566, 0.9842917919158936, 0.9874334335327148, 0.9883895516395569, 0.9908482432365417, 0.993033766746521, 0.9918044209480286, 0.9933069348335266, 0.9938532710075378, 0.9952192306518555, 0.9952192306518555, 0.9950826168060303, 0.9886627793312073, 0.9842917919158936, 0.9804671406745911, 0.9822428822517395, 0.9724081158638, 0.9621636271476746, 0.7079634070396423, 0.6704002022743225, 0.6704002022743225]\n",
            "[0.917486310005188, 0.9617486596107483, 0.9721311330795288, 0.9765027165412903, 0.9759562611579895, 0.9732240438461304, 0.9639344215393066, 0.9743169546127319, 0.9562841653823853, 0.9650273323059082, 0.9661202430725098, 0.9672130942344666, 0.9677595496177673, 0.9590163826942444, 0.9721311330795288, 0.9633879661560059, 0.956830620765686, 0.951366126537323, 0.956830620765686, 0.9617486596107483, 0.9409835934638977, 0.9448087215423584, 0.6710382699966431, 0.6710382699966431, 0.6710382699966431]\n",
            "Epoch 1/25\n",
            "229/229 [==============================] - 24s 70ms/step - loss: 0.5261 - binary_accuracy: 0.7206 - val_loss: 0.3071 - val_binary_accuracy: 0.8689\n",
            "Epoch 2/25\n",
            "229/229 [==============================] - 16s 68ms/step - loss: 0.2598 - binary_accuracy: 0.8947 - val_loss: 0.1570 - val_binary_accuracy: 0.9443\n",
            "Epoch 3/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.1498 - binary_accuracy: 0.9480 - val_loss: 0.1052 - val_binary_accuracy: 0.9672\n",
            "Epoch 4/25\n",
            "229/229 [==============================] - 15s 68ms/step - loss: 0.1147 - binary_accuracy: 0.9628 - val_loss: 0.1062 - val_binary_accuracy: 0.9689\n",
            "Epoch 5/25\n",
            "229/229 [==============================] - 15s 68ms/step - loss: 0.0871 - binary_accuracy: 0.9720 - val_loss: 0.1083 - val_binary_accuracy: 0.9710\n",
            "Epoch 6/25\n",
            "229/229 [==============================] - 16s 68ms/step - loss: 0.0766 - binary_accuracy: 0.9757 - val_loss: 0.1075 - val_binary_accuracy: 0.9770\n",
            "Epoch 7/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0550 - binary_accuracy: 0.9839 - val_loss: 0.1100 - val_binary_accuracy: 0.9716\n",
            "Epoch 8/25\n",
            "229/229 [==============================] - 16s 68ms/step - loss: 0.0466 - binary_accuracy: 0.9851 - val_loss: 0.1061 - val_binary_accuracy: 0.9754\n",
            "Epoch 9/25\n",
            "229/229 [==============================] - 15s 68ms/step - loss: 0.0419 - binary_accuracy: 0.9887 - val_loss: 0.1498 - val_binary_accuracy: 0.9727\n",
            "Epoch 10/25\n",
            "229/229 [==============================] - 15s 68ms/step - loss: 0.0368 - binary_accuracy: 0.9914 - val_loss: 0.1462 - val_binary_accuracy: 0.9727\n",
            "Epoch 11/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0322 - binary_accuracy: 0.9910 - val_loss: 0.1626 - val_binary_accuracy: 0.9721\n",
            "Epoch 12/25\n",
            "229/229 [==============================] - 16s 68ms/step - loss: 0.0373 - binary_accuracy: 0.9900 - val_loss: 0.1591 - val_binary_accuracy: 0.9732\n",
            "Epoch 13/25\n",
            "229/229 [==============================] - 16s 68ms/step - loss: 0.0300 - binary_accuracy: 0.9934 - val_loss: 0.2413 - val_binary_accuracy: 0.9596\n",
            "Epoch 14/25\n",
            "229/229 [==============================] - 15s 68ms/step - loss: 0.0342 - binary_accuracy: 0.9922 - val_loss: 0.1651 - val_binary_accuracy: 0.9710\n",
            "Epoch 15/25\n",
            "229/229 [==============================] - 15s 68ms/step - loss: 0.0204 - binary_accuracy: 0.9956 - val_loss: 0.1627 - val_binary_accuracy: 0.9754\n",
            "Epoch 16/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0192 - binary_accuracy: 0.9947 - val_loss: 0.2015 - val_binary_accuracy: 0.9672\n",
            "Epoch 17/25\n",
            "229/229 [==============================] - 15s 68ms/step - loss: 0.0269 - binary_accuracy: 0.9948 - val_loss: 0.1471 - val_binary_accuracy: 0.9743\n",
            "Epoch 18/25\n",
            "229/229 [==============================] - 16s 68ms/step - loss: 0.0201 - binary_accuracy: 0.9958 - val_loss: 0.1656 - val_binary_accuracy: 0.9721\n",
            "Epoch 19/25\n",
            "229/229 [==============================] - 16s 68ms/step - loss: 0.0201 - binary_accuracy: 0.9955 - val_loss: 0.1675 - val_binary_accuracy: 0.9754\n",
            "Epoch 20/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0208 - binary_accuracy: 0.9954 - val_loss: 0.1444 - val_binary_accuracy: 0.9760\n",
            "Epoch 21/25\n",
            "229/229 [==============================] - 16s 68ms/step - loss: 0.0182 - binary_accuracy: 0.9954 - val_loss: 0.2677 - val_binary_accuracy: 0.9617\n",
            "Epoch 22/25\n",
            "229/229 [==============================] - 16s 68ms/step - loss: 0.0189 - binary_accuracy: 0.9959 - val_loss: 0.2200 - val_binary_accuracy: 0.9689\n",
            "Epoch 23/25\n",
            "229/229 [==============================] - 16s 68ms/step - loss: 0.0111 - binary_accuracy: 0.9975 - val_loss: 0.2017 - val_binary_accuracy: 0.9694\n",
            "Epoch 24/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0224 - binary_accuracy: 0.9947 - val_loss: 0.2724 - val_binary_accuracy: 0.9546\n",
            "Epoch 25/25\n",
            "229/229 [==============================] - 15s 68ms/step - loss: 0.0221 - binary_accuracy: 0.9952 - val_loss: 0.1635 - val_binary_accuracy: 0.9672\n",
            "Learning rate 0.001, Batch size 32\n",
            "[0.526069700717926, 0.2598111033439636, 0.1497793197631836, 0.11469471454620361, 0.08705061674118042, 0.0766325518488884, 0.054971326142549515, 0.04660408943891525, 0.04194432497024536, 0.036809042096138, 0.03216753154993057, 0.03732345998287201, 0.030021406710147858, 0.03422441706061363, 0.0203698817640543, 0.0191985834389925, 0.026854077354073524, 0.02014143206179142, 0.020078064873814583, 0.020829906687140465, 0.018231486901640892, 0.018881570547819138, 0.011127186007797718, 0.02243565395474434, 0.02207784913480282]\n",
            "[0.3071155250072479, 0.15704165399074554, 0.10517364740371704, 0.10619459301233292, 0.10829105228185654, 0.10752163082361221, 0.1099950298666954, 0.1061108186841011, 0.14982706308364868, 0.14623673260211945, 0.16263632476329803, 0.15908585488796234, 0.24129034578800201, 0.1650753617286682, 0.16265322268009186, 0.20154692232608795, 0.14713925123214722, 0.16561080515384674, 0.16753733158111572, 0.1443857103586197, 0.2677234411239624, 0.21996034681797028, 0.2016870677471161, 0.2723776698112488, 0.16345436871051788]\n",
            "[0.7205770015716553, 0.8946865200996399, 0.9479579329490662, 0.962846577167511, 0.9719983339309692, 0.9756863713264465, 0.9838820099830627, 0.9851112961769104, 0.9886627793312073, 0.9913946390151978, 0.9909848570823669, 0.9900286793708801, 0.993443489074707, 0.9922142028808594, 0.9956290125846863, 0.9946728348731995, 0.9948094487190247, 0.9957656264305115, 0.9954923987388611, 0.9953558444976807, 0.9953558444976807, 0.9959021806716919, 0.9975413084030151, 0.9946728348731995, 0.9952192306518555]\n",
            "[0.868852436542511, 0.9442622661590576, 0.9672130942344666, 0.9688524603843689, 0.9710382223129272, 0.9770491719245911, 0.971584677696228, 0.9754098653793335, 0.9726775884628296, 0.9726775884628296, 0.9721311330795288, 0.9732240438461304, 0.9595628380775452, 0.9710382223129272, 0.9754098653793335, 0.9672130942344666, 0.9743169546127319, 0.9721311330795288, 0.9754098653793335, 0.9759562611579895, 0.9617486596107483, 0.9688524603843689, 0.9693989157676697, 0.9546447992324829, 0.9672130942344666]\n",
            "Epoch 1/25\n",
            "115/115 [==============================] - 29s 182ms/step - loss: 0.6506 - binary_accuracy: 0.7218 - val_loss: 0.4829 - val_binary_accuracy: 0.7290\n",
            "Epoch 2/25\n",
            "115/115 [==============================] - 20s 174ms/step - loss: 0.3916 - binary_accuracy: 0.8142 - val_loss: 0.2615 - val_binary_accuracy: 0.8896\n",
            "Epoch 3/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.2449 - binary_accuracy: 0.9033 - val_loss: 0.1536 - val_binary_accuracy: 0.9448\n",
            "Epoch 4/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.1633 - binary_accuracy: 0.9409 - val_loss: 0.1187 - val_binary_accuracy: 0.9585\n",
            "Epoch 5/25\n",
            "115/115 [==============================] - 20s 175ms/step - loss: 0.1233 - binary_accuracy: 0.9589 - val_loss: 0.1048 - val_binary_accuracy: 0.9672\n",
            "Epoch 6/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.1017 - binary_accuracy: 0.9643 - val_loss: 0.0925 - val_binary_accuracy: 0.9699\n",
            "Epoch 7/25\n",
            "115/115 [==============================] - 20s 175ms/step - loss: 0.0850 - binary_accuracy: 0.9715 - val_loss: 0.0937 - val_binary_accuracy: 0.9716\n",
            "Epoch 8/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.0724 - binary_accuracy: 0.9757 - val_loss: 0.1014 - val_binary_accuracy: 0.9732\n",
            "Epoch 9/25\n",
            "115/115 [==============================] - 20s 174ms/step - loss: 0.0636 - binary_accuracy: 0.9781 - val_loss: 0.0995 - val_binary_accuracy: 0.9705\n",
            "Epoch 10/25\n",
            "115/115 [==============================] - 20s 175ms/step - loss: 0.0517 - binary_accuracy: 0.9820 - val_loss: 0.1040 - val_binary_accuracy: 0.9727\n",
            "Epoch 11/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.0486 - binary_accuracy: 0.9854 - val_loss: 0.1237 - val_binary_accuracy: 0.9683\n",
            "Epoch 12/25\n",
            "115/115 [==============================] - 20s 174ms/step - loss: 0.0444 - binary_accuracy: 0.9872 - val_loss: 0.1061 - val_binary_accuracy: 0.9727\n",
            "Epoch 13/25\n",
            "115/115 [==============================] - 20s 177ms/step - loss: 0.0368 - binary_accuracy: 0.9895 - val_loss: 0.0866 - val_binary_accuracy: 0.9749\n",
            "Epoch 14/25\n",
            "115/115 [==============================] - 20s 175ms/step - loss: 0.0366 - binary_accuracy: 0.9892 - val_loss: 0.1095 - val_binary_accuracy: 0.9727\n",
            "Epoch 15/25\n",
            "115/115 [==============================] - 20s 175ms/step - loss: 0.0253 - binary_accuracy: 0.9922 - val_loss: 0.1037 - val_binary_accuracy: 0.9754\n",
            "Epoch 16/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.0279 - binary_accuracy: 0.9922 - val_loss: 0.1101 - val_binary_accuracy: 0.9732\n",
            "Epoch 17/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.0191 - binary_accuracy: 0.9936 - val_loss: 0.1028 - val_binary_accuracy: 0.9738\n",
            "Epoch 18/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.0227 - binary_accuracy: 0.9941 - val_loss: 0.1292 - val_binary_accuracy: 0.9727\n",
            "Epoch 19/25\n",
            "115/115 [==============================] - 20s 174ms/step - loss: 0.0208 - binary_accuracy: 0.9947 - val_loss: 0.1240 - val_binary_accuracy: 0.9765\n",
            "Epoch 20/25\n",
            "115/115 [==============================] - 20s 177ms/step - loss: 0.0187 - binary_accuracy: 0.9945 - val_loss: 0.1065 - val_binary_accuracy: 0.9803\n",
            "Epoch 21/25\n",
            "115/115 [==============================] - 20s 175ms/step - loss: 0.0231 - binary_accuracy: 0.9941 - val_loss: 0.1720 - val_binary_accuracy: 0.9689\n",
            "Epoch 22/25\n",
            "115/115 [==============================] - 20s 175ms/step - loss: 0.0153 - binary_accuracy: 0.9963 - val_loss: 0.1487 - val_binary_accuracy: 0.9716\n",
            "Epoch 23/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.0155 - binary_accuracy: 0.9954 - val_loss: 0.1080 - val_binary_accuracy: 0.9803\n",
            "Epoch 24/25\n",
            "115/115 [==============================] - 20s 175ms/step - loss: 0.0177 - binary_accuracy: 0.9951 - val_loss: 0.1517 - val_binary_accuracy: 0.9732\n",
            "Epoch 25/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.0153 - binary_accuracy: 0.9964 - val_loss: 0.1339 - val_binary_accuracy: 0.9765\n",
            "Learning rate 0.001, Batch size 64\n",
            "[0.6505687236785889, 0.3915615677833557, 0.24486654996871948, 0.16326908767223358, 0.12327322363853455, 0.1016739010810852, 0.0850357860326767, 0.07240737974643707, 0.06357716768980026, 0.051689453423023224, 0.04858092591166496, 0.044374775141477585, 0.036767419427633286, 0.036592692136764526, 0.025304866954684258, 0.027932485565543175, 0.019144508987665176, 0.022655166685581207, 0.02076450176537037, 0.018721094354987144, 0.023110877722501755, 0.01533255074173212, 0.01554880291223526, 0.017668699845671654, 0.015288138762116432]\n",
            "[0.48285651206970215, 0.2614785134792328, 0.1535826325416565, 0.11872445791959763, 0.10483104735612869, 0.09251821041107178, 0.09373407065868378, 0.10141657292842865, 0.09948275238275528, 0.10400612652301788, 0.12371740490198135, 0.10611695796251297, 0.08661337196826935, 0.10948804020881653, 0.10369234532117844, 0.11009823530912399, 0.10275683552026749, 0.12923677265644073, 0.12404055893421173, 0.10645206272602081, 0.17201553285121918, 0.14867007732391357, 0.10803083330392838, 0.15174955129623413, 0.13390758633613586]\n",
            "[0.7217790484428406, 0.8142330050468445, 0.9032918810844421, 0.940855085849762, 0.9588853716850281, 0.9643491506576538, 0.971451997756958, 0.9756863713264465, 0.9781450629234314, 0.9819696545600891, 0.9853845238685608, 0.9871602058410645, 0.9894822835922241, 0.9892091155052185, 0.9922142028808594, 0.9922142028808594, 0.9935801029205322, 0.9941264986991882, 0.9946728348731995, 0.994536280632019, 0.9941264986991882, 0.9963119626045227, 0.9953558444976807, 0.9950826168060303, 0.9964485764503479]\n",
            "[0.7289617657661438, 0.8896175026893616, 0.9448087215423584, 0.9584699273109436, 0.9672130942344666, 0.9699453711509705, 0.971584677696228, 0.9732240438461304, 0.9704918265342712, 0.9726775884628296, 0.9683060050010681, 0.9726775884628296, 0.9748634099960327, 0.9726775884628296, 0.9754098653793335, 0.9732240438461304, 0.9737704992294312, 0.9726775884628296, 0.9765027165412903, 0.980327844619751, 0.9688524603843689, 0.971584677696228, 0.980327844619751, 0.9732240438461304, 0.9765027165412903]\n",
            "Epoch 1/25\n",
            "58/58 [==============================] - 42s 572ms/step - loss: 0.6515 - binary_accuracy: 0.7316 - val_loss: 0.5634 - val_binary_accuracy: 0.6716\n",
            "Epoch 2/25\n",
            "58/58 [==============================] - 33s 566ms/step - loss: 0.5209 - binary_accuracy: 0.7066 - val_loss: 0.4175 - val_binary_accuracy: 0.7995\n",
            "Epoch 3/25\n",
            "58/58 [==============================] - 32s 561ms/step - loss: 0.3978 - binary_accuracy: 0.8148 - val_loss: 0.3149 - val_binary_accuracy: 0.8623\n",
            "Epoch 4/25\n",
            "58/58 [==============================] - 33s 563ms/step - loss: 0.3126 - binary_accuracy: 0.8678 - val_loss: 0.2450 - val_binary_accuracy: 0.9000\n",
            "Epoch 5/25\n",
            "58/58 [==============================] - 32s 561ms/step - loss: 0.2434 - binary_accuracy: 0.9008 - val_loss: 0.1905 - val_binary_accuracy: 0.9246\n",
            "Epoch 6/25\n",
            "58/58 [==============================] - 33s 564ms/step - loss: 0.1933 - binary_accuracy: 0.9277 - val_loss: 0.1536 - val_binary_accuracy: 0.9432\n",
            "Epoch 7/25\n",
            "58/58 [==============================] - 32s 561ms/step - loss: 0.1532 - binary_accuracy: 0.9448 - val_loss: 0.1294 - val_binary_accuracy: 0.9536\n",
            "Epoch 8/25\n",
            "58/58 [==============================] - 33s 566ms/step - loss: 0.1251 - binary_accuracy: 0.9557 - val_loss: 0.1056 - val_binary_accuracy: 0.9650\n",
            "Epoch 9/25\n",
            "58/58 [==============================] - 32s 561ms/step - loss: 0.1003 - binary_accuracy: 0.9665 - val_loss: 0.0930 - val_binary_accuracy: 0.9710\n",
            "Epoch 10/25\n",
            "58/58 [==============================] - 32s 561ms/step - loss: 0.0907 - binary_accuracy: 0.9701 - val_loss: 0.0891 - val_binary_accuracy: 0.9710\n",
            "Epoch 11/25\n",
            "58/58 [==============================] - 32s 562ms/step - loss: 0.0808 - binary_accuracy: 0.9730 - val_loss: 0.0890 - val_binary_accuracy: 0.9721\n",
            "Epoch 12/25\n",
            "58/58 [==============================] - 33s 565ms/step - loss: 0.0737 - binary_accuracy: 0.9758 - val_loss: 0.0970 - val_binary_accuracy: 0.9699\n",
            "Epoch 13/25\n",
            "58/58 [==============================] - 33s 565ms/step - loss: 0.0704 - binary_accuracy: 0.9754 - val_loss: 0.0842 - val_binary_accuracy: 0.9754\n",
            "Epoch 14/25\n",
            "58/58 [==============================] - 32s 561ms/step - loss: 0.0530 - binary_accuracy: 0.9846 - val_loss: 0.0902 - val_binary_accuracy: 0.9727\n",
            "Epoch 15/25\n",
            "58/58 [==============================] - 33s 564ms/step - loss: 0.0509 - binary_accuracy: 0.9828 - val_loss: 0.0856 - val_binary_accuracy: 0.9754\n",
            "Epoch 16/25\n",
            "58/58 [==============================] - 32s 561ms/step - loss: 0.0412 - binary_accuracy: 0.9857 - val_loss: 0.0934 - val_binary_accuracy: 0.9749\n",
            "Epoch 17/25\n",
            "58/58 [==============================] - 32s 562ms/step - loss: 0.0406 - binary_accuracy: 0.9874 - val_loss: 0.0978 - val_binary_accuracy: 0.9738\n",
            "Epoch 18/25\n",
            "58/58 [==============================] - 33s 563ms/step - loss: 0.0348 - binary_accuracy: 0.9888 - val_loss: 0.0913 - val_binary_accuracy: 0.9760\n",
            "Epoch 19/25\n",
            "58/58 [==============================] - 33s 565ms/step - loss: 0.0264 - binary_accuracy: 0.9918 - val_loss: 0.1012 - val_binary_accuracy: 0.9705\n",
            "Epoch 20/25\n",
            "58/58 [==============================] - 32s 561ms/step - loss: 0.0274 - binary_accuracy: 0.9902 - val_loss: 0.1026 - val_binary_accuracy: 0.9781\n",
            "Epoch 21/25\n",
            "58/58 [==============================] - 33s 572ms/step - loss: 0.0213 - binary_accuracy: 0.9940 - val_loss: 0.1111 - val_binary_accuracy: 0.9743\n",
            "Epoch 22/25\n",
            "58/58 [==============================] - 33s 568ms/step - loss: 0.0320 - binary_accuracy: 0.9893 - val_loss: 0.1093 - val_binary_accuracy: 0.9743\n",
            "Epoch 23/25\n",
            "58/58 [==============================] - 33s 572ms/step - loss: 0.0213 - binary_accuracy: 0.9933 - val_loss: 0.1027 - val_binary_accuracy: 0.9798\n",
            "Epoch 24/25\n",
            "58/58 [==============================] - 33s 568ms/step - loss: 0.0176 - binary_accuracy: 0.9943 - val_loss: 0.1032 - val_binary_accuracy: 0.9798\n",
            "Epoch 25/25\n",
            "58/58 [==============================] - 33s 573ms/step - loss: 0.0182 - binary_accuracy: 0.9952 - val_loss: 0.0996 - val_binary_accuracy: 0.9754\n",
            "Learning rate 0.001, Batch size 128\n",
            "[0.6514577865600586, 0.5209241509437561, 0.397845596075058, 0.31257951259613037, 0.24337269365787506, 0.19334615767002106, 0.1532057672739029, 0.1251438856124878, 0.10032393038272858, 0.0907110720872879, 0.0808076560497284, 0.07367207854986191, 0.07039798051118851, 0.05296199023723602, 0.05092562735080719, 0.04119550809264183, 0.04062755033373833, 0.03484983369708061, 0.026386545971035957, 0.027430720627307892, 0.021331554278731346, 0.03195589408278465, 0.021276233717799187, 0.017641862854361534, 0.018240073695778847]\n",
            "[0.5634165406227112, 0.4174675941467285, 0.3149482309818268, 0.2449965476989746, 0.19050106406211853, 0.15356330573558807, 0.1294173151254654, 0.10557611286640167, 0.09304581582546234, 0.08907288312911987, 0.088969886302948, 0.09700679779052734, 0.08419513702392578, 0.09017398953437805, 0.08563283830881119, 0.09341870993375778, 0.09775962680578232, 0.09132485091686249, 0.10115326195955276, 0.10256946831941605, 0.1111164540052414, 0.10926585644483566, 0.1027388721704483, 0.10320688784122467, 0.09964676201343536]\n",
            "[0.7316140532493591, 0.7065974473953247, 0.8147794008255005, 0.8677776455879211, 0.900833249092102, 0.9277421236038208, 0.9448162913322449, 0.9557437300682068, 0.9665346145629883, 0.9700860381126404, 0.972954511642456, 0.9758229851722717, 0.9754132032394409, 0.9845649600028992, 0.9827892184257507, 0.9856576919555664, 0.9874334335327148, 0.9887993335723877, 0.9918044209480286, 0.9901652932167053, 0.993989884853363, 0.9893457293510437, 0.9933069348335266, 0.9942630529403687, 0.9952192306518555]\n",
            "[0.6715847253799438, 0.7994535565376282, 0.8622950911521912, 0.8999999761581421, 0.9245901703834534, 0.9431694149971008, 0.9535518884658813, 0.9650273323059082, 0.9710382223129272, 0.9710382223129272, 0.9721311330795288, 0.9699453711509705, 0.9754098653793335, 0.9726775884628296, 0.9754098653793335, 0.9748634099960327, 0.9737704992294312, 0.9759562611579895, 0.9704918265342712, 0.9781420826911926, 0.9743169546127319, 0.9743169546127319, 0.979781448841095, 0.979781448841095, 0.9754098653793335]\n",
            "Epoch 1/25\n",
            "29/29 [==============================] - 70s 2s/step - loss: 0.6185 - binary_accuracy: 0.7317 - val_loss: 0.6078 - val_binary_accuracy: 0.6716\n",
            "Epoch 2/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.5832 - binary_accuracy: 0.6741 - val_loss: 0.5435 - val_binary_accuracy: 0.6760\n",
            "Epoch 3/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.5218 - binary_accuracy: 0.6979 - val_loss: 0.4602 - val_binary_accuracy: 0.7415\n",
            "Epoch 4/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.4501 - binary_accuracy: 0.7616 - val_loss: 0.3804 - val_binary_accuracy: 0.8148\n",
            "Epoch 5/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.3763 - binary_accuracy: 0.8243 - val_loss: 0.3137 - val_binary_accuracy: 0.8645\n",
            "Epoch 6/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.3205 - binary_accuracy: 0.8607 - val_loss: 0.2612 - val_binary_accuracy: 0.8929\n",
            "Epoch 7/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.2757 - binary_accuracy: 0.8905 - val_loss: 0.2219 - val_binary_accuracy: 0.9131\n",
            "Epoch 8/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.2352 - binary_accuracy: 0.9071 - val_loss: 0.1852 - val_binary_accuracy: 0.9284\n",
            "Epoch 9/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.2009 - binary_accuracy: 0.9262 - val_loss: 0.1571 - val_binary_accuracy: 0.9437\n",
            "Epoch 10/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.1749 - binary_accuracy: 0.9361 - val_loss: 0.1373 - val_binary_accuracy: 0.9525\n",
            "Epoch 11/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.1535 - binary_accuracy: 0.9439 - val_loss: 0.1271 - val_binary_accuracy: 0.9541\n",
            "Epoch 12/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.1366 - binary_accuracy: 0.9516 - val_loss: 0.1128 - val_binary_accuracy: 0.9601\n",
            "Epoch 13/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.1217 - binary_accuracy: 0.9581 - val_loss: 0.1045 - val_binary_accuracy: 0.9634\n",
            "Epoch 14/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.1116 - binary_accuracy: 0.9620 - val_loss: 0.0998 - val_binary_accuracy: 0.9650\n",
            "Epoch 15/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.1017 - binary_accuracy: 0.9663 - val_loss: 0.1006 - val_binary_accuracy: 0.9667\n",
            "Epoch 16/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.0970 - binary_accuracy: 0.9667 - val_loss: 0.0928 - val_binary_accuracy: 0.9689\n",
            "Epoch 17/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.0881 - binary_accuracy: 0.9694 - val_loss: 0.0880 - val_binary_accuracy: 0.9710\n",
            "Epoch 18/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.0765 - binary_accuracy: 0.9753 - val_loss: 0.0879 - val_binary_accuracy: 0.9699\n",
            "Epoch 19/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.0708 - binary_accuracy: 0.9771 - val_loss: 0.0871 - val_binary_accuracy: 0.9743\n",
            "Epoch 20/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.0618 - binary_accuracy: 0.9799 - val_loss: 0.0850 - val_binary_accuracy: 0.9732\n",
            "Epoch 21/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.0634 - binary_accuracy: 0.9794 - val_loss: 0.0852 - val_binary_accuracy: 0.9743\n",
            "Epoch 22/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.0542 - binary_accuracy: 0.9829 - val_loss: 0.0855 - val_binary_accuracy: 0.9732\n",
            "Epoch 23/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.0483 - binary_accuracy: 0.9835 - val_loss: 0.0871 - val_binary_accuracy: 0.9743\n",
            "Epoch 24/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.0463 - binary_accuracy: 0.9861 - val_loss: 0.0869 - val_binary_accuracy: 0.9738\n",
            "Epoch 25/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.0429 - binary_accuracy: 0.9861 - val_loss: 0.0942 - val_binary_accuracy: 0.9727\n",
            "Learning rate 0.001, Batch size 256\n",
            "[0.6184707880020142, 0.5831573605537415, 0.5218116641044617, 0.4500863552093506, 0.37629592418670654, 0.3204934000968933, 0.27569156885147095, 0.23518352210521698, 0.20085227489471436, 0.17494340240955353, 0.15354853868484497, 0.136625736951828, 0.12168797105550766, 0.11164375394582748, 0.1017189621925354, 0.09701760858297348, 0.08814677596092224, 0.07653369754552841, 0.07080449163913727, 0.06184392794966698, 0.0634177029132843, 0.054197292774915695, 0.04828106611967087, 0.04628107696771622, 0.04291829094290733]\n",
            "[0.6078420281410217, 0.5435051321983337, 0.46017178893089294, 0.3804168105125427, 0.31373131275177, 0.2612314820289612, 0.22185271978378296, 0.18523810803890228, 0.15708521008491516, 0.13725678622722626, 0.12711386382579803, 0.11275166273117065, 0.10452493280172348, 0.09979382157325745, 0.10061978548765182, 0.09282013773918152, 0.08800969272851944, 0.0879269689321518, 0.08711574226617813, 0.08501863479614258, 0.08524263650178909, 0.08550702035427094, 0.08711054921150208, 0.08689839392900467, 0.09423776715993881]\n",
            "[0.7317233085632324, 0.6740882396697998, 0.6978554725646973, 0.7616446018218994, 0.8243409395217896, 0.8606747984886169, 0.8904521465301514, 0.9071165323257446, 0.9262396097183228, 0.9360743165016174, 0.9438601136207581, 0.9516459703445435, 0.9580658674240112, 0.9620270729064941, 0.9662614464759827, 0.9666712284088135, 0.969403088092804, 0.9752765893936157, 0.9770523309707642, 0.9799208045005798, 0.9793744087219238, 0.9829258322715759, 0.9834722280502319, 0.9860674738883972, 0.9860674738883972]\n",
            "[0.6715847253799438, 0.6759563088417053, 0.7415300607681274, 0.814754068851471, 0.8644808530807495, 0.8928961753845215, 0.9131147265434265, 0.9284152984619141, 0.9437158703804016, 0.9524590373039246, 0.9540983438491821, 0.960109293460846, 0.9633879661560059, 0.9650273323059082, 0.9666666388511658, 0.9688524603843689, 0.9710382223129272, 0.9699453711509705, 0.9743169546127319, 0.9732240438461304, 0.9743169546127319, 0.9732240438461304, 0.9743169546127319, 0.9737704992294312, 0.9726775884628296]\n",
            "Epoch 1/25\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-3d17224bbcde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m                             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                             metrics=metrics)\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mhistory_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Learning rate {rate}, Batch size {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    961\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 785\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    786\u001b[0m             *args, **kwds))\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2981\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2982\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2983\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2984\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_call_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3292\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3293\u001b[0m           self._function_cache.add(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   3294\u001b[0m                                    graph_function)\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3128\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3129\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3130\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3131\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3132\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1134\u001b[0m           \u001b[0;31m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m             return autograph.converted_call(\n\u001b[0m\u001b[1;32m   1137\u001b[0m                 \u001b[0moriginal_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1008\u001b[0m             run_step, jit_compile=True, experimental_relax_shapes=True)\n\u001b[1;32m   1009\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m       outputs = reduce_per_replica(\n\u001b[1;32m   1012\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1311\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1312\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2886\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2887\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2888\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2890\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3687\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3688\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3689\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3691\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_target_and_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[0;31m# Run backwards pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \"\"\"\n\u001b[0;32m--> 530\u001b[0;31m     grads_and_vars = self._compute_gradients(\n\u001b[0m\u001b[1;32m    531\u001b[0m         loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/gradients\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m       \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     self._assert_valid_dtypes([\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_get_gradients\u001b[0;34m(self, tape, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0;34m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1082\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backward_function\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backward_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m       \u001b[0mcall_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rewrite_forward_and_call_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_backward_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_rewrite_forward_and_call_backward\u001b[0;34m(self, op, *doutputs)\u001b[0m\n\u001b[1;32m    666\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rewrite_forward_and_call_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[0;34m\"\"\"Add outputs to the forward call and feed them to the grad function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m     \u001b[0mforward_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackwards_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbackwards_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mbackwards_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_backward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m     \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_forward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_function_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_construct_forward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    642\u001b[0m       backwards_graph = func_graph_module.FuncGraph(\n\u001b[1;32m    643\u001b[0m           _backward_name(self._func_graph.name))\n\u001b[0;32m--> 644\u001b[0;31m       func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    645\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackwards_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m           \u001b[0mpython_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_backprop_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backprop_function\u001b[0;34m(*grad_ys)\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backprop_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad_ys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         return gradients_util._GradientsHelper(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    636\u001b[0m             \u001b[0mtrainable_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;31m# If grad_fn was found, do not use SymbolicGradient even for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0m\u001b[1;32m    680\u001b[0m                                          lambda: grad_fn(op, *out_grads))\n\u001b[1;32m    681\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m   \u001b[0;31m# If the gradients are supposed to be compiled separately, we give them a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 680\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    681\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_rewrite_forward_and_call_backward\u001b[0;34m(self, op, *doutputs)\u001b[0m\n\u001b[1;32m    666\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rewrite_forward_and_call_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[0;34m\"\"\"Add outputs to the forward call and feed them to the grad function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m     \u001b[0mforward_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackwards_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbackwards_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mbackwards_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_backward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m     \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_forward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_function_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_construct_forward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    642\u001b[0m       backwards_graph = func_graph_module.FuncGraph(\n\u001b[1;32m    643\u001b[0m           _backward_name(self._func_graph.name))\n\u001b[0;32m--> 644\u001b[0;31m       func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    645\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackwards_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m           \u001b[0mpython_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_backprop_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backprop_function\u001b[0;34m(*grad_ys)\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backprop_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad_ys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         return gradients_util._GradientsHelper(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    636\u001b[0m             \u001b[0mtrainable_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;31m# If grad_fn was found, do not use SymbolicGradient even for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0m\u001b[1;32m    680\u001b[0m                                          lambda: grad_fn(op, *out_grads))\n\u001b[1;32m    681\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m   \u001b[0;31m# If the gradients are supposed to be compiled separately, we give them a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 680\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    681\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_rewrite_forward_and_call_backward\u001b[0;34m(self, op, *doutputs)\u001b[0m\n\u001b[1;32m    666\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rewrite_forward_and_call_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[0;34m\"\"\"Add outputs to the forward call and feed them to the grad function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m     \u001b[0mforward_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackwards_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbackwards_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mbackwards_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_backward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m     \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_forward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_function_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_construct_forward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    642\u001b[0m       backwards_graph = func_graph_module.FuncGraph(\n\u001b[1;32m    643\u001b[0m           _backward_name(self._func_graph.name))\n\u001b[0;32m--> 644\u001b[0;31m       func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    645\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackwards_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m           \u001b[0mpython_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_backprop_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backprop_function\u001b[0;34m(*grad_ys)\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backprop_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad_ys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         return gradients_util._GradientsHelper(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    636\u001b[0m             \u001b[0mtrainable_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;31m# If grad_fn was found, do not use SymbolicGradient even for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0m\u001b[1;32m    680\u001b[0m                                          lambda: grad_fn(op, *out_grads))\n\u001b[1;32m    681\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m   \u001b[0;31m# If the gradients are supposed to be compiled separately, we give them a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 680\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    681\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_rewrite_forward_and_call_backward\u001b[0;34m(self, op, *doutputs)\u001b[0m\n\u001b[1;32m    666\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rewrite_forward_and_call_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[0;34m\"\"\"Add outputs to the forward call and feed them to the grad function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m     \u001b[0mforward_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackwards_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbackwards_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mbackwards_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_backward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m     \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_forward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_function_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_construct_forward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    642\u001b[0m       backwards_graph = func_graph_module.FuncGraph(\n\u001b[1;32m    643\u001b[0m           _backward_name(self._func_graph.name))\n\u001b[0;32m--> 644\u001b[0;31m       func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    645\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackwards_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m           \u001b[0mpython_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_backprop_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backprop_function\u001b[0;34m(*grad_ys)\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backprop_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad_ys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         return gradients_util._GradientsHelper(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    636\u001b[0m             \u001b[0mtrainable_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;31m# If grad_fn was found, do not use SymbolicGradient even for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0m\u001b[1;32m    680\u001b[0m                                          lambda: grad_fn(op, *out_grads))\n\u001b[1;32m    681\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m   \u001b[0;31m# If the gradients are supposed to be compiled separately, we give them a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 680\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    681\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_AddGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[0mgx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m     \u001b[0mgx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mskip_input_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[0mgy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2309\u001b[0m   \"\"\"\n\u001b[1;32m   2310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[0m\u001b[1;32m   2312\u001b[0m                               _ReductionDims(input_tensor, axis))\n\u001b[1;32m   2313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum_with_dims\u001b[0;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[1;32m   2321\u001b[0m   return _may_reduce_to_scalar(\n\u001b[1;32m   2322\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2323\u001b[0;31m       gen_math_ops._sum(input_tensor, dims, keepdims, name=name))\n\u001b[0m\u001b[1;32m   2324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m  11206\u001b[0m     \u001b[0mkeep_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11207\u001b[0m   \u001b[0mkeep_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keep_dims\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11208\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m  11209\u001b[0m         \u001b[0;34m\"Sum\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11210\u001b[0m                name=name)\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    738\u001b[0m       \u001b[0;31m# Add Op to graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0m\u001b[1;32m    741\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m                                  attrs=attr_protos, op_def=op_def)\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    691\u001b[0m       \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m       \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         compute_device)\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3773\u001b[0m     \u001b[0;31m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3774\u001b[0m     \u001b[0;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3775\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3776\u001b[0m       ret = Operation(\n\u001b[1;32m   3777\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/lock_util.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/lock_util.py\u001b[0m in \u001b[0;36macquire\u001b[0;34m(self, group_id)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_group_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_another_group_active\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epochs = 25\n",
        "\n",
        "learning_rate_list = [0.001, 0.0005, 0.0001]  \n",
        "batch_size_list = [16, 32, 64, 128, 256]\n",
        "\n",
        "for rate in learning_rate_list:\n",
        "  for size in batch_size_list:\n",
        "    batch_size = size\n",
        "    init_lr = rate\n",
        "\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    # training and validation dataset\n",
        "    train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "\n",
        "    train_val_data.shuffle(buffer_size=len(list(train_val_data)), seed = 12345)\n",
        "\n",
        "    train_val_set_size = len(list(train_val_data))\n",
        "    val_n = int(0.2*train_val_set_size)\n",
        "\n",
        "    train_data = train_val_data.skip(val_n).batch(batch_size)\n",
        "    train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "    val_data = train_val_data.take(val_n).batch(batch_size)\n",
        "    val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    steps_per_epoch = train_val_set_size - val_n\n",
        "    num_train_steps = steps_per_epoch * epochs\n",
        "    num_warmup_steps = int(0.1*num_train_steps)\n",
        "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                              num_train_steps=num_train_steps,\n",
        "                                              num_warmup_steps=num_warmup_steps,\n",
        "                                              optimizer_type='adamw')\n",
        "\n",
        "    classifier = build_classifier_model(0)\n",
        "    classifier.compile(optimizer=optimizer,\n",
        "                            loss=loss,\n",
        "                            metrics=metrics)\n",
        "    history = classifier.fit(x=train_data, validation_data=val_data, epochs=epochs)\n",
        "    history_dict=history.history\n",
        "    print(f\"Learning rate {rate}, Batch size {size}\")\n",
        "    print(history_dict['loss'])\n",
        "    print(history_dict['val_loss'])\n",
        "    print(history_dict['binary_accuracy'])\n",
        "    print(history_dict['val_binary_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs-0hz1-vCoK",
        "outputId": "c19cd71e-61b5-4ace-a802-c3a1042fbe40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "458/458 [==============================] - 32s 41ms/step - loss: 0.4799 - binary_accuracy: 0.7586 - val_loss: 0.2506 - val_binary_accuracy: 0.8995\n",
            "Epoch 2/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.2231 - binary_accuracy: 0.9123 - val_loss: 0.1502 - val_binary_accuracy: 0.9519\n",
            "Epoch 3/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.1422 - binary_accuracy: 0.9534 - val_loss: 0.1295 - val_binary_accuracy: 0.9705\n",
            "Epoch 4/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.1121 - binary_accuracy: 0.9674 - val_loss: 0.1173 - val_binary_accuracy: 0.9699\n",
            "Epoch 5/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0891 - binary_accuracy: 0.9754 - val_loss: 0.1116 - val_binary_accuracy: 0.9749\n",
            "Epoch 6/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0754 - binary_accuracy: 0.9809 - val_loss: 0.1464 - val_binary_accuracy: 0.9721\n",
            "Epoch 7/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0634 - binary_accuracy: 0.9831 - val_loss: 0.1478 - val_binary_accuracy: 0.9721\n",
            "Epoch 8/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0522 - binary_accuracy: 0.9877 - val_loss: 0.1538 - val_binary_accuracy: 0.9716\n",
            "Epoch 9/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0466 - binary_accuracy: 0.9892 - val_loss: 0.1179 - val_binary_accuracy: 0.9787\n",
            "Epoch 10/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0416 - binary_accuracy: 0.9891 - val_loss: 0.1385 - val_binary_accuracy: 0.9749\n",
            "Epoch 11/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0379 - binary_accuracy: 0.9917 - val_loss: 0.1506 - val_binary_accuracy: 0.9732\n",
            "Epoch 12/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0339 - binary_accuracy: 0.9925 - val_loss: 0.1735 - val_binary_accuracy: 0.9738\n",
            "Epoch 13/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0266 - binary_accuracy: 0.9941 - val_loss: 0.1660 - val_binary_accuracy: 0.9727\n",
            "Epoch 14/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0347 - binary_accuracy: 0.9932 - val_loss: 0.1581 - val_binary_accuracy: 0.9716\n",
            "Epoch 15/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0301 - binary_accuracy: 0.9932 - val_loss: 0.2300 - val_binary_accuracy: 0.9672\n",
            "Epoch 16/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0333 - binary_accuracy: 0.9932 - val_loss: 0.1565 - val_binary_accuracy: 0.9732\n",
            "Epoch 17/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0174 - binary_accuracy: 0.9962 - val_loss: 0.2302 - val_binary_accuracy: 0.9694\n",
            "Epoch 18/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0188 - binary_accuracy: 0.9964 - val_loss: 0.2628 - val_binary_accuracy: 0.9656\n",
            "Epoch 19/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0283 - binary_accuracy: 0.9948 - val_loss: 0.1898 - val_binary_accuracy: 0.9710\n",
            "Epoch 20/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0240 - binary_accuracy: 0.9952 - val_loss: 0.1807 - val_binary_accuracy: 0.9738\n",
            "Epoch 21/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0146 - binary_accuracy: 0.9973 - val_loss: 0.2127 - val_binary_accuracy: 0.9678\n",
            "Epoch 22/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.0173 - binary_accuracy: 0.9967 - val_loss: 0.1815 - val_binary_accuracy: 0.9716\n",
            "Epoch 23/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0155 - binary_accuracy: 0.9964 - val_loss: 0.2961 - val_binary_accuracy: 0.9650\n",
            "Epoch 24/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.0160 - binary_accuracy: 0.9974 - val_loss: 0.2832 - val_binary_accuracy: 0.9585\n",
            "Epoch 25/25\n",
            "458/458 [==============================] - 18s 39ms/step - loss: 0.0271 - binary_accuracy: 0.9952 - val_loss: 0.2728 - val_binary_accuracy: 0.9601\n",
            "Learning rate 0.0005, Batch size 16\n",
            "[0.47993236780166626, 0.2231285274028778, 0.142234206199646, 0.11207100749015808, 0.08913278579711914, 0.07543811947107315, 0.06336501240730286, 0.05222389101982117, 0.046629417687654495, 0.04163915291428566, 0.03788808360695839, 0.0339348241686821, 0.026611369103193283, 0.03467671945691109, 0.03008589893579483, 0.03327391296625137, 0.0174233540892601, 0.018759526312351227, 0.028328074142336845, 0.024000249803066254, 0.014613613486289978, 0.017256051301956177, 0.015497534535825253, 0.016027577221393585, 0.027144530788064003]\n",
            "[0.25059422850608826, 0.15019147098064423, 0.12949353456497192, 0.11726914346218109, 0.11160805821418762, 0.14639492332935333, 0.14779908955097198, 0.15380330383777618, 0.11789150536060333, 0.1384962946176529, 0.15064384043216705, 0.17346599698066711, 0.16599877178668976, 0.1580541878938675, 0.22996726632118225, 0.15646344423294067, 0.2302306592464447, 0.2627519369125366, 0.18979990482330322, 0.18070068955421448, 0.21266767382621765, 0.18148751556873322, 0.2961196005344391, 0.2832111716270447, 0.27283045649528503]\n",
            "[0.7586395144462585, 0.91230708360672, 0.9534216523170471, 0.9673541784286499, 0.9754132032394409, 0.9808769226074219, 0.9830624461174011, 0.9877066016197205, 0.9892091155052185, 0.9890725016593933, 0.9916678071022034, 0.992487370967865, 0.9941264986991882, 0.9931703209877014, 0.9931703209877014, 0.9931703209877014, 0.9961754083633423, 0.9964485764503479, 0.9948094487190247, 0.9952192306518555, 0.9972681403160095, 0.9967217445373535, 0.9964485764503479, 0.9974047541618347, 0.9952192306518555]\n",
            "[0.8994535803794861, 0.9519125819206238, 0.9704918265342712, 0.9699453711509705, 0.9748634099960327, 0.9721311330795288, 0.9721311330795288, 0.971584677696228, 0.9786885380744934, 0.9748634099960327, 0.9732240438461304, 0.9737704992294312, 0.9726775884628296, 0.971584677696228, 0.9672130942344666, 0.9732240438461304, 0.9693989157676697, 0.965573787689209, 0.9710382223129272, 0.9737704992294312, 0.9677595496177673, 0.971584677696228, 0.9650273323059082, 0.9584699273109436, 0.960109293460846]\n",
            "Epoch 1/25\n",
            "229/229 [==============================] - 25s 72ms/step - loss: 0.8213 - binary_accuracy: 0.6264 - val_loss: 0.4293 - val_binary_accuracy: 0.7705\n",
            "Epoch 2/25\n",
            "229/229 [==============================] - 16s 68ms/step - loss: 0.3467 - binary_accuracy: 0.8493 - val_loss: 0.2335 - val_binary_accuracy: 0.9071\n",
            "Epoch 3/25\n",
            "229/229 [==============================] - 16s 68ms/step - loss: 0.2157 - binary_accuracy: 0.9145 - val_loss: 0.1310 - val_binary_accuracy: 0.9568\n",
            "Epoch 4/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.1433 - binary_accuracy: 0.9495 - val_loss: 0.1088 - val_binary_accuracy: 0.9656\n",
            "Epoch 5/25\n",
            "229/229 [==============================] - 16s 68ms/step - loss: 0.1108 - binary_accuracy: 0.9627 - val_loss: 0.0971 - val_binary_accuracy: 0.9694\n",
            "Epoch 6/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0945 - binary_accuracy: 0.9690 - val_loss: 0.0939 - val_binary_accuracy: 0.9716\n",
            "Epoch 7/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0765 - binary_accuracy: 0.9750 - val_loss: 0.1031 - val_binary_accuracy: 0.9743\n",
            "Epoch 8/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.0696 - binary_accuracy: 0.9791 - val_loss: 0.1032 - val_binary_accuracy: 0.9727\n",
            "Epoch 9/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0626 - binary_accuracy: 0.9818 - val_loss: 0.1261 - val_binary_accuracy: 0.9683\n",
            "Epoch 10/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0494 - binary_accuracy: 0.9862 - val_loss: 0.1119 - val_binary_accuracy: 0.9721\n",
            "Epoch 11/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0472 - binary_accuracy: 0.9861 - val_loss: 0.1262 - val_binary_accuracy: 0.9749\n",
            "Epoch 12/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0365 - binary_accuracy: 0.9904 - val_loss: 0.1220 - val_binary_accuracy: 0.9770\n",
            "Epoch 13/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0351 - binary_accuracy: 0.9908 - val_loss: 0.1178 - val_binary_accuracy: 0.9770\n",
            "Epoch 14/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0274 - binary_accuracy: 0.9917 - val_loss: 0.1196 - val_binary_accuracy: 0.9738\n",
            "Epoch 15/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0310 - binary_accuracy: 0.9919 - val_loss: 0.1469 - val_binary_accuracy: 0.9743\n",
            "Epoch 16/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0206 - binary_accuracy: 0.9949 - val_loss: 0.1419 - val_binary_accuracy: 0.9738\n",
            "Epoch 17/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.0264 - binary_accuracy: 0.9939 - val_loss: 0.1669 - val_binary_accuracy: 0.9743\n",
            "Epoch 18/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.0261 - binary_accuracy: 0.9934 - val_loss: 0.1744 - val_binary_accuracy: 0.9727\n",
            "Epoch 19/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0181 - binary_accuracy: 0.9951 - val_loss: 0.1860 - val_binary_accuracy: 0.9716\n",
            "Epoch 20/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0238 - binary_accuracy: 0.9955 - val_loss: 0.1408 - val_binary_accuracy: 0.9776\n",
            "Epoch 21/25\n",
            "229/229 [==============================] - 16s 68ms/step - loss: 0.0153 - binary_accuracy: 0.9970 - val_loss: 0.1558 - val_binary_accuracy: 0.9743\n",
            "Epoch 22/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0109 - binary_accuracy: 0.9974 - val_loss: 0.1740 - val_binary_accuracy: 0.9770\n",
            "Epoch 23/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.0182 - binary_accuracy: 0.9963 - val_loss: 0.1912 - val_binary_accuracy: 0.9732\n",
            "Epoch 24/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.0188 - binary_accuracy: 0.9956 - val_loss: 0.1937 - val_binary_accuracy: 0.9760\n",
            "Epoch 25/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0161 - binary_accuracy: 0.9960 - val_loss: 0.1936 - val_binary_accuracy: 0.9743\n",
            "Learning rate 0.0005, Batch size 32\n",
            "[0.8212554454803467, 0.3467101454734802, 0.21570241451263428, 0.14326883852481842, 0.11083827912807465, 0.09448272734880447, 0.0765099972486496, 0.0695667564868927, 0.06259678304195404, 0.04942919686436653, 0.04722417891025543, 0.03651819005608559, 0.03506167232990265, 0.027379734441637993, 0.030964553356170654, 0.0205635204911232, 0.026439767330884933, 0.026050424203276634, 0.018078893423080444, 0.023758457973599434, 0.015295174904167652, 0.010909789241850376, 0.018152521923184395, 0.018757835030555725, 0.01612558402121067]\n",
            "[0.4293326437473297, 0.23347635567188263, 0.1309940069913864, 0.1087750643491745, 0.09711185097694397, 0.09385786205530167, 0.10308629274368286, 0.10323991626501083, 0.1260715126991272, 0.11193998903036118, 0.12615202367305756, 0.1220393031835556, 0.11779538542032242, 0.11956787109375, 0.14688393473625183, 0.1419089287519455, 0.16688191890716553, 0.17443044483661652, 0.18601013720035553, 0.1407991498708725, 0.15578503906726837, 0.1739930510520935, 0.19117708504199982, 0.19374988973140717, 0.19358576834201813]\n",
            "[0.6263796091079712, 0.8493375182151794, 0.9144925475120544, 0.9494604468345642, 0.9627100229263306, 0.9689933061599731, 0.9750034213066101, 0.9791012406349182, 0.9818331003189087, 0.9862040877342224, 0.9860674738883972, 0.9904384613037109, 0.9908482432365417, 0.9916678071022034, 0.991940975189209, 0.9949460625648499, 0.9938532710075378, 0.993443489074707, 0.9950826168060303, 0.9954923987388611, 0.9969949722290039, 0.9974047541618347, 0.9963119626045227, 0.9956290125846863, 0.9960387945175171]\n",
            "[0.7704917788505554, 0.9071038365364075, 0.956830620765686, 0.965573787689209, 0.9693989157676697, 0.971584677696228, 0.9743169546127319, 0.9726775884628296, 0.9683060050010681, 0.9721311330795288, 0.9748634099960327, 0.9770491719245911, 0.9770491719245911, 0.9737704992294312, 0.9743169546127319, 0.9737704992294312, 0.9743169546127319, 0.9726775884628296, 0.971584677696228, 0.9775956273078918, 0.9743169546127319, 0.9770491719245911, 0.9732240438461304, 0.9759562611579895, 0.9743169546127319]\n",
            "Epoch 1/25\n",
            "115/115 [==============================] - 30s 180ms/step - loss: 0.6203 - binary_accuracy: 0.7312 - val_loss: 0.5106 - val_binary_accuracy: 0.6732\n",
            "Epoch 2/25\n",
            "115/115 [==============================] - 20s 177ms/step - loss: 0.4594 - binary_accuracy: 0.7448 - val_loss: 0.3590 - val_binary_accuracy: 0.8169\n",
            "Epoch 3/25\n",
            "115/115 [==============================] - 20s 175ms/step - loss: 0.3324 - binary_accuracy: 0.8487 - val_loss: 0.2585 - val_binary_accuracy: 0.8803\n",
            "Epoch 4/25\n",
            "115/115 [==============================] - 20s 175ms/step - loss: 0.2429 - binary_accuracy: 0.9025 - val_loss: 0.1859 - val_binary_accuracy: 0.9208\n",
            "Epoch 5/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.1805 - binary_accuracy: 0.9320 - val_loss: 0.1507 - val_binary_accuracy: 0.9421\n",
            "Epoch 6/25\n",
            "115/115 [==============================] - 20s 174ms/step - loss: 0.1373 - binary_accuracy: 0.9536 - val_loss: 0.1121 - val_binary_accuracy: 0.9607\n",
            "Epoch 7/25\n",
            "115/115 [==============================] - 20s 177ms/step - loss: 0.1147 - binary_accuracy: 0.9589 - val_loss: 0.0986 - val_binary_accuracy: 0.9628\n",
            "Epoch 8/25\n",
            "115/115 [==============================] - 20s 175ms/step - loss: 0.0920 - binary_accuracy: 0.9693 - val_loss: 0.1024 - val_binary_accuracy: 0.9678\n",
            "Epoch 9/25\n",
            "115/115 [==============================] - 20s 177ms/step - loss: 0.0810 - binary_accuracy: 0.9712 - val_loss: 0.0958 - val_binary_accuracy: 0.9667\n",
            "Epoch 10/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.0746 - binary_accuracy: 0.9743 - val_loss: 0.0934 - val_binary_accuracy: 0.9699\n",
            "Epoch 11/25\n",
            "115/115 [==============================] - 20s 174ms/step - loss: 0.0662 - binary_accuracy: 0.9786 - val_loss: 0.0928 - val_binary_accuracy: 0.9727\n",
            "Epoch 12/25\n",
            "115/115 [==============================] - 20s 177ms/step - loss: 0.0635 - binary_accuracy: 0.9787 - val_loss: 0.0879 - val_binary_accuracy: 0.9738\n",
            "Epoch 13/25\n",
            "115/115 [==============================] - 20s 174ms/step - loss: 0.0544 - binary_accuracy: 0.9817 - val_loss: 0.0937 - val_binary_accuracy: 0.9749\n",
            "Epoch 14/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.0446 - binary_accuracy: 0.9847 - val_loss: 0.1028 - val_binary_accuracy: 0.9710\n",
            "Epoch 15/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.0401 - binary_accuracy: 0.9862 - val_loss: 0.0964 - val_binary_accuracy: 0.9738\n",
            "Epoch 16/25\n",
            "115/115 [==============================] - 20s 175ms/step - loss: 0.0376 - binary_accuracy: 0.9887 - val_loss: 0.1072 - val_binary_accuracy: 0.9732\n",
            "Epoch 17/25\n",
            "115/115 [==============================] - 20s 177ms/step - loss: 0.0306 - binary_accuracy: 0.9899 - val_loss: 0.1211 - val_binary_accuracy: 0.9716\n",
            "Epoch 18/25\n",
            "115/115 [==============================] - 20s 174ms/step - loss: 0.0267 - binary_accuracy: 0.9930 - val_loss: 0.1286 - val_binary_accuracy: 0.9710\n",
            "Epoch 19/25\n",
            "115/115 [==============================] - 20s 177ms/step - loss: 0.0301 - binary_accuracy: 0.9907 - val_loss: 0.1274 - val_binary_accuracy: 0.9710\n",
            "Epoch 20/25\n",
            "115/115 [==============================] - 20s 175ms/step - loss: 0.0278 - binary_accuracy: 0.9924 - val_loss: 0.1157 - val_binary_accuracy: 0.9738\n",
            "Epoch 21/25\n",
            "115/115 [==============================] - 20s 175ms/step - loss: 0.0178 - binary_accuracy: 0.9951 - val_loss: 0.1376 - val_binary_accuracy: 0.9727\n",
            "Epoch 22/25\n",
            "115/115 [==============================] - 20s 177ms/step - loss: 0.0163 - binary_accuracy: 0.9952 - val_loss: 0.1335 - val_binary_accuracy: 0.9716\n",
            "Epoch 23/25\n",
            "115/115 [==============================] - 20s 174ms/step - loss: 0.0230 - binary_accuracy: 0.9941 - val_loss: 0.1483 - val_binary_accuracy: 0.9732\n",
            "Epoch 24/25\n",
            "115/115 [==============================] - 21s 179ms/step - loss: 0.0201 - binary_accuracy: 0.9947 - val_loss: 0.1243 - val_binary_accuracy: 0.9749\n",
            "Epoch 25/25\n",
            "115/115 [==============================] - 20s 175ms/step - loss: 0.0192 - binary_accuracy: 0.9954 - val_loss: 0.1344 - val_binary_accuracy: 0.9738\n",
            "Learning rate 0.0005, Batch size 64\n",
            "[0.6203454732894897, 0.45940324664115906, 0.33243003487586975, 0.2428947538137436, 0.18050476908683777, 0.1373341977596283, 0.11474485695362091, 0.09199702739715576, 0.08104023337364197, 0.0746365562081337, 0.06619162857532501, 0.06348009407520294, 0.054417409002780914, 0.044607385993003845, 0.040056485682725906, 0.037623535841703415, 0.03063206560909748, 0.026693182066082954, 0.030058743432164192, 0.02778300829231739, 0.017780305817723274, 0.016312476247549057, 0.023008892312645912, 0.020138217136263847, 0.01922416128218174]\n",
            "[0.5106355547904968, 0.3590294420719147, 0.2585146427154541, 0.1858944147825241, 0.15073569118976593, 0.11207382380962372, 0.09859377890825272, 0.10241075605154037, 0.0957537591457367, 0.09343445301055908, 0.0928146243095398, 0.08790253847837448, 0.09372927248477936, 0.10283512622117996, 0.09636514633893967, 0.10718108713626862, 0.12114723771810532, 0.12857669591903687, 0.12739011645317078, 0.11570790410041809, 0.13755418360233307, 0.1335461139678955, 0.14829908311367035, 0.12427227199077606, 0.13435925543308258]\n",
            "[0.7311769127845764, 0.744843602180481, 0.848654568195343, 0.9024723172187805, 0.9319764971733093, 0.9535582661628723, 0.9588853716850281, 0.9692664742469788, 0.9711788296699524, 0.9743204712867737, 0.9785548448562622, 0.9786914587020874, 0.9816964864730835, 0.9847015142440796, 0.9862040877342224, 0.9886627793312073, 0.9898920655250549, 0.993033766746521, 0.9907116293907166, 0.9923507571220398, 0.9950826168060303, 0.9952192306518555, 0.9941264986991882, 0.9946728348731995, 0.9953558444976807]\n",
            "[0.6732240319252014, 0.8169398903846741, 0.8803278803825378, 0.9207650423049927, 0.9420765042304993, 0.9606557488441467, 0.9628415107727051, 0.9677595496177673, 0.9666666388511658, 0.9699453711509705, 0.9726775884628296, 0.9737704992294312, 0.9748634099960327, 0.9710382223129272, 0.9737704992294312, 0.9732240438461304, 0.971584677696228, 0.9710382223129272, 0.9710382223129272, 0.9737704992294312, 0.9726775884628296, 0.971584677696228, 0.9732240438461304, 0.9748634099960327, 0.9737704992294312]\n",
            "Epoch 1/25\n",
            "58/58 [==============================] - 42s 575ms/step - loss: 0.6946 - binary_accuracy: 0.7089 - val_loss: 0.6557 - val_binary_accuracy: 0.6699\n",
            "Epoch 2/25\n",
            "58/58 [==============================] - 33s 565ms/step - loss: 0.6252 - binary_accuracy: 0.6699 - val_loss: 0.5581 - val_binary_accuracy: 0.6787\n",
            "Epoch 3/25\n",
            "58/58 [==============================] - 33s 567ms/step - loss: 0.5252 - binary_accuracy: 0.7152 - val_loss: 0.4391 - val_binary_accuracy: 0.7546\n",
            "Epoch 4/25\n",
            "58/58 [==============================] - 32s 561ms/step - loss: 0.4223 - binary_accuracy: 0.7988 - val_loss: 0.3476 - val_binary_accuracy: 0.8404\n",
            "Epoch 5/25\n",
            "58/58 [==============================] - 33s 566ms/step - loss: 0.3355 - binary_accuracy: 0.8571 - val_loss: 0.2705 - val_binary_accuracy: 0.8891\n",
            "Epoch 6/25\n",
            "58/58 [==============================] - 33s 565ms/step - loss: 0.2824 - binary_accuracy: 0.8857 - val_loss: 0.2306 - val_binary_accuracy: 0.9027\n",
            "Epoch 7/25\n",
            "58/58 [==============================] - 33s 563ms/step - loss: 0.2409 - binary_accuracy: 0.9044 - val_loss: 0.1831 - val_binary_accuracy: 0.9279\n",
            "Epoch 8/25\n",
            "58/58 [==============================] - 33s 564ms/step - loss: 0.2046 - binary_accuracy: 0.9236 - val_loss: 0.1532 - val_binary_accuracy: 0.9454\n",
            "Epoch 9/25\n",
            "58/58 [==============================] - 33s 566ms/step - loss: 0.1728 - binary_accuracy: 0.9399 - val_loss: 0.1378 - val_binary_accuracy: 0.9492\n",
            "Epoch 10/25\n",
            "58/58 [==============================] - 32s 562ms/step - loss: 0.1544 - binary_accuracy: 0.9450 - val_loss: 0.1232 - val_binary_accuracy: 0.9563\n",
            "Epoch 11/25\n",
            "58/58 [==============================] - 33s 565ms/step - loss: 0.1301 - binary_accuracy: 0.9560 - val_loss: 0.1142 - val_binary_accuracy: 0.9617\n",
            "Epoch 12/25\n",
            "58/58 [==============================] - 33s 563ms/step - loss: 0.1214 - binary_accuracy: 0.9581 - val_loss: 0.1028 - val_binary_accuracy: 0.9634\n",
            "Epoch 13/25\n",
            "58/58 [==============================] - 33s 563ms/step - loss: 0.1051 - binary_accuracy: 0.9649 - val_loss: 0.0993 - val_binary_accuracy: 0.9656\n",
            "Epoch 14/25\n",
            "58/58 [==============================] - 33s 566ms/step - loss: 0.0954 - binary_accuracy: 0.9671 - val_loss: 0.1021 - val_binary_accuracy: 0.9661\n",
            "Epoch 15/25\n",
            "58/58 [==============================] - 33s 564ms/step - loss: 0.0920 - binary_accuracy: 0.9676 - val_loss: 0.1039 - val_binary_accuracy: 0.9667\n",
            "Epoch 16/25\n",
            "58/58 [==============================] - 32s 559ms/step - loss: 0.0837 - binary_accuracy: 0.9746 - val_loss: 0.1085 - val_binary_accuracy: 0.9678\n",
            "Epoch 17/25\n",
            "58/58 [==============================] - 33s 563ms/step - loss: 0.0809 - binary_accuracy: 0.9736 - val_loss: 0.1022 - val_binary_accuracy: 0.9672\n",
            "Epoch 18/25\n",
            "58/58 [==============================] - 33s 568ms/step - loss: 0.0711 - binary_accuracy: 0.9758 - val_loss: 0.1039 - val_binary_accuracy: 0.9678\n",
            "Epoch 19/25\n",
            "58/58 [==============================] - 32s 562ms/step - loss: 0.0692 - binary_accuracy: 0.9781 - val_loss: 0.0927 - val_binary_accuracy: 0.9716\n",
            "Epoch 20/25\n",
            "58/58 [==============================] - 32s 562ms/step - loss: 0.0626 - binary_accuracy: 0.9790 - val_loss: 0.0883 - val_binary_accuracy: 0.9716\n",
            "Epoch 21/25\n",
            "58/58 [==============================] - 33s 567ms/step - loss: 0.0575 - binary_accuracy: 0.9816 - val_loss: 0.1027 - val_binary_accuracy: 0.9667\n",
            "Epoch 22/25\n",
            "58/58 [==============================] - 33s 563ms/step - loss: 0.0494 - binary_accuracy: 0.9832 - val_loss: 0.0876 - val_binary_accuracy: 0.9716\n",
            "Epoch 23/25\n",
            "58/58 [==============================] - 32s 561ms/step - loss: 0.0472 - binary_accuracy: 0.9851 - val_loss: 0.0982 - val_binary_accuracy: 0.9689\n",
            "Epoch 24/25\n",
            "58/58 [==============================] - 33s 564ms/step - loss: 0.0418 - binary_accuracy: 0.9873 - val_loss: 0.0976 - val_binary_accuracy: 0.9716\n",
            "Epoch 25/25\n",
            "58/58 [==============================] - 33s 563ms/step - loss: 0.0357 - binary_accuracy: 0.9881 - val_loss: 0.0973 - val_binary_accuracy: 0.9738\n",
            "Learning rate 0.0005, Batch size 128\n",
            "[0.6945895552635193, 0.6252259016036987, 0.5252342224121094, 0.42225876450538635, 0.3354509770870209, 0.28239861130714417, 0.24090158939361572, 0.204607754945755, 0.172751322388649, 0.15440568327903748, 0.13008639216423035, 0.12138748168945312, 0.1050940528512001, 0.09535928815603256, 0.09203745424747467, 0.0837007537484169, 0.08091988414525986, 0.07105038315057755, 0.06923193484544754, 0.06259991973638535, 0.057498764246702194, 0.04941980168223381, 0.04717540368437767, 0.041753288358449936, 0.03569865971803665]\n",
            "[0.6556649804115295, 0.558127760887146, 0.43909892439842224, 0.34756872057914734, 0.2705013155937195, 0.23061390221118927, 0.18313167989253998, 0.15318746864795685, 0.1378442496061325, 0.12315140664577484, 0.1142442375421524, 0.10283961147069931, 0.09930737316608429, 0.10211635380983353, 0.10388994961977005, 0.10850425064563751, 0.1021871492266655, 0.1039423868060112, 0.09271484613418579, 0.08832193166017532, 0.10271713882684708, 0.08760660886764526, 0.09824756532907486, 0.09761159121990204, 0.0972730964422226]\n",
            "[0.7088842988014221, 0.6698538661003113, 0.7152028679847717, 0.7987979650497437, 0.8571233153343201, 0.8856713771820068, 0.9043846726417542, 0.9236443042755127, 0.9398989081382751, 0.9449528455734253, 0.9560169577598572, 0.9580658674240112, 0.964895486831665, 0.9670810103416443, 0.9676273465156555, 0.9745936393737793, 0.9736374616622925, 0.9758229851722717, 0.9781450629234314, 0.978964626789093, 0.9815598726272583, 0.9831990003585815, 0.9851112961769104, 0.9872968196868896, 0.9881163835525513]\n",
            "[0.6699453592300415, 0.6786885261535645, 0.7546448111534119, 0.8404371738433838, 0.8890710473060608, 0.902732253074646, 0.9278688430786133, 0.9453551769256592, 0.9491803050041199, 0.9562841653823853, 0.9617486596107483, 0.9633879661560059, 0.965573787689209, 0.9661202430725098, 0.9666666388511658, 0.9677595496177673, 0.9672130942344666, 0.9677595496177673, 0.971584677696228, 0.971584677696228, 0.9666666388511658, 0.971584677696228, 0.9688524603843689, 0.971584677696228, 0.9737704992294312]\n",
            "Epoch 1/25\n",
            "29/29 [==============================] - 68s 2s/step - loss: 0.7350 - binary_accuracy: 0.7311 - val_loss: 0.6931 - val_binary_accuracy: 0.6710\n",
            "Epoch 2/25\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.6938 - binary_accuracy: 0.6704 - val_loss: 0.6270 - val_binary_accuracy: 0.6710\n",
            "Epoch 3/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.6194 - binary_accuracy: 0.6705 - val_loss: 0.5466 - val_binary_accuracy: 0.6732\n",
            "Epoch 4/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.5464 - binary_accuracy: 0.6806 - val_loss: 0.4798 - val_binary_accuracy: 0.7120\n",
            "Epoch 5/25\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.4848 - binary_accuracy: 0.7234 - val_loss: 0.4157 - val_binary_accuracy: 0.7754\n",
            "Epoch 6/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.4294 - binary_accuracy: 0.7830 - val_loss: 0.3606 - val_binary_accuracy: 0.8328\n",
            "Epoch 7/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.3824 - binary_accuracy: 0.8172 - val_loss: 0.3191 - val_binary_accuracy: 0.8579\n",
            "Epoch 8/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.3463 - binary_accuracy: 0.8413 - val_loss: 0.2877 - val_binary_accuracy: 0.8798\n",
            "Epoch 9/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.3098 - binary_accuracy: 0.8660 - val_loss: 0.2602 - val_binary_accuracy: 0.8945\n",
            "Epoch 10/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.2806 - binary_accuracy: 0.8861 - val_loss: 0.2364 - val_binary_accuracy: 0.9049\n",
            "Epoch 11/25\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2539 - binary_accuracy: 0.8962 - val_loss: 0.2152 - val_binary_accuracy: 0.9153\n",
            "Epoch 12/25\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2328 - binary_accuracy: 0.9040 - val_loss: 0.1972 - val_binary_accuracy: 0.9213\n",
            "Epoch 13/25\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2155 - binary_accuracy: 0.9153 - val_loss: 0.1802 - val_binary_accuracy: 0.9295\n",
            "Epoch 14/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.1967 - binary_accuracy: 0.9241 - val_loss: 0.1629 - val_binary_accuracy: 0.9350\n",
            "Epoch 15/25\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1776 - binary_accuracy: 0.9365 - val_loss: 0.1514 - val_binary_accuracy: 0.9410\n",
            "Epoch 16/25\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1595 - binary_accuracy: 0.9413 - val_loss: 0.1408 - val_binary_accuracy: 0.9475\n",
            "Epoch 17/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.1504 - binary_accuracy: 0.9448 - val_loss: 0.1322 - val_binary_accuracy: 0.9497\n",
            "Epoch 18/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.1412 - binary_accuracy: 0.9495 - val_loss: 0.1241 - val_binary_accuracy: 0.9574\n",
            "Epoch 19/25\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1292 - binary_accuracy: 0.9545 - val_loss: 0.1198 - val_binary_accuracy: 0.9607\n",
            "Epoch 20/25\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1221 - binary_accuracy: 0.9567 - val_loss: 0.1141 - val_binary_accuracy: 0.9634\n",
            "Epoch 21/25\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1143 - binary_accuracy: 0.9600 - val_loss: 0.1111 - val_binary_accuracy: 0.9639\n",
            "Epoch 22/25\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1041 - binary_accuracy: 0.9638 - val_loss: 0.1072 - val_binary_accuracy: 0.9667\n",
            "Epoch 23/25\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0964 - binary_accuracy: 0.9652 - val_loss: 0.1065 - val_binary_accuracy: 0.9650\n",
            "Epoch 24/25\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0908 - binary_accuracy: 0.9699 - val_loss: 0.1012 - val_binary_accuracy: 0.9656\n",
            "Epoch 25/25\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0846 - binary_accuracy: 0.9717 - val_loss: 0.0967 - val_binary_accuracy: 0.9672\n",
            "Learning rate 0.0005, Batch size 256\n",
            "[0.7350019216537476, 0.6937848925590515, 0.6194216012954712, 0.5463869571685791, 0.48478126525878906, 0.4294099509716034, 0.38243138790130615, 0.3462808132171631, 0.30976271629333496, 0.28063955903053284, 0.25386524200439453, 0.23279665410518646, 0.21554699540138245, 0.1966828554868698, 0.17763951420783997, 0.1595495641231537, 0.1503559798002243, 0.14117582142353058, 0.12918099761009216, 0.12211279571056366, 0.11433704197406769, 0.10410591214895248, 0.0963885709643364, 0.0908234491944313, 0.0846264436841011]\n",
            "[0.6930968761444092, 0.6270477771759033, 0.5465562343597412, 0.47978320717811584, 0.41565296053886414, 0.3606005012989044, 0.3190735876560211, 0.2877306342124939, 0.26023000478744507, 0.23641285300254822, 0.21520282328128815, 0.197203129529953, 0.1801624298095703, 0.16287441551685333, 0.1513875275850296, 0.14082051813602448, 0.1322038620710373, 0.12410370260477066, 0.11981851607561111, 0.11410320550203323, 0.1110992506146431, 0.107223279774189, 0.10647343844175339, 0.10115861147642136, 0.09669710695743561]\n",
            "[0.7310676574707031, 0.6704002022743225, 0.6705368161201477, 0.680644690990448, 0.7233984470367432, 0.782953143119812, 0.8172380924224854, 0.8412784934043884, 0.8660019040107727, 0.8860811591148376, 0.8961890339851379, 0.9039748907089233, 0.9153121113777161, 0.9240540862083435, 0.9364840984344482, 0.9412648677825928, 0.9448162913322449, 0.9494604468345642, 0.9545143842697144, 0.9566999077796936, 0.9599781632423401, 0.9638027548789978, 0.9651687145233154, 0.96994948387146, 0.9717251658439636]\n",
            "[0.6710382699966431, 0.6710382699966431, 0.6732240319252014, 0.7120218873023987, 0.7754098176956177, 0.8327868580818176, 0.8579235076904297, 0.8797814249992371, 0.8945355415344238, 0.9049180150032043, 0.9153005480766296, 0.9213114976882935, 0.9295082092285156, 0.9349727034568787, 0.9409835934638977, 0.9475409984588623, 0.9497267603874207, 0.9573770761489868, 0.9606557488441467, 0.9633879661560059, 0.9639344215393066, 0.9666666388511658, 0.9650273323059082, 0.965573787689209, 0.9672130942344666]\n",
            "Epoch 1/25\n",
            "458/458 [==============================] - 28s 42ms/step - loss: 0.6104 - binary_accuracy: 0.7318 - val_loss: 0.4872 - val_binary_accuracy: 0.6902\n",
            "Epoch 2/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.3985 - binary_accuracy: 0.7883 - val_loss: 0.2853 - val_binary_accuracy: 0.8738\n",
            "Epoch 3/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.2707 - binary_accuracy: 0.8817 - val_loss: 0.2066 - val_binary_accuracy: 0.9158\n",
            "Epoch 4/25\n",
            "458/458 [==============================] - 19s 41ms/step - loss: 0.2042 - binary_accuracy: 0.9191 - val_loss: 0.1589 - val_binary_accuracy: 0.9432\n",
            "Epoch 5/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.1607 - binary_accuracy: 0.9389 - val_loss: 0.1347 - val_binary_accuracy: 0.9579\n",
            "Epoch 6/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.1364 - binary_accuracy: 0.9536 - val_loss: 0.1318 - val_binary_accuracy: 0.9617\n",
            "Epoch 7/25\n",
            "458/458 [==============================] - 19s 41ms/step - loss: 0.1115 - binary_accuracy: 0.9654 - val_loss: 0.1185 - val_binary_accuracy: 0.9689\n",
            "Epoch 8/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.0981 - binary_accuracy: 0.9720 - val_loss: 0.1230 - val_binary_accuracy: 0.9683\n",
            "Epoch 9/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.0847 - binary_accuracy: 0.9761 - val_loss: 0.1172 - val_binary_accuracy: 0.9721\n",
            "Epoch 10/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.0743 - binary_accuracy: 0.9814 - val_loss: 0.1191 - val_binary_accuracy: 0.9732\n",
            "Epoch 11/25\n",
            "458/458 [==============================] - 19s 41ms/step - loss: 0.0659 - binary_accuracy: 0.9816 - val_loss: 0.1201 - val_binary_accuracy: 0.9721\n",
            "Epoch 12/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.0505 - binary_accuracy: 0.9873 - val_loss: 0.1166 - val_binary_accuracy: 0.9760\n",
            "Epoch 13/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.0497 - binary_accuracy: 0.9873 - val_loss: 0.1384 - val_binary_accuracy: 0.9749\n",
            "Epoch 14/25\n",
            "458/458 [==============================] - 19s 41ms/step - loss: 0.0439 - binary_accuracy: 0.9881 - val_loss: 0.1370 - val_binary_accuracy: 0.9765\n",
            "Epoch 15/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.0459 - binary_accuracy: 0.9885 - val_loss: 0.1402 - val_binary_accuracy: 0.9743\n",
            "Epoch 16/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.0273 - binary_accuracy: 0.9944 - val_loss: 0.1481 - val_binary_accuracy: 0.9749\n",
            "Epoch 17/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.0262 - binary_accuracy: 0.9939 - val_loss: 0.1765 - val_binary_accuracy: 0.9705\n",
            "Epoch 18/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.0284 - binary_accuracy: 0.9939 - val_loss: 0.1585 - val_binary_accuracy: 0.9760\n",
            "Epoch 19/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.0242 - binary_accuracy: 0.9947 - val_loss: 0.1590 - val_binary_accuracy: 0.9743\n",
            "Epoch 20/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.0278 - binary_accuracy: 0.9939 - val_loss: 0.1886 - val_binary_accuracy: 0.9721\n",
            "Epoch 21/25\n",
            "458/458 [==============================] - 19s 41ms/step - loss: 0.0278 - binary_accuracy: 0.9937 - val_loss: 0.1446 - val_binary_accuracy: 0.9776\n",
            "Epoch 22/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.0172 - binary_accuracy: 0.9958 - val_loss: 0.1957 - val_binary_accuracy: 0.9727\n",
            "Epoch 23/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.0164 - binary_accuracy: 0.9966 - val_loss: 0.1803 - val_binary_accuracy: 0.9760\n",
            "Epoch 24/25\n",
            "458/458 [==============================] - 19s 41ms/step - loss: 0.0270 - binary_accuracy: 0.9936 - val_loss: 0.1317 - val_binary_accuracy: 0.9798\n",
            "Epoch 25/25\n",
            "458/458 [==============================] - 18s 40ms/step - loss: 0.0186 - binary_accuracy: 0.9963 - val_loss: 0.1773 - val_binary_accuracy: 0.9765\n",
            "Learning rate 0.0001, Batch size 16\n",
            "[0.6104059815406799, 0.3984560966491699, 0.27073875069618225, 0.20419353246688843, 0.16072681546211243, 0.13644523918628693, 0.11151959747076035, 0.09809044748544693, 0.08472034335136414, 0.07431761175394058, 0.0659089908003807, 0.050520047545433044, 0.04969720169901848, 0.04392746835947037, 0.0459236241877079, 0.02725772187113762, 0.02624737285077572, 0.028368152678012848, 0.024211004376411438, 0.02781982161104679, 0.027805449441075325, 0.017170339822769165, 0.016432806849479675, 0.027035489678382874, 0.018568232655525208]\n",
            "[0.48719486594200134, 0.2853183448314667, 0.2065771073102951, 0.1589391678571701, 0.1347227543592453, 0.13175827264785767, 0.1184961125254631, 0.12295924872159958, 0.11724035441875458, 0.11911582946777344, 0.1200675517320633, 0.1166042611002922, 0.1384219378232956, 0.13698814809322357, 0.1402326375246048, 0.1481081247329712, 0.17646221816539764, 0.15853697061538696, 0.15897637605667114, 0.18862797319889069, 0.14461997151374817, 0.1957414299249649, 0.18033352494239807, 0.1316504180431366, 0.1773010492324829]\n",
            "[0.7318325638771057, 0.7882803082466125, 0.8817101716995239, 0.9191367030143738, 0.9389427900314331, 0.9535582661628723, 0.965441882610321, 0.9719983339309692, 0.9760961532592773, 0.9814233183860779, 0.9815598726272583, 0.9872968196868896, 0.9872968196868896, 0.9881163835525513, 0.9885261654853821, 0.9943996667861938, 0.9938532710075378, 0.9938532710075378, 0.9946728348731995, 0.9938532710075378, 0.9937167167663574, 0.9957656264305115, 0.9965851902961731, 0.9935801029205322, 0.9963119626045227]\n",
            "[0.6901639103889465, 0.8737704753875732, 0.9158470034599304, 0.9431694149971008, 0.9579234719276428, 0.9617486596107483, 0.9688524603843689, 0.9683060050010681, 0.9721311330795288, 0.9732240438461304, 0.9721311330795288, 0.9759562611579895, 0.9748634099960327, 0.9765027165412903, 0.9743169546127319, 0.9748634099960327, 0.9704918265342712, 0.9759562611579895, 0.9743169546127319, 0.9721311330795288, 0.9775956273078918, 0.9726775884628296, 0.9759562611579895, 0.979781448841095, 0.9765027165412903]\n",
            "Epoch 1/25\n",
            "229/229 [==============================] - 25s 72ms/step - loss: 0.5635 - binary_accuracy: 0.7347 - val_loss: 0.4871 - val_binary_accuracy: 0.6798\n",
            "Epoch 2/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.4591 - binary_accuracy: 0.7414 - val_loss: 0.3777 - val_binary_accuracy: 0.7847\n",
            "Epoch 3/25\n",
            "229/229 [==============================] - 16s 71ms/step - loss: 0.3696 - binary_accuracy: 0.8241 - val_loss: 0.3024 - val_binary_accuracy: 0.8492\n",
            "Epoch 4/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.2969 - binary_accuracy: 0.8713 - val_loss: 0.2447 - val_binary_accuracy: 0.8863\n",
            "Epoch 5/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.2392 - binary_accuracy: 0.9017 - val_loss: 0.1972 - val_binary_accuracy: 0.9175\n",
            "Epoch 6/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.2008 - binary_accuracy: 0.9201 - val_loss: 0.1693 - val_binary_accuracy: 0.9295\n",
            "Epoch 7/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.1707 - binary_accuracy: 0.9362 - val_loss: 0.1420 - val_binary_accuracy: 0.9503\n",
            "Epoch 8/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.1430 - binary_accuracy: 0.9492 - val_loss: 0.1281 - val_binary_accuracy: 0.9546\n",
            "Epoch 9/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.1216 - binary_accuracy: 0.9578 - val_loss: 0.1160 - val_binary_accuracy: 0.9623\n",
            "Epoch 10/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.1113 - binary_accuracy: 0.9616 - val_loss: 0.1042 - val_binary_accuracy: 0.9650\n",
            "Epoch 11/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.0946 - binary_accuracy: 0.9663 - val_loss: 0.1030 - val_binary_accuracy: 0.9650\n",
            "Epoch 12/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.0850 - binary_accuracy: 0.9702 - val_loss: 0.1056 - val_binary_accuracy: 0.9672\n",
            "Epoch 13/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.0755 - binary_accuracy: 0.9771 - val_loss: 0.1090 - val_binary_accuracy: 0.9678\n",
            "Epoch 14/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.0648 - binary_accuracy: 0.9799 - val_loss: 0.1050 - val_binary_accuracy: 0.9721\n",
            "Epoch 15/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.0723 - binary_accuracy: 0.9779 - val_loss: 0.1087 - val_binary_accuracy: 0.9694\n",
            "Epoch 16/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.0592 - binary_accuracy: 0.9816 - val_loss: 0.1072 - val_binary_accuracy: 0.9716\n",
            "Epoch 17/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.0534 - binary_accuracy: 0.9829 - val_loss: 0.1161 - val_binary_accuracy: 0.9716\n",
            "Epoch 18/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.0488 - binary_accuracy: 0.9855 - val_loss: 0.1229 - val_binary_accuracy: 0.9699\n",
            "Epoch 19/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.0373 - binary_accuracy: 0.9891 - val_loss: 0.1144 - val_binary_accuracy: 0.9716\n",
            "Epoch 20/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.0423 - binary_accuracy: 0.9878 - val_loss: 0.1198 - val_binary_accuracy: 0.9727\n",
            "Epoch 21/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.0319 - binary_accuracy: 0.9908 - val_loss: 0.1152 - val_binary_accuracy: 0.9743\n",
            "Epoch 22/25\n",
            "229/229 [==============================] - 16s 71ms/step - loss: 0.0341 - binary_accuracy: 0.9898 - val_loss: 0.1160 - val_binary_accuracy: 0.9743\n",
            "Epoch 23/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0328 - binary_accuracy: 0.9910 - val_loss: 0.1172 - val_binary_accuracy: 0.9749\n",
            "Epoch 24/25\n",
            "229/229 [==============================] - 16s 69ms/step - loss: 0.0230 - binary_accuracy: 0.9944 - val_loss: 0.1288 - val_binary_accuracy: 0.9765\n",
            "Epoch 25/25\n",
            "229/229 [==============================] - 16s 70ms/step - loss: 0.0248 - binary_accuracy: 0.9933 - val_loss: 0.1284 - val_binary_accuracy: 0.9743\n",
            "Learning rate 0.0001, Batch size 32\n",
            "[0.5635055899620056, 0.4591379165649414, 0.36960259079933167, 0.29691845178604126, 0.2392248809337616, 0.2008107602596283, 0.17066983878612518, 0.1430288851261139, 0.1215958446264267, 0.11128662526607513, 0.09464482963085175, 0.08497866243124008, 0.0754956379532814, 0.06479296833276749, 0.07233445346355438, 0.05916304513812065, 0.05336952209472656, 0.048774030059576035, 0.03732074052095413, 0.04233328253030777, 0.03191019967198372, 0.034115198999643326, 0.03278540447354317, 0.023025210946798325, 0.024766361340880394]\n",
            "[0.4870661795139313, 0.37770670652389526, 0.3023609220981598, 0.24467593431472778, 0.1971779614686966, 0.16931018233299255, 0.14204296469688416, 0.12806113064289093, 0.11598629504442215, 0.10417711734771729, 0.10296710580587387, 0.1055959090590477, 0.10897649824619293, 0.10503316670656204, 0.10869945585727692, 0.1071569174528122, 0.11609210073947906, 0.12289251387119293, 0.11441484093666077, 0.1198437437415123, 0.11518881469964981, 0.11604847759008408, 0.11721481382846832, 0.12879104912281036, 0.1283779889345169]\n",
            "[0.734673798084259, 0.741428792476654, 0.8240677714347839, 0.8713290691375732, 0.9016527533531189, 0.9200928807258606, 0.9362108707427979, 0.9491872787475586, 0.9577926397323608, 0.9616172909736633, 0.9662614464759827, 0.9702226519584656, 0.9770523309707642, 0.9799208045005798, 0.9778718948364258, 0.9815598726272583, 0.9829258322715759, 0.9855210781097412, 0.9890725016593933, 0.9878432154655457, 0.9908482432365417, 0.9897555112838745, 0.9909848570823669, 0.9943996667861938, 0.9933069348335266]\n",
            "[0.679781436920166, 0.7846994400024414, 0.8491803407669067, 0.8863387703895569, 0.917486310005188, 0.9295082092285156, 0.9502732157707214, 0.9546447992324829, 0.9622950553894043, 0.9650273323059082, 0.9650273323059082, 0.9672130942344666, 0.9677595496177673, 0.9721311330795288, 0.9693989157676697, 0.971584677696228, 0.971584677696228, 0.9699453711509705, 0.971584677696228, 0.9726775884628296, 0.9743169546127319, 0.9743169546127319, 0.9748634099960327, 0.9765027165412903, 0.9743169546127319]\n",
            "Epoch 1/25\n",
            "115/115 [==============================] - 30s 184ms/step - loss: 0.5826 - binary_accuracy: 0.7337 - val_loss: 0.5622 - val_binary_accuracy: 0.6749\n",
            "Epoch 2/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.5463 - binary_accuracy: 0.6894 - val_loss: 0.4969 - val_binary_accuracy: 0.7115\n",
            "Epoch 3/25\n",
            "115/115 [==============================] - 21s 179ms/step - loss: 0.4819 - binary_accuracy: 0.7433 - val_loss: 0.4130 - val_binary_accuracy: 0.7874\n",
            "Epoch 4/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.4073 - binary_accuracy: 0.8030 - val_loss: 0.3401 - val_binary_accuracy: 0.8432\n",
            "Epoch 5/25\n",
            "115/115 [==============================] - 21s 180ms/step - loss: 0.3490 - binary_accuracy: 0.8448 - val_loss: 0.2901 - val_binary_accuracy: 0.8727\n",
            "Epoch 6/25\n",
            "115/115 [==============================] - 20s 177ms/step - loss: 0.3063 - binary_accuracy: 0.8674 - val_loss: 0.2543 - val_binary_accuracy: 0.8913\n",
            "Epoch 7/25\n",
            "115/115 [==============================] - 20s 177ms/step - loss: 0.2749 - binary_accuracy: 0.8858 - val_loss: 0.2227 - val_binary_accuracy: 0.9115\n",
            "Epoch 8/25\n",
            "115/115 [==============================] - 21s 180ms/step - loss: 0.2448 - binary_accuracy: 0.9036 - val_loss: 0.1997 - val_binary_accuracy: 0.9137\n",
            "Epoch 9/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.2163 - binary_accuracy: 0.9127 - val_loss: 0.1771 - val_binary_accuracy: 0.9208\n",
            "Epoch 10/25\n",
            "115/115 [==============================] - 21s 179ms/step - loss: 0.1895 - binary_accuracy: 0.9249 - val_loss: 0.1537 - val_binary_accuracy: 0.9355\n",
            "Epoch 11/25\n",
            "115/115 [==============================] - 20s 177ms/step - loss: 0.1706 - binary_accuracy: 0.9346 - val_loss: 0.1424 - val_binary_accuracy: 0.9437\n",
            "Epoch 12/25\n",
            "115/115 [==============================] - 21s 179ms/step - loss: 0.1566 - binary_accuracy: 0.9414 - val_loss: 0.1273 - val_binary_accuracy: 0.9508\n",
            "Epoch 13/25\n",
            "115/115 [==============================] - 20s 177ms/step - loss: 0.1369 - binary_accuracy: 0.9500 - val_loss: 0.1154 - val_binary_accuracy: 0.9568\n",
            "Epoch 14/25\n",
            "115/115 [==============================] - 20s 177ms/step - loss: 0.1227 - binary_accuracy: 0.9563 - val_loss: 0.1076 - val_binary_accuracy: 0.9623\n",
            "Epoch 15/25\n",
            "115/115 [==============================] - 20s 178ms/step - loss: 0.1096 - binary_accuracy: 0.9613 - val_loss: 0.1017 - val_binary_accuracy: 0.9661\n",
            "Epoch 16/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.1015 - binary_accuracy: 0.9656 - val_loss: 0.0969 - val_binary_accuracy: 0.9678\n",
            "Epoch 17/25\n",
            "115/115 [==============================] - 21s 179ms/step - loss: 0.0917 - binary_accuracy: 0.9663 - val_loss: 0.0965 - val_binary_accuracy: 0.9683\n",
            "Epoch 18/25\n",
            "115/115 [==============================] - 20s 176ms/step - loss: 0.0850 - binary_accuracy: 0.9704 - val_loss: 0.0915 - val_binary_accuracy: 0.9689\n",
            "Epoch 19/25\n",
            "115/115 [==============================] - 20s 178ms/step - loss: 0.0781 - binary_accuracy: 0.9728 - val_loss: 0.0932 - val_binary_accuracy: 0.9689\n",
            "Epoch 20/25\n",
            "115/115 [==============================] - 20s 177ms/step - loss: 0.0786 - binary_accuracy: 0.9725 - val_loss: 0.0891 - val_binary_accuracy: 0.9705\n",
            "Epoch 21/25\n",
            "115/115 [==============================] - 20s 178ms/step - loss: 0.0745 - binary_accuracy: 0.9735 - val_loss: 0.0911 - val_binary_accuracy: 0.9694\n",
            "Epoch 22/25\n",
            "115/115 [==============================] - 20s 178ms/step - loss: 0.0657 - binary_accuracy: 0.9771 - val_loss: 0.0903 - val_binary_accuracy: 0.9699\n",
            "Epoch 23/25\n",
            "115/115 [==============================] - 21s 180ms/step - loss: 0.0612 - binary_accuracy: 0.9776 - val_loss: 0.0929 - val_binary_accuracy: 0.9705\n",
            "Epoch 24/25\n",
            "115/115 [==============================] - 20s 178ms/step - loss: 0.0573 - binary_accuracy: 0.9820 - val_loss: 0.0939 - val_binary_accuracy: 0.9721\n",
            "Epoch 25/25\n",
            "115/115 [==============================] - 20s 177ms/step - loss: 0.0536 - binary_accuracy: 0.9825 - val_loss: 0.1004 - val_binary_accuracy: 0.9721\n",
            "Learning rate 0.0001, Batch size 64\n",
            "[0.582621157169342, 0.5462648272514343, 0.4819093942642212, 0.4073038399219513, 0.34902018308639526, 0.30633896589279175, 0.2748662829399109, 0.24475054442882538, 0.21632787585258484, 0.1894829273223877, 0.17063599824905396, 0.1566419154405594, 0.13694994151592255, 0.12272951751947403, 0.10961218923330307, 0.10151748359203339, 0.09165308624505997, 0.08499620109796524, 0.07806428521871567, 0.0786358192563057, 0.07449225336313248, 0.06573742628097534, 0.06119093298912048, 0.057306114584207535, 0.05356699600815773]\n",
            "[0.5621746778488159, 0.4969087243080139, 0.4130277931690216, 0.34005433320999146, 0.29007014632225037, 0.25432127714157104, 0.22267191112041473, 0.19970083236694336, 0.17709830403327942, 0.1536705046892166, 0.14242613315582275, 0.12732169032096863, 0.11543692648410797, 0.10758652538061142, 0.10171789675951004, 0.0969463437795639, 0.09646207839250565, 0.09152591973543167, 0.09315986931324005, 0.08913923054933548, 0.09109409153461456, 0.09027356654405594, 0.09286085516214371, 0.09387221932411194, 0.10040648281574249]\n",
            "[0.7336903214454651, 0.6893867254257202, 0.7433410882949829, 0.803032398223877, 0.8448299169540405, 0.8673678636550903, 0.8858079314231873, 0.9035651087760925, 0.9127168655395508, 0.9248736500740051, 0.9345718026161194, 0.9414014220237732, 0.9500068426132202, 0.9562901258468628, 0.9613440632820129, 0.9655784964561462, 0.9662614464759827, 0.9703592658042908, 0.9728178977966309, 0.9725447297096252, 0.9735009074211121, 0.9770523309707642, 0.9775986671447754, 0.9819696545600891, 0.9825160503387451]\n",
            "[0.6748633980751038, 0.7114754319190979, 0.7874317169189453, 0.8431693911552429, 0.8726776242256165, 0.8912568092346191, 0.911475419998169, 0.9136611819267273, 0.9207650423049927, 0.9355190992355347, 0.9437158703804016, 0.9508196711540222, 0.956830620765686, 0.9622950553894043, 0.9661202430725098, 0.9677595496177673, 0.9683060050010681, 0.9688524603843689, 0.9688524603843689, 0.9704918265342712, 0.9693989157676697, 0.9699453711509705, 0.9704918265342712, 0.9721311330795288, 0.9721311330795288]\n",
            "Epoch 1/25\n",
            "58/58 [==============================] - 42s 578ms/step - loss: 1.5155 - binary_accuracy: 0.4574 - val_loss: 1.5267 - val_binary_accuracy: 0.3290\n",
            "Epoch 2/25\n",
            "58/58 [==============================] - 33s 563ms/step - loss: 1.4199 - binary_accuracy: 0.3280 - val_loss: 1.3519 - val_binary_accuracy: 0.3284\n",
            "Epoch 3/25\n",
            "58/58 [==============================] - 33s 567ms/step - loss: 1.2112 - binary_accuracy: 0.3224 - val_loss: 1.0754 - val_binary_accuracy: 0.3230\n",
            "Epoch 4/25\n",
            "58/58 [==============================] - 33s 569ms/step - loss: 0.9420 - binary_accuracy: 0.3840 - val_loss: 0.7798 - val_binary_accuracy: 0.5623\n",
            "Epoch 5/25\n",
            "58/58 [==============================] - 33s 569ms/step - loss: 0.7053 - binary_accuracy: 0.6245 - val_loss: 0.6080 - val_binary_accuracy: 0.6683\n",
            "Epoch 6/25\n",
            "58/58 [==============================] - 33s 566ms/step - loss: 0.5830 - binary_accuracy: 0.6693 - val_loss: 0.5220 - val_binary_accuracy: 0.6781\n",
            "Epoch 7/25\n",
            "58/58 [==============================] - 33s 568ms/step - loss: 0.5062 - binary_accuracy: 0.6980 - val_loss: 0.4445 - val_binary_accuracy: 0.7361\n",
            "Epoch 8/25\n",
            "58/58 [==============================] - 33s 570ms/step - loss: 0.4401 - binary_accuracy: 0.7611 - val_loss: 0.3780 - val_binary_accuracy: 0.7951\n",
            "Epoch 9/25\n",
            "58/58 [==============================] - 33s 566ms/step - loss: 0.3866 - binary_accuracy: 0.8155 - val_loss: 0.3309 - val_binary_accuracy: 0.8432\n",
            "Epoch 10/25\n",
            "58/58 [==============================] - 33s 566ms/step - loss: 0.3437 - binary_accuracy: 0.8507 - val_loss: 0.2932 - val_binary_accuracy: 0.8732\n",
            "Epoch 11/25\n",
            "58/58 [==============================] - 33s 567ms/step - loss: 0.3113 - binary_accuracy: 0.8678 - val_loss: 0.2629 - val_binary_accuracy: 0.8885\n",
            "Epoch 12/25\n",
            "58/58 [==============================] - 33s 569ms/step - loss: 0.2835 - binary_accuracy: 0.8865 - val_loss: 0.2376 - val_binary_accuracy: 0.9005\n",
            "Epoch 13/25\n",
            "58/58 [==============================] - 33s 570ms/step - loss: 0.2553 - binary_accuracy: 0.8986 - val_loss: 0.2124 - val_binary_accuracy: 0.9148\n",
            "Epoch 14/25\n",
            "58/58 [==============================] - 33s 564ms/step - loss: 0.2320 - binary_accuracy: 0.9109 - val_loss: 0.1920 - val_binary_accuracy: 0.9240\n",
            "Epoch 15/25\n",
            "58/58 [==============================] - 33s 568ms/step - loss: 0.2069 - binary_accuracy: 0.9226 - val_loss: 0.1736 - val_binary_accuracy: 0.9322\n",
            "Epoch 16/25\n",
            "58/58 [==============================] - 33s 569ms/step - loss: 0.1933 - binary_accuracy: 0.9277 - val_loss: 0.1529 - val_binary_accuracy: 0.9415\n",
            "Epoch 17/25\n",
            "58/58 [==============================] - 33s 573ms/step - loss: 0.1774 - binary_accuracy: 0.9333 - val_loss: 0.1403 - val_binary_accuracy: 0.9492\n",
            "Epoch 18/25\n",
            "58/58 [==============================] - 33s 566ms/step - loss: 0.1578 - binary_accuracy: 0.9425 - val_loss: 0.1278 - val_binary_accuracy: 0.9530\n",
            "Epoch 19/25\n",
            "58/58 [==============================] - 33s 567ms/step - loss: 0.1461 - binary_accuracy: 0.9485 - val_loss: 0.1180 - val_binary_accuracy: 0.9607\n",
            "Epoch 20/25\n",
            "58/58 [==============================] - 33s 571ms/step - loss: 0.1378 - binary_accuracy: 0.9519 - val_loss: 0.1095 - val_binary_accuracy: 0.9612\n",
            "Epoch 21/25\n",
            "58/58 [==============================] - 33s 566ms/step - loss: 0.1289 - binary_accuracy: 0.9559 - val_loss: 0.1043 - val_binary_accuracy: 0.9634\n",
            "Epoch 22/25\n",
            "58/58 [==============================] - 33s 566ms/step - loss: 0.1220 - binary_accuracy: 0.9603 - val_loss: 0.1005 - val_binary_accuracy: 0.9639\n",
            "Epoch 23/25\n",
            "58/58 [==============================] - 33s 568ms/step - loss: 0.1175 - binary_accuracy: 0.9593 - val_loss: 0.0978 - val_binary_accuracy: 0.9650\n",
            "Epoch 24/25\n",
            "58/58 [==============================] - 33s 568ms/step - loss: 0.1073 - binary_accuracy: 0.9639 - val_loss: 0.0950 - val_binary_accuracy: 0.9672\n",
            "Epoch 25/25\n",
            "58/58 [==============================] - 33s 565ms/step - loss: 0.1049 - binary_accuracy: 0.9642 - val_loss: 0.0931 - val_binary_accuracy: 0.9683\n",
            "Learning rate 0.0001, Batch size 128\n",
            "[1.5155192613601685, 1.4199367761611938, 1.211165428161621, 0.9419702291488647, 0.7053354978561401, 0.5829540491104126, 0.5061866641044617, 0.4400610327720642, 0.3866252303123474, 0.34369558095932007, 0.31126606464385986, 0.28352341055870056, 0.2552766799926758, 0.2319759726524353, 0.20687881112098694, 0.1932806372642517, 0.17742934823036194, 0.15783052146434784, 0.14613716304302216, 0.13779525458812714, 0.1288619041442871, 0.12200577557086945, 0.11753430962562561, 0.10729821026325226, 0.10490048676729202]\n",
            "[1.5266505479812622, 1.3518860340118408, 1.0754411220550537, 0.7797995805740356, 0.6079734563827515, 0.5219921469688416, 0.4445117115974426, 0.37803295254707336, 0.330875039100647, 0.2931758463382721, 0.26292914152145386, 0.23759908974170685, 0.21237966418266296, 0.19204464554786682, 0.1735958456993103, 0.15287582576274872, 0.14031967520713806, 0.12778429687023163, 0.1180182471871376, 0.10954765975475311, 0.10433976352214813, 0.10045645385980606, 0.09783478081226349, 0.09500166028738022, 0.09306782484054565]\n",
            "[0.45743635296821594, 0.32796066999435425, 0.3223603367805481, 0.383963942527771, 0.6245048642158508, 0.6693074703216553, 0.6979920864105225, 0.7610982060432434, 0.8154623508453369, 0.8507034778594971, 0.8677776455879211, 0.8864909410476685, 0.8986477255821228, 0.9109411239624023, 0.9225515723228455, 0.9277421236038208, 0.933342456817627, 0.9424942135810852, 0.9485043287277222, 0.9519191384315491, 0.955880343914032, 0.9602513313293457, 0.9592951536178589, 0.963939368724823, 0.9642125368118286]\n",
            "[0.3289617598056793, 0.32841530442237854, 0.3229508101940155, 0.5622950792312622, 0.6683059930801392, 0.6781420707702637, 0.7360655665397644, 0.7950819730758667, 0.8431693911552429, 0.8732240200042725, 0.88852459192276, 0.9005464315414429, 0.9147540926933289, 0.9240437150001526, 0.9322404265403748, 0.9415300488471985, 0.9491803050041199, 0.9530054926872253, 0.9606557488441467, 0.9612022042274475, 0.9633879661560059, 0.9639344215393066, 0.9650273323059082, 0.9672130942344666, 0.9683060050010681]\n",
            "Epoch 1/25\n",
            "29/29 [==============================] - 69s 2s/step - loss: 0.7704 - binary_accuracy: 0.7030 - val_loss: 0.7642 - val_binary_accuracy: 0.6803\n",
            "Epoch 2/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.7611 - binary_accuracy: 0.6578 - val_loss: 0.7512 - val_binary_accuracy: 0.6973\n",
            "Epoch 3/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.7445 - binary_accuracy: 0.6718 - val_loss: 0.7304 - val_binary_accuracy: 0.7016\n",
            "Epoch 4/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.7214 - binary_accuracy: 0.6875 - val_loss: 0.7032 - val_binary_accuracy: 0.6984\n",
            "Epoch 5/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.6941 - binary_accuracy: 0.6923 - val_loss: 0.6710 - val_binary_accuracy: 0.6869\n",
            "Epoch 6/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.6617 - binary_accuracy: 0.6835 - val_loss: 0.6366 - val_binary_accuracy: 0.6803\n",
            "Epoch 7/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.6310 - binary_accuracy: 0.6802 - val_loss: 0.6029 - val_binary_accuracy: 0.6765\n",
            "Epoch 8/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.5982 - binary_accuracy: 0.6778 - val_loss: 0.5704 - val_binary_accuracy: 0.6754\n",
            "Epoch 9/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.5708 - binary_accuracy: 0.6790 - val_loss: 0.5386 - val_binary_accuracy: 0.6798\n",
            "Epoch 10/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.5421 - binary_accuracy: 0.6858 - val_loss: 0.5060 - val_binary_accuracy: 0.6995\n",
            "Epoch 11/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.5109 - binary_accuracy: 0.7063 - val_loss: 0.4744 - val_binary_accuracy: 0.7213\n",
            "Epoch 12/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.4845 - binary_accuracy: 0.7323 - val_loss: 0.4445 - val_binary_accuracy: 0.7503\n",
            "Epoch 13/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.4547 - binary_accuracy: 0.7604 - val_loss: 0.4168 - val_binary_accuracy: 0.7705\n",
            "Epoch 14/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.4293 - binary_accuracy: 0.7821 - val_loss: 0.3922 - val_binary_accuracy: 0.7874\n",
            "Epoch 15/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.4063 - binary_accuracy: 0.8017 - val_loss: 0.3683 - val_binary_accuracy: 0.8071\n",
            "Epoch 16/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.3832 - binary_accuracy: 0.8232 - val_loss: 0.3469 - val_binary_accuracy: 0.8333\n",
            "Epoch 17/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.3623 - binary_accuracy: 0.8347 - val_loss: 0.3268 - val_binary_accuracy: 0.8475\n",
            "Epoch 18/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.3417 - binary_accuracy: 0.8482 - val_loss: 0.3083 - val_binary_accuracy: 0.8634\n",
            "Epoch 19/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.3220 - binary_accuracy: 0.8626 - val_loss: 0.2917 - val_binary_accuracy: 0.8705\n",
            "Epoch 20/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.3054 - binary_accuracy: 0.8701 - val_loss: 0.2757 - val_binary_accuracy: 0.8792\n",
            "Epoch 21/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.2894 - binary_accuracy: 0.8791 - val_loss: 0.2605 - val_binary_accuracy: 0.8891\n",
            "Epoch 22/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.2732 - binary_accuracy: 0.8887 - val_loss: 0.2440 - val_binary_accuracy: 0.9000\n",
            "Epoch 23/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.2609 - binary_accuracy: 0.8954 - val_loss: 0.2313 - val_binary_accuracy: 0.9071\n",
            "Epoch 24/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.2423 - binary_accuracy: 0.9045 - val_loss: 0.2194 - val_binary_accuracy: 0.9131\n",
            "Epoch 25/25\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.2336 - binary_accuracy: 0.9075 - val_loss: 0.2070 - val_binary_accuracy: 0.9186\n",
            "Learning rate 0.0001, Batch size 256\n",
            "[0.7703850269317627, 0.761142909526825, 0.7444528341293335, 0.7213535904884338, 0.6940672397613525, 0.6617315411567688, 0.6309704184532166, 0.598231852054596, 0.5707824230194092, 0.5420528650283813, 0.5109480619430542, 0.48448097705841064, 0.45472148060798645, 0.42931225895881653, 0.4063430726528168, 0.38317444920539856, 0.36228469014167786, 0.3416653871536255, 0.3219791054725647, 0.3054247498512268, 0.2894359529018402, 0.27320143580436707, 0.26086345314979553, 0.24226528406143188, 0.2335800975561142]\n",
            "[0.7642446756362915, 0.7512009739875793, 0.7304219007492065, 0.7031956315040588, 0.6709643602371216, 0.6366080641746521, 0.602932333946228, 0.5704310536384583, 0.538610577583313, 0.5059769153594971, 0.47440004348754883, 0.44452598690986633, 0.41682109236717224, 0.3922407031059265, 0.3683319687843323, 0.34687894582748413, 0.32684993743896484, 0.3083094656467438, 0.29167285561561584, 0.27568504214286804, 0.2604549527168274, 0.24400164186954498, 0.2313249558210373, 0.21942338347434998, 0.20703788101673126]\n",
            "[0.7029832601547241, 0.6578336358070374, 0.6717661619186401, 0.6874743700027466, 0.6922551393508911, 0.6835131645202637, 0.680234968662262, 0.6777762770652771, 0.6790056228637695, 0.6858352422714233, 0.7063242793083191, 0.7322770357131958, 0.760415256023407, 0.7821335792541504, 0.8016664385795593, 0.8232482075691223, 0.8347220420837402, 0.8482447862625122, 0.8625870943069458, 0.8700997233390808, 0.8791148662567139, 0.8886764049530029, 0.8953694701194763, 0.9045212268829346, 0.9075263142585754]\n",
            "[0.6803278923034668, 0.6972677707672119, 0.7016393542289734, 0.6983606815338135, 0.6868852376937866, 0.6803278923034668, 0.6765027046203613, 0.6754098534584045, 0.679781436920166, 0.6994535326957703, 0.7213114500045776, 0.7502732276916504, 0.7704917788505554, 0.7874317169189453, 0.8071038126945496, 0.8333333134651184, 0.8475409746170044, 0.8633880019187927, 0.8704918026924133, 0.8792349696159363, 0.8890710473060608, 0.8999999761581421, 0.9071038365364075, 0.9131147265434265, 0.9185792207717896]\n"
          ]
        }
      ],
      "source": [
        "epochs = 25\n",
        "\n",
        "learning_rate_list = [0.0005, 0.0001]  \n",
        "batch_size_list = [16, 32, 64, 128, 256]\n",
        "\n",
        "for rate in learning_rate_list:\n",
        "  for size in batch_size_list:\n",
        "    batch_size = size\n",
        "    init_lr = rate\n",
        "\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    # training and validation dataset\n",
        "    train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "\n",
        "    train_val_data.shuffle(buffer_size=len(list(train_val_data)), seed = 12345)\n",
        "\n",
        "    train_val_set_size = len(list(train_val_data))\n",
        "    val_n = int(0.2*train_val_set_size)\n",
        "\n",
        "    train_data = train_val_data.skip(val_n).batch(batch_size)\n",
        "    train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "    val_data = train_val_data.take(val_n).batch(batch_size)\n",
        "    val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    steps_per_epoch = train_val_set_size - val_n\n",
        "    num_train_steps = steps_per_epoch * epochs\n",
        "    num_warmup_steps = int(0.1*num_train_steps)\n",
        "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                              num_train_steps=num_train_steps,\n",
        "                                              num_warmup_steps=num_warmup_steps,\n",
        "                                              optimizer_type='adamw')\n",
        "\n",
        "    classifier = build_classifier_model(0)\n",
        "    classifier.compile(optimizer=optimizer,\n",
        "                            loss=loss,\n",
        "                            metrics=metrics)\n",
        "    history = classifier.fit(x=train_data, validation_data=val_data, epochs=epochs)\n",
        "    history_dict=history.history\n",
        "    print(f\"Learning rate {rate}, Batch size {size}\")\n",
        "    print(history_dict['loss'])\n",
        "    print(history_dict['val_loss'])\n",
        "    print(history_dict['binary_accuracy'])\n",
        "    print(history_dict['val_binary_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L6FvYVZ4j4P9",
        "outputId": "401557b7-5f25-42a6-f3c7-d69a8d5d0143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/33\n",
            "58/58 [==============================] - 46s 574ms/step - loss: 0.9013 - binary_accuracy: 0.3711 - val_loss: 0.8915 - val_binary_accuracy: 0.3557\n",
            "Epoch 2/33\n",
            "58/58 [==============================] - 32s 555ms/step - loss: 0.8634 - binary_accuracy: 0.4132 - val_loss: 0.8325 - val_binary_accuracy: 0.4623\n",
            "Epoch 3/33\n",
            "58/58 [==============================] - 32s 555ms/step - loss: 0.7977 - binary_accuracy: 0.5300 - val_loss: 0.7487 - val_binary_accuracy: 0.6175\n",
            "Epoch 4/33\n",
            "58/58 [==============================] - 32s 556ms/step - loss: 0.7175 - binary_accuracy: 0.6417 - val_loss: 0.6650 - val_binary_accuracy: 0.6705\n",
            "Epoch 5/33\n",
            "58/58 [==============================] - 32s 557ms/step - loss: 0.6523 - binary_accuracy: 0.6696 - val_loss: 0.5988 - val_binary_accuracy: 0.6710\n",
            "Epoch 6/33\n",
            "58/58 [==============================] - 32s 560ms/step - loss: 0.5941 - binary_accuracy: 0.6745 - val_loss: 0.5360 - val_binary_accuracy: 0.6749\n",
            "Epoch 7/33\n",
            "58/58 [==============================] - 32s 557ms/step - loss: 0.5369 - binary_accuracy: 0.6931 - val_loss: 0.4700 - val_binary_accuracy: 0.7142\n",
            "Epoch 8/33\n",
            "58/58 [==============================] - 32s 555ms/step - loss: 0.4854 - binary_accuracy: 0.7264 - val_loss: 0.4162 - val_binary_accuracy: 0.7672\n",
            "Epoch 9/33\n",
            "58/58 [==============================] - 32s 558ms/step - loss: 0.4397 - binary_accuracy: 0.7689 - val_loss: 0.3728 - val_binary_accuracy: 0.7984\n",
            "Epoch 10/33\n",
            "58/58 [==============================] - 32s 555ms/step - loss: 0.3999 - binary_accuracy: 0.7977 - val_loss: 0.3398 - val_binary_accuracy: 0.8251\n",
            "Epoch 11/33\n",
            "58/58 [==============================] - 32s 557ms/step - loss: 0.3707 - binary_accuracy: 0.8181 - val_loss: 0.3152 - val_binary_accuracy: 0.8470\n",
            "Epoch 12/33\n",
            "58/58 [==============================] - 32s 557ms/step - loss: 0.3519 - binary_accuracy: 0.8347 - val_loss: 0.2967 - val_binary_accuracy: 0.8634\n",
            "Epoch 13/33\n",
            "58/58 [==============================] - 32s 560ms/step - loss: 0.3245 - binary_accuracy: 0.8504 - val_loss: 0.2773 - val_binary_accuracy: 0.8765\n",
            "Epoch 14/33\n",
            "58/58 [==============================] - 32s 554ms/step - loss: 0.3009 - binary_accuracy: 0.8637 - val_loss: 0.2571 - val_binary_accuracy: 0.8852\n",
            "Epoch 15/33\n",
            "58/58 [==============================] - 32s 561ms/step - loss: 0.2827 - binary_accuracy: 0.8779 - val_loss: 0.2419 - val_binary_accuracy: 0.8907\n",
            "Epoch 16/33\n",
            "58/58 [==============================] - 32s 557ms/step - loss: 0.2643 - binary_accuracy: 0.8883 - val_loss: 0.2234 - val_binary_accuracy: 0.8995\n",
            "Epoch 17/33\n",
            "58/58 [==============================] - 32s 558ms/step - loss: 0.2459 - binary_accuracy: 0.8969 - val_loss: 0.2075 - val_binary_accuracy: 0.9082\n",
            "Epoch 18/33\n",
            "58/58 [==============================] - 32s 553ms/step - loss: 0.2290 - binary_accuracy: 0.9053 - val_loss: 0.1916 - val_binary_accuracy: 0.9224\n",
            "Epoch 19/33\n",
            "58/58 [==============================] - 32s 557ms/step - loss: 0.2085 - binary_accuracy: 0.9160 - val_loss: 0.1755 - val_binary_accuracy: 0.9279\n",
            "Epoch 20/33\n",
            "58/58 [==============================] - 32s 555ms/step - loss: 0.1974 - binary_accuracy: 0.9245 - val_loss: 0.1604 - val_binary_accuracy: 0.9399\n",
            "Epoch 21/33\n",
            "58/58 [==============================] - 32s 558ms/step - loss: 0.1806 - binary_accuracy: 0.9303 - val_loss: 0.1481 - val_binary_accuracy: 0.9432\n",
            "Epoch 22/33\n",
            "58/58 [==============================] - 32s 556ms/step - loss: 0.1641 - binary_accuracy: 0.9366 - val_loss: 0.1399 - val_binary_accuracy: 0.9464\n",
            "Epoch 23/33\n",
            "58/58 [==============================] - 32s 555ms/step - loss: 0.1522 - binary_accuracy: 0.9402 - val_loss: 0.1292 - val_binary_accuracy: 0.9536\n",
            "Epoch 24/33\n",
            "58/58 [==============================] - 32s 555ms/step - loss: 0.1439 - binary_accuracy: 0.9486 - val_loss: 0.1242 - val_binary_accuracy: 0.9563\n",
            "Epoch 25/33\n",
            "58/58 [==============================] - 32s 558ms/step - loss: 0.1351 - binary_accuracy: 0.9508 - val_loss: 0.1169 - val_binary_accuracy: 0.9601\n",
            "Epoch 26/33\n",
            "58/58 [==============================] - 32s 557ms/step - loss: 0.1221 - binary_accuracy: 0.9549 - val_loss: 0.1104 - val_binary_accuracy: 0.9601\n",
            "Epoch 27/33\n",
            "58/58 [==============================] - 32s 553ms/step - loss: 0.1187 - binary_accuracy: 0.9579 - val_loss: 0.1057 - val_binary_accuracy: 0.9634\n",
            "Epoch 28/33\n",
            "58/58 [==============================] - 32s 559ms/step - loss: 0.1084 - binary_accuracy: 0.9631 - val_loss: 0.1039 - val_binary_accuracy: 0.9650\n",
            "Epoch 29/33\n",
            "58/58 [==============================] - 32s 553ms/step - loss: 0.1019 - binary_accuracy: 0.9650 - val_loss: 0.1043 - val_binary_accuracy: 0.9650\n",
            "Epoch 30/33\n",
            "58/58 [==============================] - 32s 556ms/step - loss: 0.1010 - binary_accuracy: 0.9648 - val_loss: 0.1003 - val_binary_accuracy: 0.9656\n",
            "Epoch 31/33\n",
            "58/58 [==============================] - 32s 557ms/step - loss: 0.0964 - binary_accuracy: 0.9665 - val_loss: 0.0977 - val_binary_accuracy: 0.9672\n",
            "Epoch 32/33\n",
            "58/58 [==============================] - 32s 556ms/step - loss: 0.0876 - binary_accuracy: 0.9697 - val_loss: 0.0972 - val_binary_accuracy: 0.9683\n",
            "Epoch 33/33\n",
            "58/58 [==============================] - 32s 556ms/step - loss: 0.0902 - binary_accuracy: 0.9691 - val_loss: 0.0967 - val_binary_accuracy: 0.9683\n",
            "Learning rate 0.0001, Batch size 128\n",
            "[0.9012681841850281, 0.863418459892273, 0.7976599335670471, 0.7175170183181763, 0.6522788405418396, 0.5940676927566528, 0.5369176864624023, 0.48544013500213623, 0.4397401511669159, 0.3998970687389374, 0.37065422534942627, 0.3518705368041992, 0.3244662582874298, 0.3009088635444641, 0.2826591730117798, 0.26432108879089355, 0.24594810605049133, 0.22900819778442383, 0.20845241844654083, 0.19738203287124634, 0.18061117827892303, 0.16406241059303284, 0.15218935906887054, 0.14386768639087677, 0.13508304953575134, 0.12208638340234756, 0.11866699904203415, 0.1083969920873642, 0.1018839180469513, 0.10095668584108353, 0.09635692834854126, 0.08759354799985886, 0.09016629308462143]\n",
            "[0.8914626836776733, 0.8324511051177979, 0.7486540675163269, 0.6649555563926697, 0.5988205671310425, 0.535951554775238, 0.47002023458480835, 0.41621607542037964, 0.3728303909301758, 0.3397828936576843, 0.3152450621128082, 0.2967263460159302, 0.27733421325683594, 0.2571260333061218, 0.24190379679203033, 0.22341974079608917, 0.20752207934856415, 0.19155514240264893, 0.17550930380821228, 0.16037426888942719, 0.14810232818126678, 0.13985392451286316, 0.12918399274349213, 0.1242460384964943, 0.11689399927854538, 0.11042363941669464, 0.10567016899585724, 0.10386490076780319, 0.10433216392993927, 0.10031703114509583, 0.09765482693910599, 0.09723464399576187, 0.09672901779413223]\n",
            "[0.37112417817115784, 0.4131949245929718, 0.5299822688102722, 0.6417155861854553, 0.6695806384086609, 0.6744980216026306, 0.6930747032165527, 0.7264034748077393, 0.768884003162384, 0.7977052330970764, 0.818057656288147, 0.8347220420837402, 0.8504302501678467, 0.863679826259613, 0.8778855204582214, 0.8882666230201721, 0.8968719840049744, 0.9053407907485962, 0.9159950613975525, 0.9244638681411743, 0.9303373694419861, 0.9366206526756287, 0.9401721358299255, 0.9486408829689026, 0.9508264064788818, 0.9549241662025452, 0.957929253578186, 0.9631198048591614, 0.9650321006774902, 0.9647589325904846, 0.9665346145629883, 0.9696762561798096, 0.9691299200057983]\n",
            "[0.35573771595954895, 0.4622950851917267, 0.6174863576889038, 0.6704918146133423, 0.6710382699966431, 0.6748633980751038, 0.714207649230957, 0.7672131061553955, 0.7983606457710266, 0.8251366019248962, 0.8469945192337036, 0.8633880019187927, 0.8765027523040771, 0.8852459192276001, 0.8907103538513184, 0.8994535803794861, 0.908196747303009, 0.9224043488502502, 0.9278688430786133, 0.9398906826972961, 0.9431694149971008, 0.9464480876922607, 0.9535518884658813, 0.9562841653823853, 0.960109293460846, 0.960109293460846, 0.9633879661560059, 0.9650273323059082, 0.9650273323059082, 0.965573787689209, 0.9672130942344666, 0.9683060050010681, 0.9683060050010681]\n",
            "Epoch 1/60\n",
            "29/29 [==============================] - 66s 2s/step - loss: 1.0753 - binary_accuracy: 0.4542 - val_loss: 1.1170 - val_binary_accuracy: 0.3131\n",
            "Epoch 2/60\n",
            "29/29 [==============================] - 58s 2s/step - loss: 1.0562 - binary_accuracy: 0.3274 - val_loss: 1.0995 - val_binary_accuracy: 0.3109\n",
            "Epoch 3/60\n",
            "29/29 [==============================] - 58s 2s/step - loss: 1.0413 - binary_accuracy: 0.3333 - val_loss: 1.0714 - val_binary_accuracy: 0.3098\n",
            "Epoch 4/60\n",
            "29/29 [==============================] - 58s 2s/step - loss: 1.0191 - binary_accuracy: 0.3322 - val_loss: 1.0337 - val_binary_accuracy: 0.3060\n",
            "Epoch 5/60\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.9825 - binary_accuracy: 0.3427 - val_loss: 0.9889 - val_binary_accuracy: 0.3000\n",
            "Epoch 6/60\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.9449 - binary_accuracy: 0.3573 - val_loss: 0.9391 - val_binary_accuracy: 0.3137\n",
            "Epoch 7/60\n",
            " 2/29 [=>............................] - ETA: 43s - loss: 0.9368 - binary_accuracy: 0.3711"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-fa34bda3f85f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m                         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                         metrics=metrics)\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Learning rate {init_lr}, Batch size {batch_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "init_lr = 0.0001 \n",
        "batch_size = 128\n",
        "epochs = 33\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "# training and validation dataset\n",
        "train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "\n",
        "train_val_data.shuffle(buffer_size=len(list(train_val_data)), seed = 12345)\n",
        "\n",
        "train_val_set_size = len(list(train_val_data))\n",
        "val_n = int(0.2*train_val_set_size)\n",
        "\n",
        "train_data = train_val_data.skip(val_n).batch(batch_size)\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_data = train_val_data.take(val_n).batch(batch_size)\n",
        "val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "steps_per_epoch = train_val_set_size - val_n\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "classifier = build_classifier_model(0)\n",
        "classifier.compile(optimizer=optimizer,\n",
        "                        loss=loss,\n",
        "                        metrics=metrics)\n",
        "history = classifier.fit(x=train_data, validation_data=val_data, epochs=epochs)\n",
        "history_dict=history.history\n",
        "print(f\"Learning rate {init_lr}, Batch size {batch_size}\")\n",
        "print(history_dict['loss'])\n",
        "print(history_dict['val_loss'])\n",
        "print(history_dict['binary_accuracy'])\n",
        "print(history_dict['val_binary_accuracy'])\n",
        "\n",
        "init_lr = 0.0001 \n",
        "batch_size = 256\n",
        "epochs = 60\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "# training and validation dataset\n",
        "train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "\n",
        "train_val_data.shuffle(buffer_size=len(list(train_val_data)), seed = 12345)\n",
        "\n",
        "train_val_set_size = len(list(train_val_data))\n",
        "val_n = int(0.2*train_val_set_size)\n",
        "\n",
        "train_data = train_val_data.skip(val_n).batch(batch_size)\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_data = train_val_data.take(val_n).batch(batch_size)\n",
        "val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "steps_per_epoch = train_val_set_size - val_n\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "classifier = build_classifier_model(0)\n",
        "classifier.compile(optimizer=optimizer,\n",
        "                        loss=loss,\n",
        "                        metrics=metrics)\n",
        "history = classifier.fit(x=train_data, validation_data=val_data, epochs=epochs)\n",
        "history_dict=history.history\n",
        "print(f\"Learning rate {init_lr}, Batch size {batch_size}\")\n",
        "print(history_dict['loss'])\n",
        "print(history_dict['val_loss'])\n",
        "print(history_dict['binary_accuracy'])\n",
        "print(history_dict['val_binary_accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "jvZ2mMXys4hA",
        "outputId": "c1cc94ee-6557-4e5d-bc6a-8999b2683c51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "29/29 [==============================] - 66s 2s/step - loss: 0.7902 - binary_accuracy: 0.5347 - val_loss: 0.7611 - val_binary_accuracy: 0.6022\n",
            "Epoch 2/60\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.7853 - binary_accuracy: 0.5647 - val_loss: 0.7539 - val_binary_accuracy: 0.6071\n",
            "Epoch 3/60\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.7782 - binary_accuracy: 0.5730 - val_loss: 0.7427 - val_binary_accuracy: 0.6219\n",
            "Epoch 4/60\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.7622 - binary_accuracy: 0.5957 - val_loss: 0.7284 - val_binary_accuracy: 0.6448\n",
            "Epoch 5/60\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.7474 - binary_accuracy: 0.6073 - val_loss: 0.7125 - val_binary_accuracy: 0.6601\n",
            "Epoch 6/60\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.7287 - binary_accuracy: 0.6267 - val_loss: 0.6955 - val_binary_accuracy: 0.6661\n",
            "Epoch 7/60\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.7096 - binary_accuracy: 0.6408 - val_loss: 0.6785 - val_binary_accuracy: 0.6694\n",
            "Epoch 8/60\n",
            "29/29 [==============================] - 57s 2s/step - loss: 0.6914 - binary_accuracy: 0.6491 - val_loss: 0.6621 - val_binary_accuracy: 0.6710\n",
            "Epoch 9/60\n",
            "29/29 [==============================] - 57s 2s/step - loss: 0.6709 - binary_accuracy: 0.6637 - val_loss: 0.6456 - val_binary_accuracy: 0.6710\n",
            "Epoch 10/60\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.6547 - binary_accuracy: 0.6633 - val_loss: 0.6293 - val_binary_accuracy: 0.6710\n",
            "Epoch 11/60\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.6343 - binary_accuracy: 0.6668 - val_loss: 0.6130 - val_binary_accuracy: 0.6710\n",
            "Epoch 12/60\n",
            " 3/29 [==>...........................] - ETA: 42s - loss: 0.5985 - binary_accuracy: 0.7109"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c17dda240a88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                         metrics=metrics)\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Learning rate {init_lr}, Batch size {batch_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "init_lr = 0.0001 \n",
        "batch_size = 256\n",
        "epochs = 60\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "# training and validation dataset\n",
        "train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "\n",
        "train_val_data.shuffle(buffer_size=len(list(train_val_data)), seed = 12345)\n",
        "\n",
        "train_val_set_size = len(list(train_val_data))\n",
        "val_n = int(0.2*train_val_set_size)\n",
        "\n",
        "train_data = train_val_data.skip(val_n).batch(batch_size)\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_data = train_val_data.take(val_n).batch(batch_size)\n",
        "val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "steps_per_epoch = train_val_set_size - val_n\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "classifier = build_classifier_model(0)\n",
        "classifier.compile(optimizer=optimizer,\n",
        "                        loss=loss,\n",
        "                        metrics=metrics)\n",
        "history = classifier.fit(x=train_data, validation_data=val_data, epochs=epochs)\n",
        "history_dict=history.history\n",
        "print(f\"Learning rate {init_lr}, Batch size {batch_size}\")\n",
        "print(history_dict['loss'])\n",
        "print(history_dict['val_loss'])\n",
        "print(history_dict['binary_accuracy'])\n",
        "print(history_dict['val_binary_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70WRtsqyeJS4",
        "outputId": "0c7d8f07-cc49-4166-929b-6eac760238f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "229/229 [==============================] - 42s 143ms/step - loss: 0.3711 - binary_accuracy: 0.8470 - val_loss: 0.1399 - val_binary_accuracy: 0.9454\n",
            "Epoch 2/6\n",
            "229/229 [==============================] - 31s 134ms/step - loss: 0.1371 - binary_accuracy: 0.9544 - val_loss: 0.0972 - val_binary_accuracy: 0.9689\n",
            "Epoch 3/6\n",
            "229/229 [==============================] - 31s 134ms/step - loss: 0.0970 - binary_accuracy: 0.9698 - val_loss: 0.0924 - val_binary_accuracy: 0.9738\n",
            "Epoch 4/6\n",
            "229/229 [==============================] - 31s 137ms/step - loss: 0.0813 - binary_accuracy: 0.9743 - val_loss: 0.1323 - val_binary_accuracy: 0.9705\n",
            "Epoch 5/6\n",
            "229/229 [==============================] - 31s 134ms/step - loss: 0.0676 - binary_accuracy: 0.9796 - val_loss: 0.1319 - val_binary_accuracy: 0.9727\n",
            "Epoch 6/6\n",
            "229/229 [==============================] - 31s 134ms/step - loss: 0.0559 - binary_accuracy: 0.9852 - val_loss: 0.1275 - val_binary_accuracy: 0.9689\n"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(12345)\n",
        "\n",
        "epochs = 6\n",
        "\n",
        "batch_size = 32\n",
        "init_lr = 0.001\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "# training and validation dataset\n",
        "train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32], select_cols=[2,3])\n",
        "\n",
        "train_val_data.shuffle(buffer_size=len(list(train_val_data)))\n",
        "train_val_set_size = len(list(train_val_data))\n",
        "val_n = int(0.2*train_val_set_size)\n",
        "\n",
        "train_data = train_val_data.skip(val_n).batch(batch_size)\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_data = train_val_data.take(val_n).batch(batch_size)\n",
        "val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "steps_per_epoch = train_val_set_size - val_n\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "final_classifier = build_classifier_model(0)\n",
        "final_classifier.compile(optimizer=optimizer,\n",
        "                        loss=loss,\n",
        "                        metrics=metrics)\n",
        "history = final_classifier.fit(x=train_data, validation_data=val_data, epochs=epochs)\n",
        "\n",
        "final_classifier.save_weights('final_classifier')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0F30PO8ebN_",
        "outputId": "3c487667-43b2-4d2f-de5e-743de5c100dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa1cc244370>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = build_classifier_model(0)\n",
        "model.load_weights('final_classifier')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPxhfUbGfQqe",
        "outputId": "9dcc58ad-9d1f-4d1f-9431-e6c80dbab791"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "72/72 [==============================] - 8s 99ms/step - loss: 0.2204 - binary_accuracy: 0.9597\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.22041814029216766, 0.9596891403198242]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data = tf.data.experimental.CsvDataset([\"test_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "test_data = test_data.batch(batch_size)\n",
        "test_data = test_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "XS78cSPbsqZ0",
        "outputId": "2c0d3f9c-2404-4055-ae7e-266d5de7ad7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa1cc0d23a0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1,7), history.history['loss'], label='Train')\n",
        "plt.plot(range(1,7), history.history['val_loss'], label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "2JyC4tUktm1d",
        "outputId": "c4e619cc-f283-4ff5-e780-b52eb7890219"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(1,7), history.history['binary_accuracy'], label='Train')\n",
        "plt.plot(range(1,7), history.history['val_binary_accuracy'],  label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.savefig(\"accuracy_final.png\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oevNv1IhrFE6"
      },
      "source": [
        "## Cross- validation on final model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKCwc1tlf4pd"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "# training and validation dataset\n",
        "train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32], select_cols=[2,3])\n",
        "\n",
        "train_val_data.shuffle(buffer_size=len(list(train_val_data)), seed = 12345)\n",
        "\n",
        "train_val_set_size = len(list(train_val_data))\n",
        "\n",
        "split_size = int(0.2*train_val_set_size)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "cv_1 = train_val_data.take(split_size).batch(batch_size)\n",
        "cv_1 = cv_1.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "m1 = train_val_data.skip(split_size)\n",
        "cv_2 = m1.take(split_size).batch(batch_size)\n",
        "cv_2 = cv_2.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "m2 = m1.skip(split_size)\n",
        "cv_3 = m2.take(split_size).batch(batch_size)\n",
        "cv_3 = cv_3.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "m3 = m2.skip(split_size)\n",
        "cv_4 = m3.take(split_size).batch(batch_size)\n",
        "cv_4 = cv_4.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "cv_5 = m3.skip(split_size).batch(batch_size)\n",
        "cv_5 = cv_5.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F26mWrvrIoL",
        "outputId": "cc52f5d4-8392-49d6-823a-af8529b3b6c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "232/232 [==============================] - 43s 135ms/step - loss: 0.3776 - binary_accuracy: 0.8183 - val_loss: 0.1559 - val_binary_accuracy: 0.9437\n",
            "Epoch 2/6\n",
            "232/232 [==============================] - 31s 134ms/step - loss: 0.1467 - binary_accuracy: 0.9495 - val_loss: 0.0814 - val_binary_accuracy: 0.9732\n",
            "Epoch 3/6\n",
            "232/232 [==============================] - 31s 135ms/step - loss: 0.0998 - binary_accuracy: 0.9676 - val_loss: 0.0834 - val_binary_accuracy: 0.9749\n",
            "Epoch 4/6\n",
            "232/232 [==============================] - 30s 130ms/step - loss: 0.0769 - binary_accuracy: 0.9761 - val_loss: 0.1004 - val_binary_accuracy: 0.9765\n",
            "Epoch 5/6\n",
            "232/232 [==============================] - 31s 133ms/step - loss: 0.0653 - binary_accuracy: 0.9821 - val_loss: 0.1623 - val_binary_accuracy: 0.9628\n",
            "Epoch 6/6\n",
            "232/232 [==============================] - 30s 130ms/step - loss: 0.0558 - binary_accuracy: 0.9863 - val_loss: 0.1709 - val_binary_accuracy: 0.9672\n",
            "Epoch 1/6\n",
            "232/232 [==============================] - 39s 133ms/step - loss: 0.4081 - binary_accuracy: 0.8291 - val_loss: 0.1742 - val_binary_accuracy: 0.9344\n",
            "Epoch 2/6\n",
            "232/232 [==============================] - 30s 130ms/step - loss: 0.1329 - binary_accuracy: 0.9551 - val_loss: 0.1208 - val_binary_accuracy: 0.9667\n",
            "Epoch 3/6\n",
            "232/232 [==============================] - 30s 129ms/step - loss: 0.1048 - binary_accuracy: 0.9694 - val_loss: 0.1448 - val_binary_accuracy: 0.9667\n",
            "Epoch 4/6\n",
            "232/232 [==============================] - 30s 129ms/step - loss: 0.0788 - binary_accuracy: 0.9764 - val_loss: 0.1369 - val_binary_accuracy: 0.9727\n",
            "Epoch 5/6\n",
            "232/232 [==============================] - 30s 129ms/step - loss: 0.0642 - binary_accuracy: 0.9827 - val_loss: 0.1629 - val_binary_accuracy: 0.9585\n",
            "Epoch 6/6\n",
            "232/232 [==============================] - 30s 129ms/step - loss: 0.0515 - binary_accuracy: 0.9859 - val_loss: 0.1698 - val_binary_accuracy: 0.9667\n",
            "Epoch 1/6\n",
            "232/232 [==============================] - 40s 132ms/step - loss: 0.4523 - binary_accuracy: 0.7936 - val_loss: 0.1500 - val_binary_accuracy: 0.9421\n",
            "Epoch 2/6\n",
            "232/232 [==============================] - 30s 129ms/step - loss: 0.1459 - binary_accuracy: 0.9489 - val_loss: 0.0978 - val_binary_accuracy: 0.9710\n",
            "Epoch 3/6\n",
            "232/232 [==============================] - 30s 131ms/step - loss: 0.0902 - binary_accuracy: 0.9708 - val_loss: 0.1063 - val_binary_accuracy: 0.9749\n",
            "Epoch 4/6\n",
            "232/232 [==============================] - 30s 131ms/step - loss: 0.0811 - binary_accuracy: 0.9739 - val_loss: 0.1391 - val_binary_accuracy: 0.9727\n",
            "Epoch 5/6\n",
            "232/232 [==============================] - 30s 130ms/step - loss: 0.0640 - binary_accuracy: 0.9820 - val_loss: 0.1562 - val_binary_accuracy: 0.9689\n",
            "Epoch 6/6\n",
            "232/232 [==============================] - 31s 132ms/step - loss: 0.0658 - binary_accuracy: 0.9827 - val_loss: 0.1566 - val_binary_accuracy: 0.9699\n",
            "Epoch 1/6\n",
            "232/232 [==============================] - 40s 132ms/step - loss: 0.4542 - binary_accuracy: 0.7963 - val_loss: 0.1702 - val_binary_accuracy: 0.9366\n",
            "Epoch 2/6\n",
            "232/232 [==============================] - 30s 130ms/step - loss: 0.1375 - binary_accuracy: 0.9521 - val_loss: 0.1246 - val_binary_accuracy: 0.9667\n",
            "Epoch 3/6\n",
            "232/232 [==============================] - 30s 131ms/step - loss: 0.0989 - binary_accuracy: 0.9704 - val_loss: 0.1273 - val_binary_accuracy: 0.9689\n",
            "Epoch 4/6\n",
            "232/232 [==============================] - 30s 131ms/step - loss: 0.0729 - binary_accuracy: 0.9803 - val_loss: 0.1615 - val_binary_accuracy: 0.9623\n",
            "Epoch 5/6\n",
            "232/232 [==============================] - 35s 150ms/step - loss: 0.0597 - binary_accuracy: 0.9851 - val_loss: 0.1399 - val_binary_accuracy: 0.9645\n",
            "Epoch 6/6\n",
            "232/232 [==============================] - 30s 129ms/step - loss: 0.0539 - binary_accuracy: 0.9859 - val_loss: 0.1511 - val_binary_accuracy: 0.9694\n",
            "Epoch 1/6\n",
            "232/232 [==============================] - 41s 132ms/step - loss: 0.3871 - binary_accuracy: 0.8363 - val_loss: 0.1724 - val_binary_accuracy: 0.9394\n",
            "Epoch 2/6\n",
            "232/232 [==============================] - 30s 129ms/step - loss: 0.1570 - binary_accuracy: 0.9444 - val_loss: 0.1253 - val_binary_accuracy: 0.9645\n",
            "Epoch 3/6\n",
            "232/232 [==============================] - 30s 130ms/step - loss: 0.1044 - binary_accuracy: 0.9656 - val_loss: 0.1262 - val_binary_accuracy: 0.9694\n",
            "Epoch 4/6\n",
            "232/232 [==============================] - 30s 131ms/step - loss: 0.0840 - binary_accuracy: 0.9747 - val_loss: 0.1320 - val_binary_accuracy: 0.9716\n",
            "Epoch 5/6\n",
            "232/232 [==============================] - 30s 130ms/step - loss: 0.0681 - binary_accuracy: 0.9814 - val_loss: 0.1301 - val_binary_accuracy: 0.9716\n",
            "Epoch 6/6\n",
            "232/232 [==============================] - 30s 131ms/step - loss: 0.0541 - binary_accuracy: 0.9863 - val_loss: 0.1486 - val_binary_accuracy: 0.9689\n",
            "[[0.8183308243751526, 0.9494604468345642, 0.9676273465156555, 0.9760961532592773, 0.9821062684059143, 0.9863406419754028], [0.8290897011756897, 0.9550607800483704, 0.969403088092804, 0.976369321346283, 0.9826526641845703, 0.985930860042572], [0.7935744524002075, 0.948914110660553, 0.9707690477371216, 0.9739106893539429, 0.9819696545600891, 0.9826526641845703], [0.7963064312934875, 0.9520557522773743, 0.9703592658042908, 0.9803305268287659, 0.9851112961769104, 0.985930860042572], [0.8362841606140137, 0.9443988800048828, 0.965573787689209, 0.9747267961502075, 0.9814207553863525, 0.9863387942314148]]\n",
            "[[0.9437158703804016, 0.9732240438461304, 0.9748634099960327, 0.9765027165412903, 0.9628415107727051, 0.9672130942344666], [0.9344262480735779, 0.9666666388511658, 0.9666666388511658, 0.9726775884628296, 0.9584699273109436, 0.9666666388511658], [0.9420765042304993, 0.9710382223129272, 0.9748634099960327, 0.9726775884628296, 0.9688524603843689, 0.9699453711509705], [0.9366120100021362, 0.9666666388511658, 0.9688524603843689, 0.9622950553894043, 0.9644808769226074, 0.9693989157676697], [0.9393773674964905, 0.9645002484321594, 0.9694156050682068, 0.9716002345085144, 0.9716002345085144, 0.9688694477081299]]\n",
            "[[0.37759512662887573, 0.14667320251464844, 0.09981504082679749, 0.07692921906709671, 0.06528710573911667, 0.055755794048309326], [0.4080507457256317, 0.13289175927639008, 0.10480672866106033, 0.07880803197622299, 0.0642251968383789, 0.05149271711707115], [0.4523373544216156, 0.1459236741065979, 0.09022647142410278, 0.08105498552322388, 0.06397942453622818, 0.0658096969127655], [0.45417699217796326, 0.1375039964914322, 0.09891277551651001, 0.07291463017463684, 0.059663522988557816, 0.05389729142189026], [0.38705047965049744, 0.15701977908611298, 0.104371078312397, 0.08396777510643005, 0.06814102828502655, 0.054099198430776596]]\n",
            "[[0.1558905988931656, 0.08137907087802887, 0.08336880058050156, 0.10041450709104538, 0.16229237616062164, 0.17087796330451965], [0.17416632175445557, 0.1208067461848259, 0.1447857916355133, 0.1369040459394455, 0.16285011172294617, 0.16982221603393555], [0.15000197291374207, 0.09778314083814621, 0.10629517585039139, 0.13912400603294373, 0.15619921684265137, 0.15657252073287964], [0.1702309101819992, 0.12460935115814209, 0.12733209133148193, 0.1615060269832611, 0.1398785412311554, 0.15114697813987732], [0.1723753660917282, 0.12530197203159332, 0.12624463438987732, 0.13202041387557983, 0.13008944690227509, 0.14856629073619843]]\n"
          ]
        }
      ],
      "source": [
        "acc_fold = []\n",
        "val_acc_fold = []\n",
        "loss_fold = []\n",
        "val_loss_fold = []\n",
        "\n",
        "init_lr = 0.001\n",
        "epochs = 6\n",
        "\n",
        "steps_per_epoch = train_val_set_size - split_size\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "\n",
        "# fold 1\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model_cv1 = build_classifier_model(0)\n",
        "classifier_model_cv1.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv1 = classifier_model_cv1.fit(x=cv_2.concatenate(cv_3).concatenate(cv_4).concatenate(cv_5),\n",
        "                               validation_data=cv_1,\n",
        "                               epochs=epcochs)\n",
        "history_dict_cv1 = history_cv1.history\n",
        "\n",
        "acc_fold.append(history_dict_cv1['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv1['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv1['loss'])\n",
        "val_loss_fold.append(history_dict_cv1['val_loss'])\n",
        "\n",
        "# fold 2\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model_cv2 = build_classifier_model(0)\n",
        "classifier_model_cv2.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv2 = classifier_model_cv2.fit(x=cv_1.concatenate(cv_3).concatenate(cv_4).concatenate(cv_5),\n",
        "                               validation_data=cv_2,\n",
        "                               epochs=epochs)\n",
        "history_dict_cv2 = history_cv2.history\n",
        "\n",
        "acc_fold.append(history_dict_cv2['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv2['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv2['loss'])\n",
        "val_loss_fold.append(history_dict_cv2['val_loss'])\n",
        "\n",
        "#fold 3\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model_cv3 = build_classifier_model(0)\n",
        "classifier_model_cv3.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv3 = classifier_model_cv3.fit(x=cv_2.concatenate(cv_1).concatenate(cv_4).concatenate(cv_5),\n",
        "                               validation_data=cv_3,\n",
        "                               epochs=epochs)\n",
        "history_dict_cv3 = history_cv3.history\n",
        "\n",
        "acc_fold.append(history_dict_cv3['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv3['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv3['loss'])\n",
        "val_loss_fold.append(history_dict_cv3['val_loss'])\n",
        "\n",
        "# fold 4\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model_cv4 = build_classifier_model(0)\n",
        "classifier_model_cv4.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv4 = classifier_model_cv4.fit(x=cv_2.concatenate(cv_3).concatenate(cv_1).concatenate(cv_5),\n",
        "                               validation_data=cv_4,\n",
        "                               epochs=epochs)\n",
        "history_dict_cv4 = history_cv4.history\n",
        "\n",
        "acc_fold.append(history_dict_cv4['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv4['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv4['loss'])\n",
        "val_loss_fold.append(history_dict_cv4['val_loss'])\n",
        "\n",
        "#fold 5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model_cv5 = build_classifier_model(0)\n",
        "classifier_model_cv5.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv5 = classifier_model_cv5.fit(x=cv_2.concatenate(cv_3).concatenate(cv_4).concatenate(cv_1),\n",
        "                               validation_data=cv_5,\n",
        "                               epochs=epochs)\n",
        "history_dict_cv5 = history_cv5.history\n",
        "\n",
        "acc_fold.append(history_dict_cv5['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv5['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv5['loss'])\n",
        "val_loss_fold.append(history_dict_cv5['val_loss'])\n",
        "\n",
        "# print\n",
        "print(acc_fold)\n",
        "print(val_acc_fold)\n",
        "print(loss_fold)\n",
        "print(val_loss_fold)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "I3WW2qqqiKq7"
      },
      "source": [
        "# redo entire grid search with early stopping and a set seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7NZVIM5GiKJe",
        "outputId": "510911d7-e0b4-45ae-d753-4e5c0b87482e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "458/458 [==============================] - 38s 65ms/step - loss: 0.3966 - binary_accuracy: 0.7761 - val_loss: 0.1758 - val_binary_accuracy: 0.9268\n",
            "Epoch 2/25\n",
            "458/458 [==============================] - 29s 64ms/step - loss: 0.1653 - binary_accuracy: 0.9418 - val_loss: 0.1148 - val_binary_accuracy: 0.9694\n",
            "Epoch 3/25\n",
            "458/458 [==============================] - 29s 63ms/step - loss: 0.1186 - binary_accuracy: 0.9650 - val_loss: 0.1278 - val_binary_accuracy: 0.9694\n",
            "Epoch 4/25\n",
            "458/458 [==============================] - 29s 64ms/step - loss: 0.1006 - binary_accuracy: 0.9717 - val_loss: 0.1099 - val_binary_accuracy: 0.9765\n",
            "Epoch 5/25\n",
            "458/458 [==============================] - 29s 64ms/step - loss: 0.0792 - binary_accuracy: 0.9780 - val_loss: 0.1550 - val_binary_accuracy: 0.9727\n",
            "Epoch 6/25\n",
            "458/458 [==============================] - 30s 65ms/step - loss: 0.0647 - binary_accuracy: 0.9822 - val_loss: 0.1366 - val_binary_accuracy: 0.9738\n",
            "Epoch 7/25\n",
            "458/458 [==============================] - 29s 63ms/step - loss: 0.0555 - binary_accuracy: 0.9861 - val_loss: 0.1291 - val_binary_accuracy: 0.9765\n",
            "Learning rate 0.001, Batch size 16\n",
            "[0.396634578704834, 0.16531239449977875, 0.11864294856786728, 0.10064691305160522, 0.07919860631227493, 0.06474296748638153, 0.055481698364019394]\n",
            "[0.1757851392030716, 0.11481743305921555, 0.12775851786136627, 0.10986020416021347, 0.15501189231872559, 0.1365904062986374, 0.129126638174057]\n",
            "[0.7760792374610901, 0.941811203956604, 0.9650321006774902, 0.9717251658439636, 0.9780084490776062, 0.9822428822517395, 0.9860674738883972]\n",
            "[0.9267759323120117, 0.9693989157676697, 0.9693989157676697, 0.9765027165412903, 0.9726775884628296, 0.9737704992294312, 0.9765027165412903]\n",
            "Epoch 1/25\n",
            "229/229 [==============================] - 40s 133ms/step - loss: 0.4911 - binary_accuracy: 0.7888 - val_loss: 0.2936 - val_binary_accuracy: 0.8885\n",
            "Epoch 2/25\n",
            "229/229 [==============================] - 30s 130ms/step - loss: 0.2429 - binary_accuracy: 0.9027 - val_loss: 0.1384 - val_binary_accuracy: 0.9514\n",
            "Epoch 3/25\n",
            "229/229 [==============================] - 30s 130ms/step - loss: 0.1386 - binary_accuracy: 0.9526 - val_loss: 0.1032 - val_binary_accuracy: 0.9661\n",
            "Epoch 4/25\n",
            "229/229 [==============================] - 30s 130ms/step - loss: 0.1058 - binary_accuracy: 0.9661 - val_loss: 0.1015 - val_binary_accuracy: 0.9678\n",
            "Epoch 5/25\n",
            "229/229 [==============================] - 30s 131ms/step - loss: 0.0908 - binary_accuracy: 0.9723 - val_loss: 0.1128 - val_binary_accuracy: 0.9689\n",
            "Epoch 6/25\n",
            "229/229 [==============================] - 30s 130ms/step - loss: 0.0789 - binary_accuracy: 0.9755 - val_loss: 0.0943 - val_binary_accuracy: 0.9738\n",
            "Epoch 7/25\n",
            "229/229 [==============================] - 30s 130ms/step - loss: 0.0576 - binary_accuracy: 0.9820 - val_loss: 0.1176 - val_binary_accuracy: 0.9699\n",
            "Epoch 8/25\n",
            "229/229 [==============================] - 30s 131ms/step - loss: 0.0480 - binary_accuracy: 0.9878 - val_loss: 0.1185 - val_binary_accuracy: 0.9749\n",
            "Epoch 9/25\n",
            "229/229 [==============================] - 30s 131ms/step - loss: 0.0411 - binary_accuracy: 0.9888 - val_loss: 0.1216 - val_binary_accuracy: 0.9754\n",
            "Learning rate 0.001, Batch size 32\n",
            "[0.4911327660083771, 0.24294762313365936, 0.13857638835906982, 0.10581885278224945, 0.09080558270215988, 0.07889357954263687, 0.05757739022374153, 0.04799847677350044, 0.04105551913380623]\n",
            "[0.29364481568336487, 0.13838891685009003, 0.10322900861501694, 0.10147999972105026, 0.11279039829969406, 0.09427528828382492, 0.1175709068775177, 0.11849280446767807, 0.12160468846559525]\n",
            "[0.7887662649154663, 0.9027455449104309, 0.9526020884513855, 0.9661248326301575, 0.9722715616226196, 0.9755498170852661, 0.9819696545600891, 0.9878432154655457, 0.9887993335723877]\n",
            "[0.88852459192276, 0.951366126537323, 0.9661202430725098, 0.9677595496177673, 0.9688524603843689, 0.9737704992294312, 0.9699453711509705, 0.9748634099960327, 0.9754098653793335]\n",
            "Epoch 1/25\n",
            "115/115 [==============================] - 48s 341ms/step - loss: 0.5692 - binary_accuracy: 0.7443 - val_loss: 0.4366 - val_binary_accuracy: 0.7486\n",
            "Epoch 2/25\n",
            "115/115 [==============================] - 37s 318ms/step - loss: 0.3730 - binary_accuracy: 0.8257 - val_loss: 0.2631 - val_binary_accuracy: 0.8918\n",
            "Epoch 3/25\n",
            "115/115 [==============================] - 38s 334ms/step - loss: 0.2426 - binary_accuracy: 0.9034 - val_loss: 0.1637 - val_binary_accuracy: 0.9372\n",
            "Epoch 4/25\n",
            "115/115 [==============================] - 37s 319ms/step - loss: 0.1639 - binary_accuracy: 0.9404 - val_loss: 0.1236 - val_binary_accuracy: 0.9557\n",
            "Epoch 5/25\n",
            "115/115 [==============================] - 37s 323ms/step - loss: 0.1247 - binary_accuracy: 0.9566 - val_loss: 0.0941 - val_binary_accuracy: 0.9683\n",
            "Epoch 6/25\n",
            "115/115 [==============================] - 38s 330ms/step - loss: 0.0974 - binary_accuracy: 0.9664 - val_loss: 0.0857 - val_binary_accuracy: 0.9721\n",
            "Epoch 7/25\n",
            "115/115 [==============================] - 37s 325ms/step - loss: 0.0819 - binary_accuracy: 0.9724 - val_loss: 0.0872 - val_binary_accuracy: 0.9738\n",
            "Epoch 8/25\n",
            "115/115 [==============================] - 38s 335ms/step - loss: 0.0664 - binary_accuracy: 0.9776 - val_loss: 0.0877 - val_binary_accuracy: 0.9732\n",
            "Epoch 9/25\n",
            "115/115 [==============================] - 37s 320ms/step - loss: 0.0659 - binary_accuracy: 0.9794 - val_loss: 0.0887 - val_binary_accuracy: 0.9738\n",
            "Learning rate 0.001, Batch size 64\n",
            "[0.5691555738449097, 0.37304824590682983, 0.24262923002243042, 0.16387419402599335, 0.12470295280218124, 0.09742774814367294, 0.08194320648908615, 0.06635450571775436, 0.06586360186338425]\n",
            "[0.4366432726383209, 0.2630705237388611, 0.16371510922908783, 0.1236436516046524, 0.0940651074051857, 0.08569075912237167, 0.08723790943622589, 0.08772221952676773, 0.08868424594402313]\n",
            "[0.7442902326583862, 0.8257068991661072, 0.9034284949302673, 0.9404453039169312, 0.9565632939338684, 0.9663980603218079, 0.9724081158638, 0.9775986671447754, 0.9793744087219238]\n",
            "[0.748633861541748, 0.8918032646179199, 0.937158465385437, 0.9557377099990845, 0.9683060050010681, 0.9721311330795288, 0.9737704992294312, 0.9732240438461304, 0.9737704992294312]\n",
            "Epoch 1/25\n",
            "58/58 [==============================] - 63s 924ms/step - loss: 0.6182 - binary_accuracy: 0.7312 - val_loss: 0.5523 - val_binary_accuracy: 0.6721\n",
            "Epoch 2/25\n",
            "58/58 [==============================] - 54s 930ms/step - loss: 0.4985 - binary_accuracy: 0.7157 - val_loss: 0.4059 - val_binary_accuracy: 0.7967\n",
            "Epoch 3/25\n",
            "58/58 [==============================] - 54s 936ms/step - loss: 0.3868 - binary_accuracy: 0.8192 - val_loss: 0.2979 - val_binary_accuracy: 0.8770\n",
            "Epoch 4/25\n",
            "58/58 [==============================] - 52s 903ms/step - loss: 0.2986 - binary_accuracy: 0.8741 - val_loss: 0.2289 - val_binary_accuracy: 0.9104\n",
            "Epoch 5/25\n",
            "58/58 [==============================] - 54s 936ms/step - loss: 0.2275 - binary_accuracy: 0.9093 - val_loss: 0.1675 - val_binary_accuracy: 0.9361\n",
            "Epoch 6/25\n",
            "58/58 [==============================] - 53s 911ms/step - loss: 0.1750 - binary_accuracy: 0.9358 - val_loss: 0.1328 - val_binary_accuracy: 0.9492\n",
            "Epoch 7/25\n",
            "58/58 [==============================] - 52s 905ms/step - loss: 0.1418 - binary_accuracy: 0.9507 - val_loss: 0.1110 - val_binary_accuracy: 0.9617\n",
            "Epoch 8/25\n",
            "58/58 [==============================] - 54s 938ms/step - loss: 0.1168 - binary_accuracy: 0.9615 - val_loss: 0.0998 - val_binary_accuracy: 0.9667\n",
            "Epoch 9/25\n",
            "58/58 [==============================] - 52s 908ms/step - loss: 0.1012 - binary_accuracy: 0.9649 - val_loss: 0.0949 - val_binary_accuracy: 0.9678\n",
            "Epoch 10/25\n",
            "58/58 [==============================] - 54s 921ms/step - loss: 0.0886 - binary_accuracy: 0.9690 - val_loss: 0.1014 - val_binary_accuracy: 0.9678\n",
            "Epoch 11/25\n",
            "58/58 [==============================] - 54s 932ms/step - loss: 0.0821 - binary_accuracy: 0.9735 - val_loss: 0.0945 - val_binary_accuracy: 0.9721\n",
            "Epoch 12/25\n",
            "58/58 [==============================] - 52s 901ms/step - loss: 0.0792 - binary_accuracy: 0.9721 - val_loss: 0.0871 - val_binary_accuracy: 0.9738\n",
            "Epoch 13/25\n",
            "58/58 [==============================] - 54s 930ms/step - loss: 0.0621 - binary_accuracy: 0.9795 - val_loss: 0.0928 - val_binary_accuracy: 0.9743\n",
            "Epoch 14/25\n",
            "58/58 [==============================] - 52s 900ms/step - loss: 0.0584 - binary_accuracy: 0.9809 - val_loss: 0.0950 - val_binary_accuracy: 0.9743\n",
            "Epoch 15/25\n",
            "58/58 [==============================] - 54s 929ms/step - loss: 0.0546 - binary_accuracy: 0.9828 - val_loss: 0.0880 - val_binary_accuracy: 0.9738\n",
            "Learning rate 0.001, Batch size 128\n",
            "[0.6182397603988647, 0.49849650263786316, 0.38679659366607666, 0.2986372709274292, 0.22751891613006592, 0.17503471672534943, 0.14178387820720673, 0.1167585626244545, 0.10119308531284332, 0.08860287070274353, 0.08208051323890686, 0.07918105274438858, 0.062080420553684235, 0.0583985336124897, 0.054596010595560074]\n",
            "[0.5522770881652832, 0.40588250756263733, 0.2979070842266083, 0.22891147434711456, 0.16754606366157532, 0.1327722668647766, 0.1110057532787323, 0.09975085407495499, 0.09494856745004654, 0.10144619643688202, 0.09450536966323853, 0.08712875097990036, 0.09282489120960236, 0.09502622485160828, 0.08797602355480194]\n",
            "[0.7311769127845764, 0.715749204158783, 0.8191503882408142, 0.8740609288215637, 0.9093019962310791, 0.9358011484146118, 0.9506897926330566, 0.9614806771278381, 0.964895486831665, 0.9689933061599731, 0.9735009074211121, 0.9721349477767944, 0.979511022567749, 0.9808769226074219, 0.9827892184257507]\n",
            "[0.6721311211585999, 0.796721339225769, 0.8770492076873779, 0.9103825092315674, 0.9360655546188354, 0.9491803050041199, 0.9617486596107483, 0.9666666388511658, 0.9677595496177673, 0.9677595496177673, 0.9721311330795288, 0.9737704992294312, 0.9743169546127319, 0.9743169546127319, 0.9737704992294312]\n",
            "Epoch 1/25\n",
            "29/29 [==============================] - 97s 3s/step - loss: 0.6398 - binary_accuracy: 0.7311 - val_loss: 0.6225 - val_binary_accuracy: 0.6710\n",
            "Epoch 2/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.5918 - binary_accuracy: 0.6705 - val_loss: 0.5459 - val_binary_accuracy: 0.6727\n",
            "Epoch 3/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.5267 - binary_accuracy: 0.6932 - val_loss: 0.4636 - val_binary_accuracy: 0.7273\n",
            "Epoch 4/25\n",
            "29/29 [==============================] - 86s 3s/step - loss: 0.4573 - binary_accuracy: 0.7524 - val_loss: 0.3877 - val_binary_accuracy: 0.8180\n",
            "Epoch 5/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.3903 - binary_accuracy: 0.8211 - val_loss: 0.3210 - val_binary_accuracy: 0.8661\n",
            "Epoch 6/25\n",
            "29/29 [==============================] - 86s 3s/step - loss: 0.3305 - binary_accuracy: 0.8575 - val_loss: 0.2681 - val_binary_accuracy: 0.8962\n",
            "Epoch 7/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.2791 - binary_accuracy: 0.8853 - val_loss: 0.2254 - val_binary_accuracy: 0.9142\n",
            "Epoch 8/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.2399 - binary_accuracy: 0.9066 - val_loss: 0.1867 - val_binary_accuracy: 0.9230\n",
            "Epoch 9/25\n",
            "29/29 [==============================] - 86s 3s/step - loss: 0.2031 - binary_accuracy: 0.9206 - val_loss: 0.1546 - val_binary_accuracy: 0.9426\n",
            "Epoch 10/25\n",
            "29/29 [==============================] - 85s 3s/step - loss: 0.1746 - binary_accuracy: 0.9358 - val_loss: 0.1343 - val_binary_accuracy: 0.9519\n",
            "Epoch 11/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.1486 - binary_accuracy: 0.9463 - val_loss: 0.1197 - val_binary_accuracy: 0.9590\n",
            "Epoch 12/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.1334 - binary_accuracy: 0.9533 - val_loss: 0.1066 - val_binary_accuracy: 0.9623\n",
            "Epoch 13/25\n",
            "29/29 [==============================] - 86s 3s/step - loss: 0.1115 - binary_accuracy: 0.9622 - val_loss: 0.0986 - val_binary_accuracy: 0.9667\n",
            "Epoch 14/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.1021 - binary_accuracy: 0.9653 - val_loss: 0.0954 - val_binary_accuracy: 0.9694\n",
            "Epoch 15/25\n",
            "29/29 [==============================] - 86s 3s/step - loss: 0.0959 - binary_accuracy: 0.9689 - val_loss: 0.0897 - val_binary_accuracy: 0.9705\n",
            "Epoch 16/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.0908 - binary_accuracy: 0.9702 - val_loss: 0.0889 - val_binary_accuracy: 0.9710\n",
            "Epoch 17/25\n",
            "29/29 [==============================] - 86s 3s/step - loss: 0.0840 - binary_accuracy: 0.9728 - val_loss: 0.0872 - val_binary_accuracy: 0.9705\n",
            "Epoch 18/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.0744 - binary_accuracy: 0.9746 - val_loss: 0.0864 - val_binary_accuracy: 0.9727\n",
            "Epoch 19/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.0732 - binary_accuracy: 0.9753 - val_loss: 0.0872 - val_binary_accuracy: 0.9727\n",
            "Epoch 20/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.0645 - binary_accuracy: 0.9772 - val_loss: 0.0856 - val_binary_accuracy: 0.9727\n",
            "Epoch 21/25\n",
            "29/29 [==============================] - 85s 3s/step - loss: 0.0626 - binary_accuracy: 0.9784 - val_loss: 0.0852 - val_binary_accuracy: 0.9716\n",
            "Epoch 22/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.0559 - binary_accuracy: 0.9822 - val_loss: 0.0865 - val_binary_accuracy: 0.9721\n",
            "Epoch 23/25\n",
            "29/29 [==============================] - 86s 3s/step - loss: 0.0528 - binary_accuracy: 0.9824 - val_loss: 0.0836 - val_binary_accuracy: 0.9754\n",
            "Epoch 24/25\n",
            "29/29 [==============================] - 88s 3s/step - loss: 0.0493 - binary_accuracy: 0.9852 - val_loss: 0.0917 - val_binary_accuracy: 0.9727\n",
            "Epoch 25/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.0452 - binary_accuracy: 0.9848 - val_loss: 0.0874 - val_binary_accuracy: 0.9738\n",
            "Learning rate 0.001, Batch size 256\n",
            "[0.6398426294326782, 0.5917611122131348, 0.5267063975334167, 0.4572838544845581, 0.3902897834777832, 0.3305295705795288, 0.2791253328323364, 0.23985496163368225, 0.20312456786632538, 0.17455002665519714, 0.14856402575969696, 0.13341361284255981, 0.111490398645401, 0.10214545577764511, 0.09593465179204941, 0.09079346805810928, 0.08401758223772049, 0.07435187697410583, 0.07324814796447754, 0.06450590491294861, 0.0625898540019989, 0.0559249147772789, 0.052821848541498184, 0.049266327172517776, 0.045202720910310745]\n",
            "[0.6225422620773315, 0.5458610653877258, 0.4635699391365051, 0.3877457082271576, 0.32100582122802734, 0.26805782318115234, 0.22543315589427948, 0.18669244647026062, 0.15458640456199646, 0.13431651890277863, 0.11973220854997635, 0.10659587383270264, 0.09856908768415451, 0.09542249143123627, 0.08967794477939606, 0.08890167623758316, 0.08717311173677444, 0.08642823249101639, 0.08723955601453781, 0.08563178777694702, 0.08515079319477081, 0.08654586970806122, 0.08360397815704346, 0.09173744916915894, 0.08742004632949829]\n",
            "[0.7310676574707031, 0.6705368161201477, 0.6932113170623779, 0.752356231212616, 0.8210626840591431, 0.8575330972671509, 0.885261595249176, 0.9065701365470886, 0.9206392765045166, 0.9358011484146118, 0.9463188052177429, 0.9532850980758667, 0.9621636271476746, 0.9653052687644958, 0.968856692314148, 0.9702226519584656, 0.9728178977966309, 0.9745936393737793, 0.9752765893936157, 0.9771888852119446, 0.978418231010437, 0.9822428822517395, 0.9823794364929199, 0.9852479100227356, 0.9848381280899048]\n",
            "[0.6710382699966431, 0.6726775765419006, 0.7273223996162415, 0.8180328011512756, 0.8661202192306519, 0.8961748480796814, 0.9142076373100281, 0.922950804233551, 0.9426229596138, 0.9519125819206238, 0.9590163826942444, 0.9622950553894043, 0.9666666388511658, 0.9693989157676697, 0.9704918265342712, 0.9710382223129272, 0.9704918265342712, 0.9726775884628296, 0.9726775884628296, 0.9726775884628296, 0.971584677696228, 0.9721311330795288, 0.9754098653793335, 0.9726775884628296, 0.9737704992294312]\n",
            "Epoch 1/25\n",
            "458/458 [==============================] - 40s 66ms/step - loss: 0.4539 - binary_accuracy: 0.8094 - val_loss: 0.2494 - val_binary_accuracy: 0.9033\n",
            "Epoch 2/25\n",
            "458/458 [==============================] - 29s 64ms/step - loss: 0.2126 - binary_accuracy: 0.9165 - val_loss: 0.1344 - val_binary_accuracy: 0.9590\n",
            "Epoch 3/25\n",
            "458/458 [==============================] - 30s 65ms/step - loss: 0.1331 - binary_accuracy: 0.9587 - val_loss: 0.1235 - val_binary_accuracy: 0.9656\n",
            "Epoch 4/25\n",
            "458/458 [==============================] - 30s 65ms/step - loss: 0.1122 - binary_accuracy: 0.9680 - val_loss: 0.1223 - val_binary_accuracy: 0.9699\n",
            "Epoch 5/25\n",
            "458/458 [==============================] - 30s 65ms/step - loss: 0.0918 - binary_accuracy: 0.9757 - val_loss: 0.1337 - val_binary_accuracy: 0.9716\n",
            "Epoch 6/25\n",
            "458/458 [==============================] - 30s 65ms/step - loss: 0.0728 - binary_accuracy: 0.9818 - val_loss: 0.1431 - val_binary_accuracy: 0.9721\n",
            "Epoch 7/25\n",
            "458/458 [==============================] - 30s 65ms/step - loss: 0.0602 - binary_accuracy: 0.9842 - val_loss: 0.1595 - val_binary_accuracy: 0.9716\n",
            "Learning rate 0.0005, Batch size 16\n",
            "[0.45385852456092834, 0.2125970721244812, 0.13307277858257294, 0.11220826953649521, 0.09180909395217896, 0.07279949635267258, 0.06021837145090103]\n",
            "[0.24944965541362762, 0.13440707325935364, 0.12353669852018356, 0.12225645780563354, 0.13371336460113525, 0.14307481050491333, 0.15945963561534882]\n",
            "[0.8094197511672974, 0.9165414571762085, 0.9587488174438477, 0.9680371284484863, 0.9756863713264465, 0.9818331003189087, 0.9841551780700684]\n",
            "[0.9032787084579468, 0.9590163826942444, 0.965573787689209, 0.9699453711509705, 0.971584677696228, 0.9721311330795288, 0.971584677696228]\n",
            "Epoch 1/25\n",
            "229/229 [==============================] - 39s 134ms/step - loss: 0.5369 - binary_accuracy: 0.7579 - val_loss: 0.3850 - val_binary_accuracy: 0.8027\n",
            "Epoch 2/25\n",
            "229/229 [==============================] - 30s 130ms/step - loss: 0.3217 - binary_accuracy: 0.8608 - val_loss: 0.2064 - val_binary_accuracy: 0.9191\n",
            "Epoch 3/25\n",
            "229/229 [==============================] - 30s 130ms/step - loss: 0.1954 - binary_accuracy: 0.9243 - val_loss: 0.1330 - val_binary_accuracy: 0.9519\n",
            "Epoch 4/25\n",
            "229/229 [==============================] - 30s 131ms/step - loss: 0.1318 - binary_accuracy: 0.9526 - val_loss: 0.1025 - val_binary_accuracy: 0.9650\n",
            "Epoch 5/25\n",
            "229/229 [==============================] - 30s 130ms/step - loss: 0.1043 - binary_accuracy: 0.9646 - val_loss: 0.0929 - val_binary_accuracy: 0.9694\n",
            "Epoch 6/25\n",
            "229/229 [==============================] - 30s 131ms/step - loss: 0.0894 - binary_accuracy: 0.9691 - val_loss: 0.0950 - val_binary_accuracy: 0.9721\n",
            "Epoch 7/25\n",
            "229/229 [==============================] - 30s 130ms/step - loss: 0.0725 - binary_accuracy: 0.9757 - val_loss: 0.1041 - val_binary_accuracy: 0.9699\n",
            "Epoch 8/25\n",
            "229/229 [==============================] - 30s 130ms/step - loss: 0.0693 - binary_accuracy: 0.9776 - val_loss: 0.0979 - val_binary_accuracy: 0.9738\n",
            "Learning rate 0.0005, Batch size 32\n",
            "[0.5369048714637756, 0.3217288851737976, 0.1953885704278946, 0.13180547952651978, 0.10430524498224258, 0.08936053514480591, 0.07254891097545624, 0.06932373344898224]\n",
            "[0.38498637080192566, 0.20636016130447388, 0.13298067450523376, 0.10250112414360046, 0.09290903806686401, 0.0949968621134758, 0.10411997139453888, 0.09788818657398224]\n",
            "[0.757949948310852, 0.8608113527297974, 0.9243272542953491, 0.9526020884513855, 0.9646223187446594, 0.9691299200057983, 0.9756863713264465, 0.9775986671447754]\n",
            "[0.8027322292327881, 0.9191256761550903, 0.9519125819206238, 0.9650273323059082, 0.9693989157676697, 0.9721311330795288, 0.9699453711509705, 0.9737704992294312]\n",
            "Epoch 1/25\n",
            "115/115 [==============================] - 46s 333ms/step - loss: 0.5967 - binary_accuracy: 0.7324 - val_loss: 0.5019 - val_binary_accuracy: 0.6880\n",
            "Epoch 2/25\n",
            "115/115 [==============================] - 38s 331ms/step - loss: 0.4464 - binary_accuracy: 0.7641 - val_loss: 0.3444 - val_binary_accuracy: 0.8404\n",
            "Epoch 3/25\n",
            "115/115 [==============================] - 37s 321ms/step - loss: 0.3270 - binary_accuracy: 0.8579 - val_loss: 0.2436 - val_binary_accuracy: 0.9066\n",
            "Epoch 4/25\n",
            "115/115 [==============================] - 38s 333ms/step - loss: 0.2418 - binary_accuracy: 0.9041 - val_loss: 0.1730 - val_binary_accuracy: 0.9295\n",
            "Epoch 5/25\n",
            "115/115 [==============================] - 37s 320ms/step - loss: 0.1779 - binary_accuracy: 0.9350 - val_loss: 0.1319 - val_binary_accuracy: 0.9503\n",
            "Epoch 6/25\n",
            "115/115 [==============================] - 39s 335ms/step - loss: 0.1372 - binary_accuracy: 0.9506 - val_loss: 0.1121 - val_binary_accuracy: 0.9617\n",
            "Epoch 7/25\n",
            "115/115 [==============================] - 37s 320ms/step - loss: 0.1078 - binary_accuracy: 0.9620 - val_loss: 0.0971 - val_binary_accuracy: 0.9683\n",
            "Epoch 8/25\n",
            "115/115 [==============================] - 37s 324ms/step - loss: 0.0975 - binary_accuracy: 0.9686 - val_loss: 0.0933 - val_binary_accuracy: 0.9683\n",
            "Epoch 9/25\n",
            "115/115 [==============================] - 38s 332ms/step - loss: 0.0832 - binary_accuracy: 0.9713 - val_loss: 0.0918 - val_binary_accuracy: 0.9749\n",
            "Epoch 10/25\n",
            "115/115 [==============================] - 37s 321ms/step - loss: 0.0761 - binary_accuracy: 0.9738 - val_loss: 0.0884 - val_binary_accuracy: 0.9743\n",
            "Epoch 11/25\n",
            "115/115 [==============================] - 39s 336ms/step - loss: 0.0657 - binary_accuracy: 0.9775 - val_loss: 0.0876 - val_binary_accuracy: 0.9721\n",
            "Epoch 12/25\n",
            "115/115 [==============================] - 37s 318ms/step - loss: 0.0588 - binary_accuracy: 0.9803 - val_loss: 0.0957 - val_binary_accuracy: 0.9732\n",
            "Epoch 13/25\n",
            "115/115 [==============================] - 38s 329ms/step - loss: 0.0506 - binary_accuracy: 0.9818 - val_loss: 0.0986 - val_binary_accuracy: 0.9727\n",
            "Epoch 14/25\n",
            "115/115 [==============================] - 42s 362ms/step - loss: 0.0377 - binary_accuracy: 0.9881 - val_loss: 0.1083 - val_binary_accuracy: 0.9699\n",
            "Learning rate 0.0005, Batch size 64\n",
            "[0.5966584086418152, 0.44637492299079895, 0.3269968628883362, 0.2418011873960495, 0.177945077419281, 0.1372424066066742, 0.10775166749954224, 0.0974762886762619, 0.08319656550884247, 0.07611789554357529, 0.06568480283021927, 0.058765705674886703, 0.0505816787481308, 0.037666600197553635]\n",
            "[0.5018740296363831, 0.34435322880744934, 0.2436446100473404, 0.17299534380435944, 0.13188126683235168, 0.11207513511180878, 0.09708547592163086, 0.09333281964063644, 0.09178677201271057, 0.08835410326719284, 0.08761077374219894, 0.09571481496095657, 0.09860116243362427, 0.10830368101596832]\n",
            "[0.7323789596557617, 0.7641032934188843, 0.8579428791999817, 0.9041114449501038, 0.9349815845489502, 0.9505531787872314, 0.9620270729064941, 0.9685835242271423, 0.9713153839111328, 0.9737740755081177, 0.977462112903595, 0.9803305268287659, 0.9818331003189087, 0.9881163835525513]\n",
            "[0.6879781484603882, 0.8404371738433838, 0.9065573811531067, 0.9295082092285156, 0.9502732157707214, 0.9617486596107483, 0.9683060050010681, 0.9683060050010681, 0.9748634099960327, 0.9743169546127319, 0.9721311330795288, 0.9732240438461304, 0.9726775884628296, 0.9699453711509705]\n",
            "Epoch 1/25\n",
            "58/58 [==============================] - 63s 946ms/step - loss: 0.6330 - binary_accuracy: 0.7303 - val_loss: 0.5965 - val_binary_accuracy: 0.6710\n",
            "Epoch 2/25\n",
            "58/58 [==============================] - 54s 935ms/step - loss: 0.5539 - binary_accuracy: 0.6782 - val_loss: 0.4852 - val_binary_accuracy: 0.7066\n",
            "Epoch 3/25\n",
            "58/58 [==============================] - 53s 904ms/step - loss: 0.4655 - binary_accuracy: 0.7427 - val_loss: 0.3885 - val_binary_accuracy: 0.8104\n",
            "Epoch 4/25\n",
            "58/58 [==============================] - 54s 937ms/step - loss: 0.3838 - binary_accuracy: 0.8174 - val_loss: 0.3118 - val_binary_accuracy: 0.8705\n",
            "Epoch 5/25\n",
            "58/58 [==============================] - 53s 915ms/step - loss: 0.3162 - binary_accuracy: 0.8656 - val_loss: 0.2536 - val_binary_accuracy: 0.9005\n",
            "Epoch 6/25\n",
            "58/58 [==============================] - 52s 905ms/step - loss: 0.2598 - binary_accuracy: 0.8935 - val_loss: 0.2045 - val_binary_accuracy: 0.9164\n",
            "Epoch 7/25\n",
            "58/58 [==============================] - 54s 939ms/step - loss: 0.2175 - binary_accuracy: 0.9153 - val_loss: 0.1652 - val_binary_accuracy: 0.9383\n",
            "Epoch 8/25\n",
            "58/58 [==============================] - 53s 924ms/step - loss: 0.1827 - binary_accuracy: 0.9318 - val_loss: 0.1346 - val_binary_accuracy: 0.9508\n",
            "Epoch 9/25\n",
            "58/58 [==============================] - 53s 916ms/step - loss: 0.1499 - binary_accuracy: 0.9451 - val_loss: 0.1200 - val_binary_accuracy: 0.9585\n",
            "Epoch 10/25\n",
            "58/58 [==============================] - 54s 940ms/step - loss: 0.1301 - binary_accuracy: 0.9557 - val_loss: 0.1083 - val_binary_accuracy: 0.9617\n",
            "Epoch 11/25\n",
            "58/58 [==============================] - 52s 905ms/step - loss: 0.1143 - binary_accuracy: 0.9619 - val_loss: 0.0954 - val_binary_accuracy: 0.9672\n",
            "Epoch 12/25\n",
            "58/58 [==============================] - 54s 941ms/step - loss: 0.1083 - binary_accuracy: 0.9635 - val_loss: 0.0901 - val_binary_accuracy: 0.9694\n",
            "Epoch 13/25\n",
            "58/58 [==============================] - 54s 936ms/step - loss: 0.0924 - binary_accuracy: 0.9684 - val_loss: 0.0893 - val_binary_accuracy: 0.9716\n",
            "Epoch 14/25\n",
            "58/58 [==============================] - 52s 902ms/step - loss: 0.0855 - binary_accuracy: 0.9702 - val_loss: 0.0864 - val_binary_accuracy: 0.9710\n",
            "Epoch 15/25\n",
            "58/58 [==============================] - 54s 935ms/step - loss: 0.0769 - binary_accuracy: 0.9746 - val_loss: 0.0900 - val_binary_accuracy: 0.9721\n",
            "Epoch 16/25\n",
            "58/58 [==============================] - 54s 938ms/step - loss: 0.0743 - binary_accuracy: 0.9745 - val_loss: 0.0898 - val_binary_accuracy: 0.9727\n",
            "Epoch 17/25\n",
            "58/58 [==============================] - 53s 910ms/step - loss: 0.0667 - binary_accuracy: 0.9777 - val_loss: 0.0913 - val_binary_accuracy: 0.9732\n",
            "Learning rate 0.0005, Batch size 128\n",
            "[0.6329538226127625, 0.5538983345031738, 0.4654710292816162, 0.3837708830833435, 0.316163569688797, 0.25978705286979675, 0.21746011078357697, 0.18270021677017212, 0.14987905323505402, 0.13012148439884186, 0.11427982151508331, 0.10830876231193542, 0.09237011522054672, 0.08554312586784363, 0.07694866508245468, 0.07428737729787827, 0.0666801854968071]\n",
            "[0.5965470671653748, 0.48516643047332764, 0.3884705901145935, 0.3117825388908386, 0.2535881996154785, 0.2045055329799652, 0.16517023742198944, 0.13463303446769714, 0.12004256248474121, 0.10833394527435303, 0.09536514431238174, 0.09008777886629105, 0.08928114175796509, 0.08635295927524567, 0.08999524265527725, 0.0897558405995369, 0.0912582129240036]\n",
            "[0.7303026914596558, 0.6781860589981079, 0.7426580786705017, 0.8173746466636658, 0.8655921220779419, 0.8934571743011475, 0.9153121113777161, 0.9318398833274841, 0.9450894594192505, 0.9557437300682068, 0.961890459060669, 0.9635295867919922, 0.9684469103813171, 0.9702226519584656, 0.9745936393737793, 0.9744570255279541, 0.9777352809906006]\n",
            "[0.6710382699966431, 0.7065573930740356, 0.8103824853897095, 0.8704918026924133, 0.9005464315414429, 0.9163934588432312, 0.9382513761520386, 0.9508196711540222, 0.9584699273109436, 0.9617486596107483, 0.9672130942344666, 0.9693989157676697, 0.971584677696228, 0.9710382223129272, 0.9721311330795288, 0.9726775884628296, 0.9732240438461304]\n",
            "Epoch 1/25\n",
            "29/29 [==============================] - 97s 3s/step - loss: 0.6409 - binary_accuracy: 0.7310 - val_loss: 0.6374 - val_binary_accuracy: 0.6710\n",
            "Epoch 2/25\n",
            "29/29 [==============================] - 86s 3s/step - loss: 0.6191 - binary_accuracy: 0.6704 - val_loss: 0.5930 - val_binary_accuracy: 0.6710\n",
            "Epoch 3/25\n",
            "29/29 [==============================] - 86s 3s/step - loss: 0.5720 - binary_accuracy: 0.6723 - val_loss: 0.5304 - val_binary_accuracy: 0.6738\n",
            "Epoch 4/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.5199 - binary_accuracy: 0.6903 - val_loss: 0.4702 - val_binary_accuracy: 0.7175\n",
            "Epoch 5/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.4745 - binary_accuracy: 0.7373 - val_loss: 0.4159 - val_binary_accuracy: 0.7792\n",
            "Epoch 6/25\n",
            "29/29 [==============================] - 90s 3s/step - loss: 0.4216 - binary_accuracy: 0.7888 - val_loss: 0.3637 - val_binary_accuracy: 0.8306\n",
            "Epoch 7/25\n",
            "29/29 [==============================] - 86s 3s/step - loss: 0.3762 - binary_accuracy: 0.8282 - val_loss: 0.3187 - val_binary_accuracy: 0.8678\n",
            "Epoch 8/25\n",
            "29/29 [==============================] - 86s 3s/step - loss: 0.3378 - binary_accuracy: 0.8532 - val_loss: 0.2816 - val_binary_accuracy: 0.8907\n",
            "Epoch 9/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.3024 - binary_accuracy: 0.8753 - val_loss: 0.2490 - val_binary_accuracy: 0.9087\n",
            "Epoch 10/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.2675 - binary_accuracy: 0.8915 - val_loss: 0.2200 - val_binary_accuracy: 0.9153\n",
            "Epoch 11/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.2424 - binary_accuracy: 0.9066 - val_loss: 0.1928 - val_binary_accuracy: 0.9257\n",
            "Epoch 12/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.2134 - binary_accuracy: 0.9174 - val_loss: 0.1689 - val_binary_accuracy: 0.9355\n",
            "Epoch 13/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.1886 - binary_accuracy: 0.9280 - val_loss: 0.1486 - val_binary_accuracy: 0.9426\n",
            "Epoch 14/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.1704 - binary_accuracy: 0.9381 - val_loss: 0.1318 - val_binary_accuracy: 0.9546\n",
            "Epoch 15/25\n",
            "29/29 [==============================] - 86s 3s/step - loss: 0.1502 - binary_accuracy: 0.9470 - val_loss: 0.1181 - val_binary_accuracy: 0.9590\n",
            "Epoch 16/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.1342 - binary_accuracy: 0.9551 - val_loss: 0.1083 - val_binary_accuracy: 0.9617\n",
            "Epoch 17/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.1271 - binary_accuracy: 0.9564 - val_loss: 0.1020 - val_binary_accuracy: 0.9656\n",
            "Epoch 18/25\n",
            "29/29 [==============================] - 86s 3s/step - loss: 0.1160 - binary_accuracy: 0.9605 - val_loss: 0.0973 - val_binary_accuracy: 0.9656\n",
            "Epoch 19/25\n",
            "29/29 [==============================] - 86s 3s/step - loss: 0.1045 - binary_accuracy: 0.9661 - val_loss: 0.0952 - val_binary_accuracy: 0.9678\n",
            "Epoch 20/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.1035 - binary_accuracy: 0.9660 - val_loss: 0.0911 - val_binary_accuracy: 0.9683\n",
            "Epoch 21/25\n",
            "29/29 [==============================] - 86s 3s/step - loss: 0.0950 - binary_accuracy: 0.9684 - val_loss: 0.0888 - val_binary_accuracy: 0.9694\n",
            "Epoch 22/25\n",
            "29/29 [==============================] - 88s 3s/step - loss: 0.0877 - binary_accuracy: 0.9704 - val_loss: 0.0878 - val_binary_accuracy: 0.9705\n",
            "Epoch 23/25\n",
            "29/29 [==============================] - 86s 3s/step - loss: 0.0849 - binary_accuracy: 0.9701 - val_loss: 0.0877 - val_binary_accuracy: 0.9705\n",
            "Epoch 24/25\n",
            "29/29 [==============================] - 87s 3s/step - loss: 0.0814 - binary_accuracy: 0.9719 - val_loss: 0.0863 - val_binary_accuracy: 0.9716\n",
            "Epoch 25/25\n",
            "29/29 [==============================] - 88s 3s/step - loss: 0.0746 - binary_accuracy: 0.9751 - val_loss: 0.0881 - val_binary_accuracy: 0.9699\n",
            "Learning rate 0.0005, Batch size 256\n",
            "[0.6408715844154358, 0.6191315054893494, 0.5720143914222717, 0.5199471712112427, 0.474530428647995, 0.42155149579048157, 0.37624168395996094, 0.3378256857395172, 0.30244335532188416, 0.2675185203552246, 0.2423652559518814, 0.2134408801794052, 0.1886136680841446, 0.17036442458629608, 0.15021415054798126, 0.13417306542396545, 0.12709510326385498, 0.11601977795362473, 0.10446921736001968, 0.10349129140377045, 0.09502299875020981, 0.08765937387943268, 0.08491018414497375, 0.08141034841537476, 0.07464874535799026]\n",
            "[0.637413501739502, 0.5929561853408813, 0.5304480195045471, 0.4701739549636841, 0.41586631536483765, 0.3636794984340668, 0.31870806217193604, 0.28156933188438416, 0.24896720051765442, 0.21999391913414001, 0.19278497993946075, 0.16893288493156433, 0.14864113926887512, 0.13182023167610168, 0.11813323944807053, 0.10831113904714584, 0.1019945740699768, 0.09734885394573212, 0.09521118551492691, 0.09107568114995956, 0.08875102549791336, 0.08780042082071304, 0.08770743757486343, 0.08629224449396133, 0.0880802646279335]\n",
            "[0.7309583425521851, 0.6704002022743225, 0.6723124980926514, 0.6903428435325623, 0.737330973148346, 0.7888266444206238, 0.8281655311584473, 0.8531621098518372, 0.8752902746200562, 0.8915448784828186, 0.9065701365470886, 0.9173610210418701, 0.9280152916908264, 0.9381232261657715, 0.9470017552375793, 0.9550607800483704, 0.956426739692688, 0.9605244994163513, 0.9661248326301575, 0.965988278388977, 0.9684469103813171, 0.9703592658042908, 0.9700860381126404, 0.9718617796897888, 0.9751400351524353]\n",
            "[0.6710382699966431, 0.6710382699966431, 0.6737704873085022, 0.7174863219261169, 0.7792349457740784, 0.8306010961532593, 0.8677595853805542, 0.8907103538513184, 0.908743143081665, 0.9153005480766296, 0.9256830811500549, 0.9355190992355347, 0.9426229596138, 0.9546447992324829, 0.9590163826942444, 0.9617486596107483, 0.965573787689209, 0.965573787689209, 0.9677595496177673, 0.9683060050010681, 0.9693989157676697, 0.9704918265342712, 0.9704918265342712, 0.971584677696228, 0.9699453711509705]\n",
            "Epoch 1/25\n",
            "458/458 [==============================] - 40s 68ms/step - loss: 0.5648 - binary_accuracy: 0.7399 - val_loss: 0.4401 - val_binary_accuracy: 0.7404\n",
            "Epoch 2/25\n",
            "458/458 [==============================] - 30s 66ms/step - loss: 0.3822 - binary_accuracy: 0.8181 - val_loss: 0.2761 - val_binary_accuracy: 0.8852\n",
            "Epoch 3/25\n",
            "458/458 [==============================] - 30s 66ms/step - loss: 0.2621 - binary_accuracy: 0.8940 - val_loss: 0.1760 - val_binary_accuracy: 0.9306\n",
            "Epoch 4/25\n",
            "458/458 [==============================] - 30s 66ms/step - loss: 0.1821 - binary_accuracy: 0.9287 - val_loss: 0.1308 - val_binary_accuracy: 0.9568\n",
            "Epoch 5/25\n",
            "458/458 [==============================] - 30s 66ms/step - loss: 0.1426 - binary_accuracy: 0.9526 - val_loss: 0.1247 - val_binary_accuracy: 0.9634\n",
            "Epoch 6/25\n",
            "458/458 [==============================] - 30s 66ms/step - loss: 0.1197 - binary_accuracy: 0.9631 - val_loss: 0.1176 - val_binary_accuracy: 0.9694\n",
            "Epoch 7/25\n",
            "458/458 [==============================] - 30s 66ms/step - loss: 0.1134 - binary_accuracy: 0.9664 - val_loss: 0.1177 - val_binary_accuracy: 0.9699\n",
            "Epoch 8/25\n",
            "458/458 [==============================] - 30s 66ms/step - loss: 0.0964 - binary_accuracy: 0.9715 - val_loss: 0.1131 - val_binary_accuracy: 0.9721\n",
            "Epoch 9/25\n",
            "458/458 [==============================] - 30s 66ms/step - loss: 0.0903 - binary_accuracy: 0.9768 - val_loss: 0.1354 - val_binary_accuracy: 0.9716\n",
            "Epoch 10/25\n",
            "458/458 [==============================] - 31s 67ms/step - loss: 0.0778 - binary_accuracy: 0.9779 - val_loss: 0.1237 - val_binary_accuracy: 0.9732\n",
            "Epoch 11/25\n",
            "458/458 [==============================] - 30s 66ms/step - loss: 0.0733 - binary_accuracy: 0.9805 - val_loss: 0.1282 - val_binary_accuracy: 0.9721\n",
            "Learning rate 0.0001, Batch size 16\n",
            "[0.5648147463798523, 0.38217395544052124, 0.26214301586151123, 0.1820926070213318, 0.14263774454593658, 0.1197117418050766, 0.11344681680202484, 0.09642437845468521, 0.09034574776887894, 0.07777376472949982, 0.07325265556573868]\n",
            "[0.4401160478591919, 0.2760973870754242, 0.17597448825836182, 0.13083645701408386, 0.12467242777347565, 0.1175621896982193, 0.117706298828125, 0.11312546581029892, 0.1354401707649231, 0.12373068183660507, 0.1281871795654297]\n",
            "[0.739919126033783, 0.818057656288147, 0.8940035700798035, 0.9286982417106628, 0.9526020884513855, 0.9631198048591614, 0.9663980603218079, 0.971451997756958, 0.9767791032791138, 0.9778718948364258, 0.9804671406745911]\n",
            "[0.7404371500015259, 0.8852459192276001, 0.9306011199951172, 0.956830620765686, 0.9633879661560059, 0.9693989157676697, 0.9699453711509705, 0.9721311330795288, 0.971584677696228, 0.9732240438461304, 0.9721311330795288]\n",
            "Epoch 1/25\n",
            "229/229 [==============================] - 40s 137ms/step - loss: 0.6121 - binary_accuracy: 0.7310 - val_loss: 0.5386 - val_binary_accuracy: 0.6732\n",
            "Epoch 2/25\n",
            "229/229 [==============================] - 31s 134ms/step - loss: 0.4933 - binary_accuracy: 0.7186 - val_loss: 0.4050 - val_binary_accuracy: 0.7896\n",
            "Epoch 3/25\n",
            "229/229 [==============================] - 30s 133ms/step - loss: 0.3841 - binary_accuracy: 0.8228 - val_loss: 0.3068 - val_binary_accuracy: 0.8667\n",
            "Epoch 4/25\n",
            "229/229 [==============================] - 30s 133ms/step - loss: 0.3027 - binary_accuracy: 0.8735 - val_loss: 0.2383 - val_binary_accuracy: 0.9066\n",
            "Epoch 5/25\n",
            "229/229 [==============================] - 30s 133ms/step - loss: 0.2381 - binary_accuracy: 0.9032 - val_loss: 0.1794 - val_binary_accuracy: 0.9268\n",
            "Epoch 6/25\n",
            "229/229 [==============================] - 30s 132ms/step - loss: 0.1920 - binary_accuracy: 0.9262 - val_loss: 0.1477 - val_binary_accuracy: 0.9443\n",
            "Epoch 7/25\n",
            "229/229 [==============================] - 30s 133ms/step - loss: 0.1544 - binary_accuracy: 0.9445 - val_loss: 0.1319 - val_binary_accuracy: 0.9530\n",
            "Epoch 8/25\n",
            "229/229 [==============================] - 30s 133ms/step - loss: 0.1308 - binary_accuracy: 0.9544 - val_loss: 0.1122 - val_binary_accuracy: 0.9628\n",
            "Epoch 9/25\n",
            "229/229 [==============================] - 30s 132ms/step - loss: 0.1185 - binary_accuracy: 0.9601 - val_loss: 0.1057 - val_binary_accuracy: 0.9667\n",
            "Epoch 10/25\n",
            "229/229 [==============================] - 30s 133ms/step - loss: 0.1022 - binary_accuracy: 0.9660 - val_loss: 0.1015 - val_binary_accuracy: 0.9678\n",
            "Epoch 11/25\n",
            "229/229 [==============================] - 30s 133ms/step - loss: 0.1006 - binary_accuracy: 0.9668 - val_loss: 0.1028 - val_binary_accuracy: 0.9689\n",
            "Epoch 12/25\n",
            "229/229 [==============================] - 31s 134ms/step - loss: 0.0835 - binary_accuracy: 0.9724 - val_loss: 0.0947 - val_binary_accuracy: 0.9721\n",
            "Epoch 13/25\n",
            "229/229 [==============================] - 31s 133ms/step - loss: 0.0772 - binary_accuracy: 0.9740 - val_loss: 0.0951 - val_binary_accuracy: 0.9743\n",
            "Epoch 14/25\n",
            "229/229 [==============================] - 31s 134ms/step - loss: 0.0730 - binary_accuracy: 0.9773 - val_loss: 0.0979 - val_binary_accuracy: 0.9716\n",
            "Epoch 15/25\n",
            "229/229 [==============================] - 31s 134ms/step - loss: 0.0639 - binary_accuracy: 0.9794 - val_loss: 0.1071 - val_binary_accuracy: 0.9749\n",
            "Learning rate 0.0001, Batch size 32\n",
            "[0.6120948791503906, 0.49333471059799194, 0.38410213589668274, 0.3027369976043701, 0.2380504161119461, 0.19203098118305206, 0.15442512929439545, 0.1307673454284668, 0.11846503615379333, 0.10224588960409164, 0.10060491412878036, 0.08347237855195999, 0.07721743732690811, 0.07304764539003372, 0.06388101726770401]\n",
            "[0.5385699272155762, 0.4049517512321472, 0.3067774176597595, 0.23832230269908905, 0.1793951690196991, 0.14772412180900574, 0.13193054497241974, 0.11216680705547333, 0.10568582266569138, 0.10145143419504166, 0.10280219465494156, 0.09470861405134201, 0.09508688747882843, 0.09790856391191483, 0.10711977630853653]\n",
            "[0.7309583425521851, 0.7186176776885986, 0.8228384256362915, 0.8735145330429077, 0.9031553268432617, 0.9262396097183228, 0.9445431232452393, 0.9543778300285339, 0.9601147174835205, 0.965988278388977, 0.9668078422546387, 0.9724081158638, 0.9740472435951233, 0.9773254990577698, 0.9793744087219238]\n",
            "[0.6732240319252014, 0.7896174788475037, 0.8666666746139526, 0.9065573811531067, 0.9267759323120117, 0.9442622661590576, 0.9530054926872253, 0.9628415107727051, 0.9666666388511658, 0.9677595496177673, 0.9688524603843689, 0.9721311330795288, 0.9743169546127319, 0.971584677696228, 0.9748634099960327]\n",
            "Epoch 1/25\n",
            "115/115 [==============================] - 48s 327ms/step - loss: 0.6354 - binary_accuracy: 0.7313 - val_loss: 0.6116 - val_binary_accuracy: 0.6710\n",
            "Epoch 2/25\n",
            "115/115 [==============================] - 38s 333ms/step - loss: 0.5721 - binary_accuracy: 0.6722 - val_loss: 0.5199 - val_binary_accuracy: 0.6754\n",
            "Epoch 3/25\n",
            "115/115 [==============================] - 38s 329ms/step - loss: 0.5024 - binary_accuracy: 0.7082 - val_loss: 0.4388 - val_binary_accuracy: 0.7590\n",
            "Epoch 4/25\n",
            "115/115 [==============================] - 38s 327ms/step - loss: 0.4353 - binary_accuracy: 0.7774 - val_loss: 0.3685 - val_binary_accuracy: 0.8301\n",
            "Epoch 5/25\n",
            "115/115 [==============================] - 42s 365ms/step - loss: 0.3747 - binary_accuracy: 0.8283 - val_loss: 0.3115 - val_binary_accuracy: 0.8727\n",
            "Epoch 6/25\n",
            "115/115 [==============================] - 37s 321ms/step - loss: 0.3246 - binary_accuracy: 0.8623 - val_loss: 0.2697 - val_binary_accuracy: 0.8885\n",
            "Epoch 7/25\n",
            "115/115 [==============================] - 39s 337ms/step - loss: 0.2846 - binary_accuracy: 0.8812 - val_loss: 0.2301 - val_binary_accuracy: 0.9120\n",
            "Epoch 8/25\n",
            "115/115 [==============================] - 37s 325ms/step - loss: 0.2443 - binary_accuracy: 0.9036 - val_loss: 0.1969 - val_binary_accuracy: 0.9186\n",
            "Epoch 9/25\n",
            "115/115 [==============================] - 37s 320ms/step - loss: 0.2136 - binary_accuracy: 0.9170 - val_loss: 0.1655 - val_binary_accuracy: 0.9350\n",
            "Epoch 10/25\n",
            "115/115 [==============================] - 39s 339ms/step - loss: 0.1846 - binary_accuracy: 0.9277 - val_loss: 0.1444 - val_binary_accuracy: 0.9481\n",
            "Epoch 11/25\n",
            "115/115 [==============================] - 37s 322ms/step - loss: 0.1630 - binary_accuracy: 0.9411 - val_loss: 0.1348 - val_binary_accuracy: 0.9514\n",
            "Epoch 12/25\n",
            "115/115 [==============================] - 39s 339ms/step - loss: 0.1443 - binary_accuracy: 0.9489 - val_loss: 0.1185 - val_binary_accuracy: 0.9585\n",
            "Epoch 13/25\n",
            "115/115 [==============================] - 38s 332ms/step - loss: 0.1293 - binary_accuracy: 0.9560 - val_loss: 0.1109 - val_binary_accuracy: 0.9623\n",
            "Epoch 14/25\n",
            "115/115 [==============================] - 37s 322ms/step - loss: 0.1174 - binary_accuracy: 0.9594 - val_loss: 0.1036 - val_binary_accuracy: 0.9645\n",
            "Epoch 15/25\n",
            "115/115 [==============================] - 39s 338ms/step - loss: 0.1105 - binary_accuracy: 0.9601 - val_loss: 0.0970 - val_binary_accuracy: 0.9672\n",
            "Epoch 16/25\n",
            "115/115 [==============================] - 38s 334ms/step - loss: 0.1006 - binary_accuracy: 0.9656 - val_loss: 0.0986 - val_binary_accuracy: 0.9672\n",
            "Epoch 17/25\n",
            "115/115 [==============================] - 37s 324ms/step - loss: 0.0950 - binary_accuracy: 0.9676 - val_loss: 0.0929 - val_binary_accuracy: 0.9694\n",
            "Epoch 18/25\n",
            "115/115 [==============================] - 39s 337ms/step - loss: 0.0893 - binary_accuracy: 0.9709 - val_loss: 0.0934 - val_binary_accuracy: 0.9705\n",
            "Epoch 19/25\n",
            "115/115 [==============================] - 42s 364ms/step - loss: 0.0801 - binary_accuracy: 0.9734 - val_loss: 0.0968 - val_binary_accuracy: 0.9694\n",
            "Epoch 20/25\n",
            "115/115 [==============================] - 37s 323ms/step - loss: 0.0810 - binary_accuracy: 0.9720 - val_loss: 0.0942 - val_binary_accuracy: 0.9721\n",
            "Learning rate 0.0001, Batch size 64\n",
            "[0.6354295611381531, 0.5721250176429749, 0.5023615956306458, 0.43526583909988403, 0.3747320771217346, 0.3246399760246277, 0.28455138206481934, 0.2442977875471115, 0.21358859539031982, 0.18456622958183289, 0.16296795010566711, 0.14425024390220642, 0.12932555377483368, 0.11743281781673431, 0.11054110527038574, 0.10064305365085602, 0.09495982527732849, 0.08925005793571472, 0.08010758459568024, 0.08097274601459503]\n",
            "[0.6115507483482361, 0.5199107527732849, 0.43877550959587097, 0.36854150891304016, 0.3114568293094635, 0.26973310112953186, 0.2300652116537094, 0.1969357579946518, 0.16545630991458893, 0.1444128006696701, 0.13483114540576935, 0.1185002401471138, 0.1109413132071495, 0.10362442582845688, 0.09701067954301834, 0.09863194823265076, 0.09291330724954605, 0.09343953430652618, 0.09678461402654648, 0.09415829926729202]\n",
            "[0.7312862277030945, 0.672175943851471, 0.708236575126648, 0.7773528099060059, 0.8283021450042725, 0.8623138666152954, 0.8811637759208679, 0.9035651087760925, 0.9169512391090393, 0.9277421236038208, 0.9411282539367676, 0.948914110660553, 0.9560169577598572, 0.9594317674636841, 0.9601147174835205, 0.9655784964561462, 0.9676273465156555, 0.970905601978302, 0.9733642935752869, 0.9719983339309692]\n",
            "[0.6710382699966431, 0.6754098534584045, 0.7590163946151733, 0.8300546407699585, 0.8726776242256165, 0.88852459192276, 0.9120218753814697, 0.9185792207717896, 0.9349727034568787, 0.9480874538421631, 0.951366126537323, 0.9584699273109436, 0.9622950553894043, 0.9644808769226074, 0.9672130942344666, 0.9672130942344666, 0.9693989157676697, 0.9704918265342712, 0.9693989157676697, 0.9721311330795288]\n",
            "Epoch 1/25\n",
            "58/58 [==============================] - 64s 939ms/step - loss: 0.6435 - binary_accuracy: 0.7307 - val_loss: 0.6411 - val_binary_accuracy: 0.6710\n",
            "Epoch 2/25\n",
            "33/58 [================>.............] - ETA: 18s - loss: 0.6271 - binary_accuracy: 0.6733"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-298cb0b5b147>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m                             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                             metrics=metrics)\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mhistory_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Learning rate {rate}, Batch size {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1387\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m     \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \"\"\"\n\u001b[1;32m   1222\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 3)\n",
        "\n",
        "epochs = 25\n",
        "\n",
        "learning_rate_list = [0.001, 0.0005, 0.0001]  \n",
        "batch_size_list = [16, 32, 64, 128, 256]\n",
        "\n",
        "for rate in learning_rate_list:\n",
        "  for size in batch_size_list:\n",
        "    tf.random.set_seed(12345)\n",
        "\n",
        "    batch_size = size\n",
        "    init_lr = rate\n",
        "\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    # training and validation dataset\n",
        "    train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "\n",
        "    train_val_data.shuffle(buffer_size=len(list(train_val_data)))\n",
        "\n",
        "    train_val_set_size = len(list(train_val_data))\n",
        "    val_n = int(0.2*train_val_set_size)\n",
        "\n",
        "    train_data = train_val_data.skip(val_n).batch(batch_size)\n",
        "    train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "    val_data = train_val_data.take(val_n).batch(batch_size)\n",
        "    val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    steps_per_epoch = train_val_set_size - val_n\n",
        "    num_train_steps = steps_per_epoch * epochs\n",
        "    num_warmup_steps = int(0.1*num_train_steps)\n",
        "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                             num_train_steps=num_train_steps,\n",
        "                                             num_warmup_steps=num_warmup_steps,\n",
        "                                             optimizer_type='adamw')\n",
        "\n",
        "    classifier = build_classifier_model(0)\n",
        "    classifier.compile(optimizer=optimizer,\n",
        "                            loss=loss,\n",
        "                            metrics=metrics)\n",
        "    history = classifier.fit(x=train_data, validation_data=val_data, epochs=epochs, callbacks=[es])\n",
        "    history_dict=history.history\n",
        "    print(f\"Learning rate {rate}, Batch size {size}\")\n",
        "    print(history_dict['loss'])\n",
        "    print(history_dict['val_loss'])\n",
        "    print(history_dict['binary_accuracy'])\n",
        "    print(history_dict['val_binary_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI-AT5A9c7P3",
        "outputId": "a602949a-d98e-4f28-fd99-13f7df8ede94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "29/29 [==============================] - 72s 2s/step - loss: 0.6392 - binary_accuracy: 0.6704 - val_loss: 0.6223 - val_binary_accuracy: 0.6710\n",
            "Epoch 2/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.5905 - binary_accuracy: 0.6708 - val_loss: 0.5456 - val_binary_accuracy: 0.6727\n",
            "Epoch 3/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.5225 - binary_accuracy: 0.6920 - val_loss: 0.4615 - val_binary_accuracy: 0.7306\n",
            "Epoch 4/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.4547 - binary_accuracy: 0.7566 - val_loss: 0.3871 - val_binary_accuracy: 0.8131\n",
            "Epoch 5/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.3905 - binary_accuracy: 0.8166 - val_loss: 0.3194 - val_binary_accuracy: 0.8678\n",
            "Epoch 6/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.3280 - binary_accuracy: 0.8605 - val_loss: 0.2659 - val_binary_accuracy: 0.8929\n",
            "Epoch 7/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2840 - binary_accuracy: 0.8836 - val_loss: 0.2240 - val_binary_accuracy: 0.9126\n",
            "Epoch 8/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2391 - binary_accuracy: 0.9063 - val_loss: 0.1880 - val_binary_accuracy: 0.9202\n",
            "Epoch 9/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2019 - binary_accuracy: 0.9232 - val_loss: 0.1560 - val_binary_accuracy: 0.9415\n",
            "Epoch 10/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1723 - binary_accuracy: 0.9366 - val_loss: 0.1342 - val_binary_accuracy: 0.9536\n",
            "Epoch 11/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1527 - binary_accuracy: 0.9451 - val_loss: 0.1194 - val_binary_accuracy: 0.9590\n",
            "Epoch 12/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1346 - binary_accuracy: 0.9522 - val_loss: 0.1090 - val_binary_accuracy: 0.9623\n",
            "Epoch 13/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1183 - binary_accuracy: 0.9607 - val_loss: 0.1059 - val_binary_accuracy: 0.9639\n",
            "Epoch 14/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1059 - binary_accuracy: 0.9630 - val_loss: 0.0940 - val_binary_accuracy: 0.9694\n",
            "Epoch 15/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1035 - binary_accuracy: 0.9641 - val_loss: 0.0906 - val_binary_accuracy: 0.9694\n",
            "Epoch 16/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0954 - binary_accuracy: 0.9668 - val_loss: 0.0902 - val_binary_accuracy: 0.9683\n",
            "Epoch 17/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0813 - binary_accuracy: 0.9710 - val_loss: 0.0886 - val_binary_accuracy: 0.9699\n",
            "Epoch 18/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0785 - binary_accuracy: 0.9734 - val_loss: 0.0894 - val_binary_accuracy: 0.9710\n",
            "Epoch 19/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0718 - binary_accuracy: 0.9753 - val_loss: 0.0877 - val_binary_accuracy: 0.9721\n",
            "Epoch 20/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0647 - binary_accuracy: 0.9784 - val_loss: 0.0869 - val_binary_accuracy: 0.9716\n",
            "Epoch 21/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0584 - binary_accuracy: 0.9812 - val_loss: 0.0895 - val_binary_accuracy: 0.9732\n",
            "Epoch 22/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0529 - binary_accuracy: 0.9821 - val_loss: 0.0860 - val_binary_accuracy: 0.9710\n",
            "Epoch 23/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0514 - binary_accuracy: 0.9820 - val_loss: 0.0867 - val_binary_accuracy: 0.9716\n",
            "Epoch 24/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0440 - binary_accuracy: 0.9862 - val_loss: 0.0816 - val_binary_accuracy: 0.9716\n",
            "Epoch 25/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0419 - binary_accuracy: 0.9868 - val_loss: 0.0908 - val_binary_accuracy: 0.9721\n",
            "Epoch 26/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0385 - binary_accuracy: 0.9858 - val_loss: 0.0866 - val_binary_accuracy: 0.9721\n",
            "Epoch 27/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0337 - binary_accuracy: 0.9898 - val_loss: 0.0978 - val_binary_accuracy: 0.9727\n",
            "Learning rate 0.001, Batch size 256\n",
            "[0.6392426490783691, 0.5905018448829651, 0.5224871039390564, 0.45469948649406433, 0.39050403237342834, 0.32804226875305176, 0.2839820086956024, 0.23908592760562897, 0.2019452154636383, 0.17232951521873474, 0.15274661779403687, 0.13462351262569427, 0.1183377206325531, 0.10594967007637024, 0.10349438339471817, 0.09538967162370682, 0.0813216045498848, 0.0784899890422821, 0.07182621210813522, 0.06471526622772217, 0.058393754065036774, 0.052916716784238815, 0.05143851414322853, 0.04401329159736633, 0.041933733969926834, 0.038469310849905014, 0.03374846279621124]\n",
            "[0.6223092079162598, 0.5455889701843262, 0.46153244376182556, 0.3870793282985687, 0.31944748759269714, 0.26589342951774597, 0.22403322160243988, 0.18799172341823578, 0.15603229403495789, 0.134229376912117, 0.11937727779150009, 0.1089676097035408, 0.10591961443424225, 0.09404630959033966, 0.09061269462108612, 0.09020306169986725, 0.08859575539827347, 0.08939731866121292, 0.0876939669251442, 0.0869174525141716, 0.08945003896951675, 0.08595021069049835, 0.0866633877158165, 0.08164844661951065, 0.09083115309476852, 0.08662278950214386, 0.09777622669935226]\n",
            "[0.6704002022743225, 0.6708099842071533, 0.6919819712638855, 0.7565906047821045, 0.8165551424026489, 0.8605381846427917, 0.8836224675178528, 0.906296968460083, 0.9232345223426819, 0.9366206526756287, 0.9450894594192505, 0.9521923065185547, 0.9606611132621765, 0.9629831910133362, 0.9640759229660034, 0.9668078422546387, 0.9710422158241272, 0.9733642935752869, 0.9752765893936157, 0.978418231010437, 0.9811500906944275, 0.9821062684059143, 0.9819696545600891, 0.9862040877342224, 0.9867504239082336, 0.9857943058013916, 0.9897555112838745]\n",
            "[0.6710382699966431, 0.6726775765419006, 0.7306010723114014, 0.8131147623062134, 0.8677595853805542, 0.8928961753845215, 0.9125683307647705, 0.9202185869216919, 0.9415300488471985, 0.9535518884658813, 0.9590163826942444, 0.9622950553894043, 0.9639344215393066, 0.9693989157676697, 0.9693989157676697, 0.9683060050010681, 0.9699453711509705, 0.9710382223129272, 0.9721311330795288, 0.971584677696228, 0.9732240438461304, 0.9710382223129272, 0.971584677696228, 0.971584677696228, 0.9721311330795288, 0.9721311330795288, 0.9726775884628296]\n"
          ]
        }
      ],
      "source": [
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 3)\n",
        "\n",
        "epochs = 25\n",
        "\n",
        "# need to run 0.001, 256 and 0.0005, 256 and 0.0001, 128 and 0.0001, 256\n",
        "\n",
        "learning_rate_list = [0.001]  \n",
        "batch_size_list = [256]\n",
        "\n",
        "for rate in learning_rate_list:\n",
        "  for size in batch_size_list:\n",
        "    tf.random.set_seed(12345)\n",
        "\n",
        "    batch_size = size\n",
        "    init_lr = rate\n",
        "\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    # training and validation dataset\n",
        "    train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "\n",
        "    train_val_data.shuffle(buffer_size=len(list(train_val_data)))\n",
        "\n",
        "    train_val_set_size = len(list(train_val_data))\n",
        "    val_n = int(0.2*train_val_set_size)\n",
        "\n",
        "    train_data = train_val_data.skip(val_n).batch(batch_size)\n",
        "    train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "    val_data = train_val_data.take(val_n).batch(batch_size)\n",
        "    val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    steps_per_epoch = train_val_set_size - val_n\n",
        "    num_train_steps = steps_per_epoch * epochs\n",
        "    num_warmup_steps = int(0.1*num_train_steps)\n",
        "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                             num_train_steps=num_train_steps,\n",
        "                                             num_warmup_steps=num_warmup_steps,\n",
        "                                             optimizer_type='adamw')\n",
        "\n",
        "    classifier = build_classifier_model(0)\n",
        "    classifier.compile(optimizer=optimizer,\n",
        "                            loss=loss,\n",
        "                            metrics=metrics)\n",
        "    history = classifier.fit(x=train_data, validation_data=val_data, epochs=100, callbacks=[es])\n",
        "    history_dict=history.history\n",
        "    print(f\"Learning rate {rate}, Batch size {size}\")\n",
        "    print(history_dict['loss'])\n",
        "    print(history_dict['val_loss'])\n",
        "    print(history_dict['binary_accuracy'])\n",
        "    print(history_dict['val_binary_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbuGx4ZDVWlR",
        "outputId": "1aad4567-ebbd-4683-d82d-f9e2ebc8d35f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "29/29 [==============================] - 67s 2s/step - loss: 0.6422 - binary_accuracy: 0.7311 - val_loss: 0.6375 - val_binary_accuracy: 0.6710\n",
            "Epoch 2/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.6193 - binary_accuracy: 0.6704 - val_loss: 0.5927 - val_binary_accuracy: 0.6710\n",
            "Epoch 3/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.5722 - binary_accuracy: 0.6722 - val_loss: 0.5307 - val_binary_accuracy: 0.6738\n",
            "Epoch 4/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.5235 - binary_accuracy: 0.6906 - val_loss: 0.4705 - val_binary_accuracy: 0.7169\n",
            "Epoch 5/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.4711 - binary_accuracy: 0.7345 - val_loss: 0.4150 - val_binary_accuracy: 0.7814\n",
            "Epoch 6/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.4215 - binary_accuracy: 0.7907 - val_loss: 0.3635 - val_binary_accuracy: 0.8301\n",
            "Epoch 7/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.3757 - binary_accuracy: 0.8279 - val_loss: 0.3168 - val_binary_accuracy: 0.8710\n",
            "Epoch 8/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.3344 - binary_accuracy: 0.8558 - val_loss: 0.2780 - val_binary_accuracy: 0.8945\n",
            "Epoch 9/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2971 - binary_accuracy: 0.8784 - val_loss: 0.2464 - val_binary_accuracy: 0.9049\n",
            "Epoch 10/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2644 - binary_accuracy: 0.8959 - val_loss: 0.2170 - val_binary_accuracy: 0.9175\n",
            "Epoch 11/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2389 - binary_accuracy: 0.9077 - val_loss: 0.1890 - val_binary_accuracy: 0.9240\n",
            "Epoch 12/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2096 - binary_accuracy: 0.9178 - val_loss: 0.1655 - val_binary_accuracy: 0.9366\n",
            "Epoch 13/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1869 - binary_accuracy: 0.9316 - val_loss: 0.1454 - val_binary_accuracy: 0.9475\n",
            "Epoch 14/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1605 - binary_accuracy: 0.9406 - val_loss: 0.1308 - val_binary_accuracy: 0.9563\n",
            "Epoch 15/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1506 - binary_accuracy: 0.9459 - val_loss: 0.1193 - val_binary_accuracy: 0.9579\n",
            "Epoch 16/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1380 - binary_accuracy: 0.9525 - val_loss: 0.1099 - val_binary_accuracy: 0.9623\n",
            "Epoch 17/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1253 - binary_accuracy: 0.9557 - val_loss: 0.1030 - val_binary_accuracy: 0.9650\n",
            "Epoch 18/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1198 - binary_accuracy: 0.9590 - val_loss: 0.0997 - val_binary_accuracy: 0.9656\n",
            "Epoch 19/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1096 - binary_accuracy: 0.9634 - val_loss: 0.0951 - val_binary_accuracy: 0.9661\n",
            "Epoch 20/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0999 - binary_accuracy: 0.9656 - val_loss: 0.0922 - val_binary_accuracy: 0.9683\n",
            "Epoch 21/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0963 - binary_accuracy: 0.9686 - val_loss: 0.0901 - val_binary_accuracy: 0.9689\n",
            "Epoch 22/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0875 - binary_accuracy: 0.9715 - val_loss: 0.0908 - val_binary_accuracy: 0.9694\n",
            "Epoch 23/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0863 - binary_accuracy: 0.9697 - val_loss: 0.0882 - val_binary_accuracy: 0.9705\n",
            "Epoch 24/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0803 - binary_accuracy: 0.9728 - val_loss: 0.0876 - val_binary_accuracy: 0.9710\n",
            "Epoch 25/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0758 - binary_accuracy: 0.9762 - val_loss: 0.0882 - val_binary_accuracy: 0.9727\n",
            "Epoch 26/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0701 - binary_accuracy: 0.9766 - val_loss: 0.0875 - val_binary_accuracy: 0.9727\n",
            "Epoch 27/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0690 - binary_accuracy: 0.9766 - val_loss: 0.0864 - val_binary_accuracy: 0.9727\n",
            "Epoch 28/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0672 - binary_accuracy: 0.9764 - val_loss: 0.0876 - val_binary_accuracy: 0.9705\n",
            "Epoch 29/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0647 - binary_accuracy: 0.9780 - val_loss: 0.0853 - val_binary_accuracy: 0.9727\n",
            "Epoch 30/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0544 - binary_accuracy: 0.9829 - val_loss: 0.0874 - val_binary_accuracy: 0.9727\n",
            "Epoch 31/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0552 - binary_accuracy: 0.9809 - val_loss: 0.0861 - val_binary_accuracy: 0.9727\n",
            "Epoch 32/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0490 - binary_accuracy: 0.9837 - val_loss: 0.0870 - val_binary_accuracy: 0.9727\n",
            "Learning rate 0.0005, Batch size 256\n",
            "[0.6421637535095215, 0.6193342208862305, 0.5721660256385803, 0.5234590172767639, 0.4711121916770935, 0.42148667573928833, 0.3757307529449463, 0.33441174030303955, 0.29713499546051025, 0.2644040882587433, 0.2388688176870346, 0.20962651073932648, 0.18692968785762787, 0.16046525537967682, 0.15056848526000977, 0.13801760971546173, 0.12529486417770386, 0.11977539211511612, 0.1095648780465126, 0.09992499649524689, 0.09627724438905716, 0.0875362977385521, 0.08629712462425232, 0.08033651858568192, 0.07575906813144684, 0.07005491852760315, 0.06901504844427109, 0.06717851758003235, 0.06468180567026138, 0.054413143545389175, 0.05520970746874809, 0.0489775687456131]\n",
            "[0.6375153660774231, 0.5927189588546753, 0.5307213068008423, 0.47048187255859375, 0.4150278866291046, 0.36345818638801575, 0.3168245851993561, 0.2779931128025055, 0.24641361832618713, 0.21700817346572876, 0.18904149532318115, 0.16549094021320343, 0.14538978040218353, 0.13077890872955322, 0.11931681632995605, 0.10987082868814468, 0.10298848152160645, 0.09972722828388214, 0.0951172262430191, 0.09222867339849472, 0.0901365578174591, 0.09076622128486633, 0.08823754638433456, 0.087599016726017, 0.0881909728050232, 0.08752968162298203, 0.08639395982027054, 0.0875803604722023, 0.08530395478010178, 0.08735845237970352, 0.08612282574176788, 0.08699731528759003]\n",
            "[0.7310676574707031, 0.6704002022743225, 0.672175943851471, 0.6906160116195679, 0.7344624996185303, 0.7907389998435974, 0.8278923630714417, 0.8557574152946472, 0.8784319162368774, 0.8959158658981323, 0.9076628684997559, 0.9177708029747009, 0.9315667152404785, 0.9405818581581116, 0.9459090232849121, 0.9524655342102051, 0.9557437300682068, 0.9590219855308533, 0.963392972946167, 0.9655784964561462, 0.9685835242271423, 0.971451997756958, 0.9696762561798096, 0.9728178977966309, 0.9762327671051025, 0.9766425490379333, 0.9766425490379333, 0.976369321346283, 0.9780084490776062, 0.9829258322715759, 0.9808769226074219, 0.9837453961372375]\n",
            "[0.6710382699966431, 0.6710382699966431, 0.6737704873085022, 0.7169398665428162, 0.7814207673072815, 0.8300546407699585, 0.8710382580757141, 0.8945355415344238, 0.9049180150032043, 0.917486310005188, 0.9240437150001526, 0.9366120100021362, 0.9475409984588623, 0.9562841653823853, 0.9579234719276428, 0.9622950553894043, 0.9650273323059082, 0.965573787689209, 0.9661202430725098, 0.9683060050010681, 0.9688524603843689, 0.9693989157676697, 0.9704918265342712, 0.9710382223129272, 0.9726775884628296, 0.9726775884628296, 0.9726775884628296, 0.9704918265342712, 0.9726775884628296, 0.9726775884628296, 0.9726775884628296, 0.9726775884628296]\n"
          ]
        }
      ],
      "source": [
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 3)\n",
        "\n",
        "epochs = 25\n",
        "\n",
        "# need to run 0.001, 256 and 0.0005, 256 and 0.0001, 128 and 0.0001, 256\n",
        "\n",
        "learning_rate_list = [0.0005]  \n",
        "batch_size_list = [256]\n",
        "\n",
        "for rate in learning_rate_list:\n",
        "  for size in batch_size_list:\n",
        "    tf.random.set_seed(12345)\n",
        "\n",
        "    batch_size = size\n",
        "    init_lr = rate\n",
        "\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    # training and validation dataset\n",
        "    train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "\n",
        "    train_val_data.shuffle(buffer_size=len(list(train_val_data)))\n",
        "\n",
        "    train_val_set_size = len(list(train_val_data))\n",
        "    val_n = int(0.2*train_val_set_size)\n",
        "\n",
        "    train_data = train_val_data.skip(val_n).batch(batch_size)\n",
        "    train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "    val_data = train_val_data.take(val_n).batch(batch_size)\n",
        "    val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    steps_per_epoch = train_val_set_size - val_n\n",
        "    num_train_steps = steps_per_epoch * epochs\n",
        "    num_warmup_steps = int(0.1*num_train_steps)\n",
        "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                             num_train_steps=num_train_steps,\n",
        "                                             num_warmup_steps=num_warmup_steps,\n",
        "                                             optimizer_type='adamw')\n",
        "\n",
        "    classifier = build_classifier_model(0)\n",
        "    classifier.compile(optimizer=optimizer,\n",
        "                            loss=loss,\n",
        "                            metrics=metrics)\n",
        "    history = classifier.fit(x=train_data, validation_data=val_data, epochs=100, callbacks=[es])\n",
        "    history_dict=history.history\n",
        "    print(f\"Learning rate {rate}, Batch size {size}\")\n",
        "    print(history_dict['loss'])\n",
        "    print(history_dict['val_loss'])\n",
        "    print(history_dict['binary_accuracy'])\n",
        "    print(history_dict['val_binary_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xGiRZBUVbCf",
        "outputId": "b139efd2-2a0e-49e9-aeb4-8506c647a9a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "58/58 [==============================] - 41s 570ms/step - loss: 0.6431 - binary_accuracy: 0.7308 - val_loss: 0.6411 - val_binary_accuracy: 0.6710\n",
            "Epoch 2/100\n",
            "58/58 [==============================] - 32s 559ms/step - loss: 0.6249 - binary_accuracy: 0.6704 - val_loss: 0.6066 - val_binary_accuracy: 0.6710\n",
            "Epoch 3/100\n",
            "58/58 [==============================] - 32s 557ms/step - loss: 0.5874 - binary_accuracy: 0.6708 - val_loss: 0.5561 - val_binary_accuracy: 0.6721\n",
            "Epoch 4/100\n",
            "58/58 [==============================] - 32s 557ms/step - loss: 0.5468 - binary_accuracy: 0.6753 - val_loss: 0.5028 - val_binary_accuracy: 0.6863\n",
            "Epoch 5/100\n",
            "58/58 [==============================] - 32s 554ms/step - loss: 0.5017 - binary_accuracy: 0.7076 - val_loss: 0.4556 - val_binary_accuracy: 0.7306\n",
            "Epoch 6/100\n",
            "58/58 [==============================] - 32s 560ms/step - loss: 0.4637 - binary_accuracy: 0.7424 - val_loss: 0.4134 - val_binary_accuracy: 0.7820\n",
            "Epoch 7/100\n",
            "58/58 [==============================] - 32s 556ms/step - loss: 0.4236 - binary_accuracy: 0.7881 - val_loss: 0.3733 - val_binary_accuracy: 0.8230\n",
            "Epoch 8/100\n",
            "58/58 [==============================] - 32s 557ms/step - loss: 0.3919 - binary_accuracy: 0.8146 - val_loss: 0.3370 - val_binary_accuracy: 0.8585\n",
            "Epoch 9/100\n",
            "58/58 [==============================] - 32s 559ms/step - loss: 0.3568 - binary_accuracy: 0.8425 - val_loss: 0.3052 - val_binary_accuracy: 0.8743\n",
            "Epoch 10/100\n",
            "58/58 [==============================] - 32s 557ms/step - loss: 0.3240 - binary_accuracy: 0.8637 - val_loss: 0.2770 - val_binary_accuracy: 0.8885\n",
            "Epoch 11/100\n",
            "58/58 [==============================] - 32s 560ms/step - loss: 0.3007 - binary_accuracy: 0.8752 - val_loss: 0.2523 - val_binary_accuracy: 0.9044\n",
            "Epoch 12/100\n",
            "58/58 [==============================] - 32s 559ms/step - loss: 0.2748 - binary_accuracy: 0.8928 - val_loss: 0.2311 - val_binary_accuracy: 0.9109\n",
            "Epoch 13/100\n",
            "58/58 [==============================] - 32s 557ms/step - loss: 0.2541 - binary_accuracy: 0.8965 - val_loss: 0.2100 - val_binary_accuracy: 0.9169\n",
            "Epoch 14/100\n",
            "58/58 [==============================] - 32s 557ms/step - loss: 0.2289 - binary_accuracy: 0.9127 - val_loss: 0.1891 - val_binary_accuracy: 0.9219\n",
            "Epoch 15/100\n",
            "58/58 [==============================] - 32s 562ms/step - loss: 0.2160 - binary_accuracy: 0.9165 - val_loss: 0.1698 - val_binary_accuracy: 0.9366\n",
            "Epoch 16/100\n",
            "58/58 [==============================] - 32s 557ms/step - loss: 0.1970 - binary_accuracy: 0.9269 - val_loss: 0.1546 - val_binary_accuracy: 0.9415\n",
            "Epoch 17/100\n",
            "58/58 [==============================] - 32s 558ms/step - loss: 0.1796 - binary_accuracy: 0.9355 - val_loss: 0.1418 - val_binary_accuracy: 0.9492\n",
            "Epoch 18/100\n",
            "58/58 [==============================] - 32s 560ms/step - loss: 0.1621 - binary_accuracy: 0.9406 - val_loss: 0.1305 - val_binary_accuracy: 0.9541\n",
            "Epoch 19/100\n",
            "58/58 [==============================] - 32s 554ms/step - loss: 0.1526 - binary_accuracy: 0.9452 - val_loss: 0.1228 - val_binary_accuracy: 0.9546\n",
            "Epoch 20/100\n",
            "58/58 [==============================] - 32s 554ms/step - loss: 0.1407 - binary_accuracy: 0.9504 - val_loss: 0.1159 - val_binary_accuracy: 0.9563\n",
            "Epoch 21/100\n",
            "58/58 [==============================] - 33s 562ms/step - loss: 0.1358 - binary_accuracy: 0.9512 - val_loss: 0.1093 - val_binary_accuracy: 0.9612\n",
            "Epoch 22/100\n",
            "58/58 [==============================] - 32s 559ms/step - loss: 0.1240 - binary_accuracy: 0.9566 - val_loss: 0.1030 - val_binary_accuracy: 0.9639\n",
            "Epoch 23/100\n",
            "58/58 [==============================] - 32s 561ms/step - loss: 0.1180 - binary_accuracy: 0.9597 - val_loss: 0.0997 - val_binary_accuracy: 0.9650\n",
            "Epoch 24/100\n",
            "58/58 [==============================] - 32s 557ms/step - loss: 0.1117 - binary_accuracy: 0.9635 - val_loss: 0.0986 - val_binary_accuracy: 0.9661\n",
            "Epoch 25/100\n",
            "58/58 [==============================] - 32s 559ms/step - loss: 0.1078 - binary_accuracy: 0.9631 - val_loss: 0.0941 - val_binary_accuracy: 0.9689\n",
            "Epoch 26/100\n",
            "58/58 [==============================] - 32s 562ms/step - loss: 0.1061 - binary_accuracy: 0.9643 - val_loss: 0.0927 - val_binary_accuracy: 0.9694\n",
            "Epoch 27/100\n",
            "58/58 [==============================] - 32s 559ms/step - loss: 0.0931 - binary_accuracy: 0.9698 - val_loss: 0.0933 - val_binary_accuracy: 0.9689\n",
            "Epoch 28/100\n",
            "58/58 [==============================] - 32s 558ms/step - loss: 0.0951 - binary_accuracy: 0.9682 - val_loss: 0.0905 - val_binary_accuracy: 0.9694\n",
            "Epoch 29/100\n",
            "58/58 [==============================] - 32s 559ms/step - loss: 0.0893 - binary_accuracy: 0.9702 - val_loss: 0.0884 - val_binary_accuracy: 0.9710\n",
            "Epoch 30/100\n",
            "58/58 [==============================] - 32s 560ms/step - loss: 0.0873 - binary_accuracy: 0.9695 - val_loss: 0.0883 - val_binary_accuracy: 0.9710\n",
            "Epoch 31/100\n",
            "58/58 [==============================] - 32s 558ms/step - loss: 0.0827 - binary_accuracy: 0.9717 - val_loss: 0.0871 - val_binary_accuracy: 0.9710\n",
            "Epoch 32/100\n",
            "58/58 [==============================] - 32s 560ms/step - loss: 0.0807 - binary_accuracy: 0.9725 - val_loss: 0.0887 - val_binary_accuracy: 0.9705\n",
            "Epoch 33/100\n",
            "58/58 [==============================] - 32s 556ms/step - loss: 0.0769 - binary_accuracy: 0.9735 - val_loss: 0.0890 - val_binary_accuracy: 0.9716\n",
            "Epoch 34/100\n",
            "58/58 [==============================] - 32s 561ms/step - loss: 0.0708 - binary_accuracy: 0.9753 - val_loss: 0.0861 - val_binary_accuracy: 0.9716\n",
            "Epoch 35/100\n",
            "58/58 [==============================] - 32s 560ms/step - loss: 0.0705 - binary_accuracy: 0.9745 - val_loss: 0.0850 - val_binary_accuracy: 0.9727\n",
            "Epoch 36/100\n",
            "58/58 [==============================] - 32s 559ms/step - loss: 0.0674 - binary_accuracy: 0.9775 - val_loss: 0.0865 - val_binary_accuracy: 0.9721\n",
            "Epoch 37/100\n",
            "58/58 [==============================] - 32s 558ms/step - loss: 0.0655 - binary_accuracy: 0.9771 - val_loss: 0.0861 - val_binary_accuracy: 0.9727\n",
            "Epoch 38/100\n",
            "58/58 [==============================] - 32s 555ms/step - loss: 0.0591 - binary_accuracy: 0.9802 - val_loss: 0.0874 - val_binary_accuracy: 0.9710\n",
            "Learning rate 0.0001, Batch size 128\n",
            "[0.6430658102035522, 0.6248631477355957, 0.5874297022819519, 0.5468236207962036, 0.5016889572143555, 0.4636771082878113, 0.4235953986644745, 0.3919374346733093, 0.35677969455718994, 0.32401078939437866, 0.3006715774536133, 0.27482548356056213, 0.25412940979003906, 0.22885799407958984, 0.21599550545215607, 0.19703440368175507, 0.1796230524778366, 0.1621135175228119, 0.1526031345129013, 0.1406579464673996, 0.13576820492744446, 0.12402009963989258, 0.11802675575017929, 0.11167453229427338, 0.10784107446670532, 0.10611116886138916, 0.09311269968748093, 0.09510116279125214, 0.08925491571426392, 0.08730114251375198, 0.08270378410816193, 0.08069328963756561, 0.07689233869314194, 0.070821113884449, 0.07048987597227097, 0.06738311052322388, 0.06549735367298126, 0.059065818786621094]\n",
            "[0.6410511136054993, 0.6065903306007385, 0.5561088919639587, 0.5027696490287781, 0.45564907789230347, 0.4133754074573517, 0.3732975125312805, 0.33698925375938416, 0.305245965719223, 0.2769679129123688, 0.2522929608821869, 0.2311447560787201, 0.21004123985767365, 0.18907596170902252, 0.16984926164150238, 0.1546369045972824, 0.14177322387695312, 0.13049161434173584, 0.12276902794837952, 0.11593732982873917, 0.10934007167816162, 0.1030186340212822, 0.09970644116401672, 0.09855924546718597, 0.09411272406578064, 0.09271438419818878, 0.09331222623586655, 0.0904870256781578, 0.08835403621196747, 0.08830948173999786, 0.08710718899965286, 0.08869031816720963, 0.08903037011623383, 0.08613719791173935, 0.08500148355960846, 0.08654376864433289, 0.08610530197620392, 0.08744897693395615]\n",
            "[0.7308490872383118, 0.6704002022743225, 0.6708099842071533, 0.6753175854682922, 0.7075536251068115, 0.7423849105834961, 0.7881436944007874, 0.8146427869796753, 0.8425078392028809, 0.863679826259613, 0.875153660774231, 0.892774224281311, 0.8964622616767883, 0.9127168655395508, 0.9165414571762085, 0.9269225597381592, 0.9355279207229614, 0.9405818581581116, 0.9452260732650757, 0.950416624546051, 0.9512361884117126, 0.9565632939338684, 0.9597049355506897, 0.9635295867919922, 0.9631198048591614, 0.9643491506576538, 0.9698128700256348, 0.9681737422943115, 0.9702226519584656, 0.9695397019386292, 0.9717251658439636, 0.9725447297096252, 0.9735009074211121, 0.9752765893936157, 0.9744570255279541, 0.977462112903595, 0.9770523309707642, 0.9801939725875854]\n",
            "[0.6710382699966431, 0.6710382699966431, 0.6721311211585999, 0.6863387823104858, 0.7306010723114014, 0.7819672226905823, 0.8229508399963379, 0.8584699630737305, 0.874316930770874, 0.88852459192276, 0.9043715596199036, 0.9109289646148682, 0.916939914226532, 0.9218579530715942, 0.9366120100021362, 0.9415300488471985, 0.9491803050041199, 0.9540983438491821, 0.9546447992324829, 0.9562841653823853, 0.9612022042274475, 0.9639344215393066, 0.9650273323059082, 0.9661202430725098, 0.9688524603843689, 0.9693989157676697, 0.9688524603843689, 0.9693989157676697, 0.9710382223129272, 0.9710382223129272, 0.9710382223129272, 0.9704918265342712, 0.971584677696228, 0.971584677696228, 0.9726775884628296, 0.9721311330795288, 0.9726775884628296, 0.9710382223129272]\n",
            "Epoch 1/100\n",
            "29/29 [==============================] - 67s 2s/step - loss: 0.6460 - binary_accuracy: 0.7305 - val_loss: 0.6501 - val_binary_accuracy: 0.6710\n",
            "Epoch 2/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.6395 - binary_accuracy: 0.6704 - val_loss: 0.6403 - val_binary_accuracy: 0.6710\n",
            "Epoch 3/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.6278 - binary_accuracy: 0.6704 - val_loss: 0.6246 - val_binary_accuracy: 0.6710\n",
            "Epoch 4/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.6143 - binary_accuracy: 0.6707 - val_loss: 0.6034 - val_binary_accuracy: 0.6710\n",
            "Epoch 5/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.5924 - binary_accuracy: 0.6709 - val_loss: 0.5777 - val_binary_accuracy: 0.6710\n",
            "Epoch 6/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.5738 - binary_accuracy: 0.6716 - val_loss: 0.5486 - val_binary_accuracy: 0.6721\n",
            "Epoch 7/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.5487 - binary_accuracy: 0.6759 - val_loss: 0.5181 - val_binary_accuracy: 0.6765\n",
            "Epoch 8/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.5254 - binary_accuracy: 0.6876 - val_loss: 0.4896 - val_binary_accuracy: 0.6962\n",
            "Epoch 9/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.4993 - binary_accuracy: 0.7037 - val_loss: 0.4622 - val_binary_accuracy: 0.7224\n",
            "Epoch 10/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.4762 - binary_accuracy: 0.7291 - val_loss: 0.4364 - val_binary_accuracy: 0.7563\n",
            "Epoch 11/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.4560 - binary_accuracy: 0.7522 - val_loss: 0.4118 - val_binary_accuracy: 0.7831\n",
            "Epoch 12/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.4322 - binary_accuracy: 0.7771 - val_loss: 0.3889 - val_binary_accuracy: 0.8093\n",
            "Epoch 13/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.4110 - binary_accuracy: 0.7995 - val_loss: 0.3672 - val_binary_accuracy: 0.8246\n",
            "Epoch 14/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.3920 - binary_accuracy: 0.8122 - val_loss: 0.3460 - val_binary_accuracy: 0.8426\n",
            "Epoch 15/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.3686 - binary_accuracy: 0.8327 - val_loss: 0.3259 - val_binary_accuracy: 0.8617\n",
            "Epoch 16/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.3497 - binary_accuracy: 0.8451 - val_loss: 0.3077 - val_binary_accuracy: 0.8738\n",
            "Epoch 17/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.3342 - binary_accuracy: 0.8578 - val_loss: 0.2897 - val_binary_accuracy: 0.8847\n",
            "Epoch 18/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.3171 - binary_accuracy: 0.8635 - val_loss: 0.2740 - val_binary_accuracy: 0.8907\n",
            "Epoch 19/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2977 - binary_accuracy: 0.8784 - val_loss: 0.2592 - val_binary_accuracy: 0.8995\n",
            "Epoch 20/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2870 - binary_accuracy: 0.8842 - val_loss: 0.2448 - val_binary_accuracy: 0.9077\n",
            "Epoch 21/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2689 - binary_accuracy: 0.8928 - val_loss: 0.2307 - val_binary_accuracy: 0.9120\n",
            "Epoch 22/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2594 - binary_accuracy: 0.8962 - val_loss: 0.2164 - val_binary_accuracy: 0.9169\n",
            "Epoch 23/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2432 - binary_accuracy: 0.9051 - val_loss: 0.2054 - val_binary_accuracy: 0.9202\n",
            "Epoch 24/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2338 - binary_accuracy: 0.9081 - val_loss: 0.1929 - val_binary_accuracy: 0.9230\n",
            "Epoch 25/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2215 - binary_accuracy: 0.9124 - val_loss: 0.1806 - val_binary_accuracy: 0.9306\n",
            "Epoch 26/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.2081 - binary_accuracy: 0.9205 - val_loss: 0.1685 - val_binary_accuracy: 0.9377\n",
            "Epoch 27/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1956 - binary_accuracy: 0.9264 - val_loss: 0.1582 - val_binary_accuracy: 0.9415\n",
            "Epoch 28/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1861 - binary_accuracy: 0.9332 - val_loss: 0.1485 - val_binary_accuracy: 0.9464\n",
            "Epoch 29/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1750 - binary_accuracy: 0.9350 - val_loss: 0.1413 - val_binary_accuracy: 0.9503\n",
            "Epoch 30/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1691 - binary_accuracy: 0.9402 - val_loss: 0.1359 - val_binary_accuracy: 0.9546\n",
            "Epoch 31/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1571 - binary_accuracy: 0.9432 - val_loss: 0.1292 - val_binary_accuracy: 0.9552\n",
            "Epoch 32/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1515 - binary_accuracy: 0.9459 - val_loss: 0.1238 - val_binary_accuracy: 0.9574\n",
            "Epoch 33/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1463 - binary_accuracy: 0.9499 - val_loss: 0.1186 - val_binary_accuracy: 0.9590\n",
            "Epoch 34/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1412 - binary_accuracy: 0.9500 - val_loss: 0.1145 - val_binary_accuracy: 0.9590\n",
            "Epoch 35/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1301 - binary_accuracy: 0.9549 - val_loss: 0.1117 - val_binary_accuracy: 0.9612\n",
            "Epoch 36/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1276 - binary_accuracy: 0.9560 - val_loss: 0.1082 - val_binary_accuracy: 0.9617\n",
            "Epoch 37/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1255 - binary_accuracy: 0.9560 - val_loss: 0.1052 - val_binary_accuracy: 0.9645\n",
            "Epoch 38/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1187 - binary_accuracy: 0.9605 - val_loss: 0.1025 - val_binary_accuracy: 0.9639\n",
            "Epoch 39/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1162 - binary_accuracy: 0.9609 - val_loss: 0.1012 - val_binary_accuracy: 0.9639\n",
            "Epoch 40/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1114 - binary_accuracy: 0.9638 - val_loss: 0.0992 - val_binary_accuracy: 0.9645\n",
            "Epoch 41/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1059 - binary_accuracy: 0.9638 - val_loss: 0.0976 - val_binary_accuracy: 0.9639\n",
            "Epoch 42/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.1065 - binary_accuracy: 0.9634 - val_loss: 0.0977 - val_binary_accuracy: 0.9661\n",
            "Epoch 43/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0994 - binary_accuracy: 0.9661 - val_loss: 0.0964 - val_binary_accuracy: 0.9661\n",
            "Epoch 44/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0989 - binary_accuracy: 0.9661 - val_loss: 0.0946 - val_binary_accuracy: 0.9678\n",
            "Epoch 45/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0985 - binary_accuracy: 0.9657 - val_loss: 0.0944 - val_binary_accuracy: 0.9661\n",
            "Epoch 46/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0942 - binary_accuracy: 0.9684 - val_loss: 0.0916 - val_binary_accuracy: 0.9694\n",
            "Epoch 47/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0908 - binary_accuracy: 0.9706 - val_loss: 0.0905 - val_binary_accuracy: 0.9689\n",
            "Epoch 48/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0911 - binary_accuracy: 0.9695 - val_loss: 0.0898 - val_binary_accuracy: 0.9683\n",
            "Epoch 49/100\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.0841 - binary_accuracy: 0.9708 - val_loss: 0.0920 - val_binary_accuracy: 0.9694\n",
            "Epoch 50/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0859 - binary_accuracy: 0.9710 - val_loss: 0.0893 - val_binary_accuracy: 0.9710\n",
            "Epoch 51/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0825 - binary_accuracy: 0.9736 - val_loss: 0.0883 - val_binary_accuracy: 0.9721\n",
            "Epoch 52/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0808 - binary_accuracy: 0.9723 - val_loss: 0.0899 - val_binary_accuracy: 0.9710\n",
            "Epoch 53/100\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0831 - binary_accuracy: 0.9720 - val_loss: 0.0908 - val_binary_accuracy: 0.9710\n",
            "Epoch 54/100\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.0766 - binary_accuracy: 0.9715 - val_loss: 0.0889 - val_binary_accuracy: 0.9716\n",
            "Learning rate 0.0001, Batch size 256\n",
            "[0.6460316181182861, 0.6394860744476318, 0.6278178095817566, 0.6142790913581848, 0.5924167633056641, 0.5737706422805786, 0.5486782789230347, 0.5254198312759399, 0.499301552772522, 0.4761556088924408, 0.4560072720050812, 0.4322430193424225, 0.4110127389431, 0.391952782869339, 0.36857664585113525, 0.3496558368206024, 0.33424192667007446, 0.3170560598373413, 0.29772984981536865, 0.2870233952999115, 0.2688528299331665, 0.25943028926849365, 0.24323174357414246, 0.23379799723625183, 0.22148314118385315, 0.20806796848773956, 0.19564040005207062, 0.18606039881706238, 0.17499135434627533, 0.16909359395503998, 0.15709266066551208, 0.15154428780078888, 0.146250918507576, 0.14120498299598694, 0.13011516630649567, 0.12755076587200165, 0.12554149329662323, 0.11865612864494324, 0.11621857434511185, 0.11144813895225525, 0.10592189431190491, 0.10648687183856964, 0.09938003867864609, 0.0989147424697876, 0.09852604568004608, 0.09415701776742935, 0.09079894423484802, 0.0910775288939476, 0.0840553492307663, 0.0858568325638771, 0.08247897028923035, 0.08080601692199707, 0.08309444040060043, 0.07663672417402267]\n",
            "[0.6500942707061768, 0.6403058171272278, 0.6246082782745361, 0.6034189462661743, 0.5777139067649841, 0.5485791563987732, 0.5180901288986206, 0.489647775888443, 0.46223676204681396, 0.43640705943107605, 0.41184931993484497, 0.3888966739177704, 0.3672073185443878, 0.3459969162940979, 0.32585281133651733, 0.3076866865158081, 0.28966185450553894, 0.27401432394981384, 0.2591845989227295, 0.24480263888835907, 0.23067456483840942, 0.21639229357242584, 0.20537467300891876, 0.19288288056850433, 0.18057763576507568, 0.16850969195365906, 0.15823300182819366, 0.1485167294740677, 0.14128708839416504, 0.13591060042381287, 0.12923376262187958, 0.12378823012113571, 0.11859643459320068, 0.11452093720436096, 0.11165778338909149, 0.10824533551931381, 0.10524652153253555, 0.10253340750932693, 0.10122676193714142, 0.09920235723257065, 0.09760629385709763, 0.09768279641866684, 0.09639786183834076, 0.0946025401353836, 0.0944337472319603, 0.09157468378543854, 0.09049595892429352, 0.08984820544719696, 0.09199228137731552, 0.08932177722454071, 0.08830150216817856, 0.08988916128873825, 0.09079191088676453, 0.0889192521572113]\n",
            "[0.7305212616920471, 0.6704002022743225, 0.6704002022743225, 0.6706734299659729, 0.6709465980529785, 0.6716295480728149, 0.6758639812469482, 0.6876109838485718, 0.703728973865509, 0.7291353344917297, 0.7522196173667908, 0.7770796418190002, 0.7994809746742249, 0.8121841549873352, 0.8326731324195862, 0.8451031446456909, 0.8578063249588013, 0.8635432124137878, 0.8784319162368774, 0.884168803691864, 0.892774224281311, 0.8961890339851379, 0.9050676226615906, 0.9080726504325867, 0.9124436378479004, 0.9205026626586914, 0.9263761639595032, 0.9332058429718018, 0.9349815845489502, 0.9401721358299255, 0.9431771636009216, 0.9459090232849121, 0.949870228767395, 0.9500068426132202, 0.9549241662025452, 0.9560169577598572, 0.9560169577598572, 0.9605244994163513, 0.9609342813491821, 0.9638027548789978, 0.9638027548789978, 0.963392972946167, 0.9661248326301575, 0.9661248326301575, 0.9657150506973267, 0.9684469103813171, 0.9706324338912964, 0.9695397019386292, 0.9707690477371216, 0.9710422158241272, 0.9736374616622925, 0.9722715616226196, 0.9719983339309692, 0.971451997756958]\n",
            "[0.6710382699966431, 0.6710382699966431, 0.6710382699966431, 0.6710382699966431, 0.6710382699966431, 0.6721311211585999, 0.6765027046203613, 0.6961748600006104, 0.7224043607711792, 0.7562841773033142, 0.7830601334571838, 0.8092896342277527, 0.8245901465415955, 0.8426229357719421, 0.8617486357688904, 0.8737704753875732, 0.8846994638442993, 0.8907103538513184, 0.8994535803794861, 0.9076502919197083, 0.9120218753814697, 0.916939914226532, 0.9202185869216919, 0.922950804233551, 0.9306011199951172, 0.9377049207687378, 0.9415300488471985, 0.9464480876922607, 0.9502732157707214, 0.9546447992324829, 0.9551912546157837, 0.9573770761489868, 0.9590163826942444, 0.9590163826942444, 0.9612022042274475, 0.9617486596107483, 0.9644808769226074, 0.9639344215393066, 0.9639344215393066, 0.9644808769226074, 0.9639344215393066, 0.9661202430725098, 0.9661202430725098, 0.9677595496177673, 0.9661202430725098, 0.9693989157676697, 0.9688524603843689, 0.9683060050010681, 0.9693989157676697, 0.9710382223129272, 0.9721311330795288, 0.9710382223129272, 0.9710382223129272, 0.971584677696228]\n"
          ]
        }
      ],
      "source": [
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 3)\n",
        "\n",
        "epochs = 25\n",
        "\n",
        "# need to run 0.001, 256 and 0.0005, 256 and 0.0001, 128 and 0.0001, 256\n",
        "\n",
        "learning_rate_list = [0.0001]  \n",
        "batch_size_list = [128, 256]\n",
        "\n",
        "for rate in learning_rate_list:\n",
        "  for size in batch_size_list:\n",
        "    tf.random.set_seed(12345)\n",
        "\n",
        "    batch_size = size\n",
        "    init_lr = rate\n",
        "\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    # training and validation dataset\n",
        "    train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "\n",
        "    train_val_data.shuffle(buffer_size=len(list(train_val_data)))\n",
        "\n",
        "    train_val_set_size = len(list(train_val_data))\n",
        "    val_n = int(0.2*train_val_set_size)\n",
        "\n",
        "    train_data = train_val_data.skip(val_n).batch(batch_size)\n",
        "    train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "    val_data = train_val_data.take(val_n).batch(batch_size)\n",
        "    val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    steps_per_epoch = train_val_set_size - val_n\n",
        "    num_train_steps = steps_per_epoch * epochs\n",
        "    num_warmup_steps = int(0.1*num_train_steps)\n",
        "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                             num_train_steps=num_train_steps,\n",
        "                                             num_warmup_steps=num_warmup_steps,\n",
        "                                             optimizer_type='adamw')\n",
        "\n",
        "    classifier = build_classifier_model(0)\n",
        "    classifier.compile(optimizer=optimizer,\n",
        "                            loss=loss,\n",
        "                            metrics=metrics)\n",
        "    history = classifier.fit(x=train_data, validation_data=val_data, epochs=100, callbacks=[es])\n",
        "    history_dict=history.history\n",
        "    print(f\"Learning rate {rate}, Batch size {size}\")\n",
        "    print(history_dict['loss'])\n",
        "    print(history_dict['val_loss'])\n",
        "    print(history_dict['binary_accuracy'])\n",
        "    print(history_dict['val_binary_accuracy'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QI9e08e-WRos"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEMDk5Hs0fI4"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(12345)\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "# training and validation dataset\n",
        "train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32], select_cols=[2,3])\n",
        "\n",
        "train_val_data.shuffle(buffer_size=len(list(train_val_data)), seed = 12345)\n",
        "\n",
        "train_val_set_size = len(list(train_val_data))\n",
        "\n",
        "split_size = int(0.2*train_val_set_size)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "cv_1 = train_val_data.take(split_size).batch(batch_size)\n",
        "cv_1 = cv_1.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "m1 = train_val_data.skip(split_size)\n",
        "cv_2 = m1.take(split_size).batch(batch_size)\n",
        "cv_2 = cv_2.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "m2 = m1.skip(split_size)\n",
        "cv_3 = m2.take(split_size).batch(batch_size)\n",
        "cv_3 = cv_3.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "m3 = m2.skip(split_size)\n",
        "cv_4 = m3.take(split_size).batch(batch_size)\n",
        "cv_4 = cv_4.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "cv_5 = m3.skip(split_size).batch(batch_size)\n",
        "cv_5 = cv_5.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxcHxsBU0sLE",
        "outputId": "422bd8a1-cab1-4d12-fb39-243742d65e8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "460/460 [==============================] - 42s 64ms/step - loss: 0.4385 - binary_accuracy: 0.7840 - val_loss: 0.1829 - val_binary_accuracy: 0.9355\n",
            "Epoch 2/7\n",
            "460/460 [==============================] - 29s 64ms/step - loss: 0.1678 - binary_accuracy: 0.9418 - val_loss: 0.1292 - val_binary_accuracy: 0.9661\n",
            "Epoch 3/7\n",
            "460/460 [==============================] - 29s 64ms/step - loss: 0.1236 - binary_accuracy: 0.9648 - val_loss: 0.1198 - val_binary_accuracy: 0.9738\n",
            "Epoch 4/7\n",
            "460/460 [==============================] - 30s 64ms/step - loss: 0.0957 - binary_accuracy: 0.9740 - val_loss: 0.1042 - val_binary_accuracy: 0.9738\n",
            "Epoch 5/7\n",
            "460/460 [==============================] - 29s 64ms/step - loss: 0.0836 - binary_accuracy: 0.9783 - val_loss: 0.1420 - val_binary_accuracy: 0.9743\n",
            "Epoch 6/7\n",
            "460/460 [==============================] - 29s 64ms/step - loss: 0.0646 - binary_accuracy: 0.9848 - val_loss: 0.1476 - val_binary_accuracy: 0.9721\n",
            "Epoch 7/7\n",
            "460/460 [==============================] - 29s 64ms/step - loss: 0.0527 - binary_accuracy: 0.9870 - val_loss: 0.1359 - val_binary_accuracy: 0.9760\n",
            "Epoch 1/7\n",
            "460/460 [==============================] - 37s 63ms/step - loss: 0.3952 - binary_accuracy: 0.8381 - val_loss: 0.1965 - val_binary_accuracy: 0.9284\n",
            "Epoch 2/7\n",
            "460/460 [==============================] - 30s 64ms/step - loss: 0.1564 - binary_accuracy: 0.9478 - val_loss: 0.1492 - val_binary_accuracy: 0.9623\n",
            "Epoch 3/7\n",
            "460/460 [==============================] - 29s 64ms/step - loss: 0.1131 - binary_accuracy: 0.9680 - val_loss: 0.1505 - val_binary_accuracy: 0.9667\n",
            "Epoch 4/7\n",
            "460/460 [==============================] - 30s 65ms/step - loss: 0.0872 - binary_accuracy: 0.9764 - val_loss: 0.2621 - val_binary_accuracy: 0.9525\n",
            "Epoch 5/7\n",
            "460/460 [==============================] - 30s 64ms/step - loss: 0.0769 - binary_accuracy: 0.9802 - val_loss: 0.1515 - val_binary_accuracy: 0.9705\n",
            "Epoch 6/7\n",
            "460/460 [==============================] - 30s 64ms/step - loss: 0.0606 - binary_accuracy: 0.9844 - val_loss: 0.1577 - val_binary_accuracy: 0.9716\n",
            "Epoch 7/7\n",
            "460/460 [==============================] - 30s 64ms/step - loss: 0.0468 - binary_accuracy: 0.9889 - val_loss: 0.1508 - val_binary_accuracy: 0.9727\n",
            "Epoch 1/7\n",
            "460/460 [==============================] - 38s 63ms/step - loss: 0.4046 - binary_accuracy: 0.8339 - val_loss: 0.2014 - val_binary_accuracy: 0.9202\n",
            "Epoch 2/7\n",
            "460/460 [==============================] - 30s 65ms/step - loss: 0.1722 - binary_accuracy: 0.9413 - val_loss: 0.1280 - val_binary_accuracy: 0.9650\n",
            "Epoch 3/7\n",
            "460/460 [==============================] - 30s 65ms/step - loss: 0.1180 - binary_accuracy: 0.9650 - val_loss: 0.1169 - val_binary_accuracy: 0.9727\n",
            "Epoch 4/7\n",
            "460/460 [==============================] - 30s 65ms/step - loss: 0.0941 - binary_accuracy: 0.9728 - val_loss: 0.1291 - val_binary_accuracy: 0.9732\n",
            "Epoch 5/7\n",
            "460/460 [==============================] - 30s 66ms/step - loss: 0.0751 - binary_accuracy: 0.9806 - val_loss: 0.1398 - val_binary_accuracy: 0.9727\n",
            "Epoch 6/7\n",
            "460/460 [==============================] - 30s 65ms/step - loss: 0.0624 - binary_accuracy: 0.9847 - val_loss: 0.1854 - val_binary_accuracy: 0.9699\n",
            "Epoch 7/7\n",
            "460/460 [==============================] - 30s 65ms/step - loss: 0.0571 - binary_accuracy: 0.9869 - val_loss: 0.1407 - val_binary_accuracy: 0.9760\n",
            "Epoch 1/7\n",
            "460/460 [==============================] - 39s 64ms/step - loss: 0.3863 - binary_accuracy: 0.8420 - val_loss: 0.1897 - val_binary_accuracy: 0.9246\n",
            "Epoch 2/7\n",
            "460/460 [==============================] - 30s 66ms/step - loss: 0.1650 - binary_accuracy: 0.9419 - val_loss: 0.1518 - val_binary_accuracy: 0.9650\n",
            "Epoch 3/7\n",
            "460/460 [==============================] - 30s 65ms/step - loss: 0.1171 - binary_accuracy: 0.9650 - val_loss: 0.1207 - val_binary_accuracy: 0.9710\n",
            "Epoch 4/7\n",
            "460/460 [==============================] - 29s 64ms/step - loss: 0.0939 - binary_accuracy: 0.9731 - val_loss: 0.1338 - val_binary_accuracy: 0.9683\n",
            "Epoch 5/7\n",
            "460/460 [==============================] - 30s 65ms/step - loss: 0.0707 - binary_accuracy: 0.9821 - val_loss: 0.1494 - val_binary_accuracy: 0.9699\n",
            "Epoch 6/7\n",
            "460/460 [==============================] - 30s 66ms/step - loss: 0.0563 - binary_accuracy: 0.9865 - val_loss: 0.1594 - val_binary_accuracy: 0.9694\n",
            "Epoch 7/7\n",
            "460/460 [==============================] - 30s 65ms/step - loss: 0.0533 - binary_accuracy: 0.9869 - val_loss: 0.1640 - val_binary_accuracy: 0.9694\n",
            "Epoch 1/7\n",
            "460/460 [==============================] - 38s 64ms/step - loss: 0.4719 - binary_accuracy: 0.7891 - val_loss: 0.2089 - val_binary_accuracy: 0.9121\n",
            "Epoch 2/7\n",
            "460/460 [==============================] - 30s 65ms/step - loss: 0.1660 - binary_accuracy: 0.9392 - val_loss: 0.1596 - val_binary_accuracy: 0.9634\n",
            "Epoch 3/7\n",
            "460/460 [==============================] - 30s 65ms/step - loss: 0.1107 - binary_accuracy: 0.9672 - val_loss: 0.1405 - val_binary_accuracy: 0.9694\n",
            "Epoch 4/7\n",
            "460/460 [==============================] - 30s 65ms/step - loss: 0.0914 - binary_accuracy: 0.9750 - val_loss: 0.1509 - val_binary_accuracy: 0.9705\n",
            "Epoch 5/7\n",
            "460/460 [==============================] - 30s 65ms/step - loss: 0.0734 - binary_accuracy: 0.9810 - val_loss: 0.1521 - val_binary_accuracy: 0.9667\n",
            "Epoch 6/7\n",
            "460/460 [==============================] - 30s 65ms/step - loss: 0.0551 - binary_accuracy: 0.9858 - val_loss: 0.1698 - val_binary_accuracy: 0.9683\n",
            "Epoch 7/7\n",
            "460/460 [==============================] - 30s 65ms/step - loss: 0.0602 - binary_accuracy: 0.9855 - val_loss: 0.1669 - val_binary_accuracy: 0.9656\n",
            "[[0.7840458750724792, 0.941811203956604, 0.9647589325904846, 0.9740472435951233, 0.9782816767692566, 0.9848381280899048, 0.987023651599884], [0.8380504846572876, 0.947821319103241, 0.9680371284484863, 0.976369321346283, 0.9801939725875854, 0.984428346157074, 0.9889359474182129], [0.8338979482650757, 0.9412648677825928, 0.9650321006774902, 0.9728178977966309, 0.9806037545204163, 0.9847015142440796, 0.9868870377540588], [0.8419845104217529, 0.9419478178024292, 0.9650321006774902, 0.9730911254882812, 0.9821062684059143, 0.986477255821228, 0.9868870377540588], [0.7890710234642029, 0.9392076730728149, 0.9672130942344666, 0.9750000238418579, 0.981010913848877, 0.985792338848114, 0.9855191111564636]]\n",
            "[[0.9355190992355347, 0.9661202430725098, 0.9737704992294312, 0.9737704992294312, 0.9743169546127319, 0.9721311330795288, 0.9759562611579895], [0.9284152984619141, 0.9622950553894043, 0.9666666388511658, 0.9524590373039246, 0.9704918265342712, 0.971584677696228, 0.9726775884628296], [0.9202185869216919, 0.9650273323059082, 0.9726775884628296, 0.9732240438461304, 0.9726775884628296, 0.9699453711509705, 0.9759562611579895], [0.9245901703834534, 0.9650273323059082, 0.9710382223129272, 0.9683060050010681, 0.9699453711509705, 0.9693989157676697, 0.9693989157676697], [0.9120699167251587, 0.9634079933166504, 0.9694156050682068, 0.9705079197883606, 0.966684877872467, 0.9683233499526978, 0.9655925631523132]]\n",
            "[[0.43846553564071655, 0.16777005791664124, 0.12362552434206009, 0.0956774577498436, 0.08361376076936722, 0.06459754705429077, 0.05266591161489487], [0.39518508315086365, 0.1563798189163208, 0.11308637261390686, 0.08723373711109161, 0.07685893028974533, 0.06055923178792, 0.04676958546042442], [0.40459874272346497, 0.1721719652414322, 0.11798731237649918, 0.09409390389919281, 0.07508935034275055, 0.06244484707713127, 0.05707934871315956], [0.3862886130809784, 0.1650175303220749, 0.11711331456899643, 0.09388229250907898, 0.07074151933193207, 0.05632396042346954, 0.053271565586328506], [0.4719412624835968, 0.16604754328727722, 0.11071489751338959, 0.09137885272502899, 0.07341453433036804, 0.055146582424640656, 0.06020819768309593]]\n",
            "[[0.18294720351696014, 0.1292276680469513, 0.11983004957437515, 0.10417504608631134, 0.14204877614974976, 0.14760416746139526, 0.13592693209648132], [0.19645966589450836, 0.1492123007774353, 0.15050946176052094, 0.2621304988861084, 0.1515202820301056, 0.15772423148155212, 0.15079231560230255], [0.201434925198555, 0.12803737819194794, 0.1168731153011322, 0.129068523645401, 0.1398095190525055, 0.1853862702846527, 0.14067664742469788], [0.1897285282611847, 0.15177683532238007, 0.12073579430580139, 0.13377216458320618, 0.14938363432884216, 0.15935981273651123, 0.16400128602981567], [0.20885369181632996, 0.1596095710992813, 0.1405012607574463, 0.150895357131958, 0.15210936963558197, 0.16983549296855927, 0.1668516993522644]]\n"
          ]
        }
      ],
      "source": [
        "acc_fold = []\n",
        "val_acc_fold = []\n",
        "loss_fold = []\n",
        "val_loss_fold = []\n",
        "\n",
        "init_lr = 0.001\n",
        "epochs = 25\n",
        "\n",
        "steps_per_epoch = train_val_set_size - split_size\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "\n",
        "# fold 1\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model_cv1 = build_classifier_model(0)\n",
        "classifier_model_cv1.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv1 = classifier_model_cv1.fit(x=cv_2.concatenate(cv_3).concatenate(cv_4).concatenate(cv_5),\n",
        "                               validation_data=cv_1,\n",
        "                               epochs=7)\n",
        "history_dict_cv1 = history_cv1.history\n",
        "\n",
        "acc_fold.append(history_dict_cv1['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv1['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv1['loss'])\n",
        "val_loss_fold.append(history_dict_cv1['val_loss'])\n",
        "\n",
        "# fold 2\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model_cv2 = build_classifier_model(0)\n",
        "classifier_model_cv2.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv2 = classifier_model_cv2.fit(x=cv_1.concatenate(cv_3).concatenate(cv_4).concatenate(cv_5),\n",
        "                               validation_data=cv_2,\n",
        "                               epochs=7)\n",
        "history_dict_cv2 = history_cv2.history\n",
        "\n",
        "acc_fold.append(history_dict_cv2['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv2['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv2['loss'])\n",
        "val_loss_fold.append(history_dict_cv2['val_loss'])\n",
        "\n",
        "#fold 3\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model_cv3 = build_classifier_model(0)\n",
        "classifier_model_cv3.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv3 = classifier_model_cv3.fit(x=cv_2.concatenate(cv_1).concatenate(cv_4).concatenate(cv_5),\n",
        "                               validation_data=cv_3,\n",
        "                               epochs=7)\n",
        "history_dict_cv3 = history_cv3.history\n",
        "\n",
        "acc_fold.append(history_dict_cv3['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv3['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv3['loss'])\n",
        "val_loss_fold.append(history_dict_cv3['val_loss'])\n",
        "\n",
        "# fold 4\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model_cv4 = build_classifier_model(0)\n",
        "classifier_model_cv4.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv4 = classifier_model_cv4.fit(x=cv_2.concatenate(cv_3).concatenate(cv_1).concatenate(cv_5),\n",
        "                               validation_data=cv_4,\n",
        "                               epochs=7)\n",
        "history_dict_cv4 = history_cv4.history\n",
        "\n",
        "acc_fold.append(history_dict_cv4['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv4['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv4['loss'])\n",
        "val_loss_fold.append(history_dict_cv4['val_loss'])\n",
        "\n",
        "#fold 5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model_cv5 = build_classifier_model(0)\n",
        "classifier_model_cv5.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv5 = classifier_model_cv5.fit(x=cv_2.concatenate(cv_3).concatenate(cv_4).concatenate(cv_1),\n",
        "                               validation_data=cv_5,\n",
        "                               epochs=7)\n",
        "history_dict_cv5 = history_cv5.history\n",
        "\n",
        "acc_fold.append(history_dict_cv5['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv5['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv5['loss'])\n",
        "val_loss_fold.append(history_dict_cv5['val_loss'])\n",
        "\n",
        "# print\n",
        "print(acc_fold)\n",
        "print(val_acc_fold)\n",
        "print(loss_fold)\n",
        "print(val_loss_fold)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GVVHV-ki74G1"
      },
      "source": [
        "# Best final model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqlSQuPl_Os8",
        "outputId": "5ae66285-5384-4782-c58d-3b3131ad8fd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "458/458 [==============================] - 39s 66ms/step - loss: 0.3965 - binary_accuracy: 0.8378 - val_loss: 0.1721 - val_binary_accuracy: 0.9366\n",
            "Epoch 2/7\n",
            "458/458 [==============================] - 31s 68ms/step - loss: 0.1663 - binary_accuracy: 0.9406 - val_loss: 0.1181 - val_binary_accuracy: 0.9656\n",
            "Epoch 3/7\n",
            "458/458 [==============================] - 31s 68ms/step - loss: 0.1149 - binary_accuracy: 0.9653 - val_loss: 0.1293 - val_binary_accuracy: 0.9716\n",
            "Epoch 4/7\n",
            "458/458 [==============================] - 31s 68ms/step - loss: 0.0975 - binary_accuracy: 0.9739 - val_loss: 0.1148 - val_binary_accuracy: 0.9732\n",
            "Epoch 5/7\n",
            "458/458 [==============================] - 32s 69ms/step - loss: 0.0834 - binary_accuracy: 0.9784 - val_loss: 0.1393 - val_binary_accuracy: 0.9716\n",
            "Epoch 6/7\n",
            "458/458 [==============================] - 31s 68ms/step - loss: 0.0684 - binary_accuracy: 0.9821 - val_loss: 0.1381 - val_binary_accuracy: 0.9705\n",
            "Epoch 7/7\n",
            "458/458 [==============================] - 31s 68ms/step - loss: 0.0540 - binary_accuracy: 0.9873 - val_loss: 0.1253 - val_binary_accuracy: 0.9765\n"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(12345)\n",
        "\n",
        "epochs = 25\n",
        "\n",
        "batch_size = 16\n",
        "init_lr = 0.001\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "# training and validation dataset\n",
        "train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32], select_cols=[2,3])\n",
        "\n",
        "train_val_data.shuffle(buffer_size=len(list(train_val_data)))\n",
        "train_val_set_size = len(list(train_val_data))\n",
        "val_n = int(0.2*train_val_set_size)\n",
        "\n",
        "train_data = train_val_data.skip(val_n).batch(batch_size)\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_data = train_val_data.take(val_n).batch(batch_size)\n",
        "val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "steps_per_epoch = train_val_set_size - val_n\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "final_classifier = build_classifier_model(0)\n",
        "final_classifier.compile(optimizer=optimizer,\n",
        "                        loss=loss,\n",
        "                        metrics=metrics)\n",
        "history = final_classifier.fit(x=train_data, validation_data=val_data, epochs=7)\n",
        "\n",
        "final_classifier.save_weights('final_classifier8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "MO72elCYKpPk",
        "outputId": "87c2afed-27d7-41ef-de91-6766b5ec5b10"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_loss = [0.3965, 0.1663, 0.1149, 0.0975, 0.0834, 0.0684, 0.0540]\n",
        "validation_loss = [0.1721, 0.1181, 0.1293, 0.1148, 0.1393, 0.1381, 0.1253]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1,8), train_loss, label='Train')\n",
        "plt.plot(range(1,8), validation_loss , label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig('Loss_graph')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIYliz409RWG",
        "outputId": "cb79f137-945a-4f34-9a25-4e47b6dfacf3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc37de96a90>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = build_classifier_model(0)\n",
        "model.load_weights('final_classifier8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEDHOuT68PjN",
        "outputId": "8ef19264-f8ec-4f48-f553-2439fa54d038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "143/143 [==============================] - 5s 34ms/step - loss: 0.1724 - binary_accuracy: 0.9733\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.1724161058664322, 0.9732879996299744]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data = tf.data.experimental.CsvDataset([\"test_bert\"], [tf.string,tf.int32] ,select_cols=[2,3])\n",
        "test_data = test_data.batch(batch_size)\n",
        "test_data = test_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "model.evaluate(test_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zcYpepdc9cEe"
      },
      "source": [
        "on test set we get an accuracy of 0.9733"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LiMdhcd-8xkk"
      },
      "source": [
        "report accuracies in table on overleaf, save the model and test on the test set, do the test on the dataset of tweets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xA2ua1HXJ7ky"
      },
      "source": [
        "## Final cross validation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1I9rAJlCJ_Z6"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "# training and validation dataset\n",
        "train_val_data = tf.data.experimental.CsvDataset([\"train_bert\"], [tf.string,tf.int32], select_cols=[2,3])\n",
        "\n",
        "train_val_data.shuffle(buffer_size=len(list(train_val_data)), seed = 12345)\n",
        "\n",
        "train_val_set_size = len(list(train_val_data))\n",
        "\n",
        "split_size = int(0.2*train_val_set_size)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "cv_1 = train_val_data.take(split_size).batch(batch_size)\n",
        "cv_1 = cv_1.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "m1 = train_val_data.skip(split_size)\n",
        "cv_2 = m1.take(split_size).batch(batch_size)\n",
        "cv_2 = cv_2.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "m2 = m1.skip(split_size)\n",
        "cv_3 = m2.take(split_size).batch(batch_size)\n",
        "cv_3 = cv_3.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "m3 = m2.skip(split_size)\n",
        "cv_4 = m3.take(split_size).batch(batch_size)\n",
        "cv_4 = cv_4.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "cv_5 = m3.skip(split_size).batch(batch_size)\n",
        "cv_5 = cv_5.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pBmaC-mKIkm",
        "outputId": "afb00ebc-f309-4952-9e9d-04d131e91cd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "232/232 [==============================] - 44s 139ms/step - loss: 0.5045 - binary_accuracy: 0.7518 - val_loss: 0.2938 - val_binary_accuracy: 0.8721\n",
            "Epoch 2/7\n",
            "232/232 [==============================] - 30s 130ms/step - loss: 0.2439 - binary_accuracy: 0.9002 - val_loss: 0.1527 - val_binary_accuracy: 0.9497\n",
            "Epoch 3/7\n",
            "232/232 [==============================] - 32s 138ms/step - loss: 0.1458 - binary_accuracy: 0.9471 - val_loss: 0.1106 - val_binary_accuracy: 0.9656\n",
            "Epoch 4/7\n",
            "232/232 [==============================] - 30s 130ms/step - loss: 0.1032 - binary_accuracy: 0.9679 - val_loss: 0.1149 - val_binary_accuracy: 0.9694\n",
            "Epoch 5/7\n",
            "232/232 [==============================] - 32s 137ms/step - loss: 0.0903 - binary_accuracy: 0.9728 - val_loss: 0.1109 - val_binary_accuracy: 0.9716\n",
            "Epoch 6/7\n",
            "232/232 [==============================] - 30s 130ms/step - loss: 0.0714 - binary_accuracy: 0.9779 - val_loss: 0.1230 - val_binary_accuracy: 0.9678\n",
            "Epoch 7/7\n",
            "232/232 [==============================] - 32s 138ms/step - loss: 0.0551 - binary_accuracy: 0.9839 - val_loss: 0.1253 - val_binary_accuracy: 0.9699\n",
            "Epoch 1/7\n",
            "232/232 [==============================] - 44s 157ms/step - loss: 0.5783 - binary_accuracy: 0.7479 - val_loss: 0.3407 - val_binary_accuracy: 0.8432\n",
            "Epoch 2/7\n",
            "232/232 [==============================] - 30s 130ms/step - loss: 0.2518 - binary_accuracy: 0.8956 - val_loss: 0.1809 - val_binary_accuracy: 0.9339\n",
            "Epoch 3/7\n",
            "232/232 [==============================] - 32s 138ms/step - loss: 0.1378 - binary_accuracy: 0.9519 - val_loss: 0.1291 - val_binary_accuracy: 0.9612\n",
            "Epoch 4/7\n",
            "232/232 [==============================] - 31s 132ms/step - loss: 0.0981 - binary_accuracy: 0.9701 - val_loss: 0.1278 - val_binary_accuracy: 0.9667\n",
            "Epoch 5/7\n",
            "232/232 [==============================] - 34s 145ms/step - loss: 0.0828 - binary_accuracy: 0.9758 - val_loss: 0.1348 - val_binary_accuracy: 0.9678\n",
            "Epoch 6/7\n",
            "232/232 [==============================] - 30s 130ms/step - loss: 0.0697 - binary_accuracy: 0.9794 - val_loss: 0.1505 - val_binary_accuracy: 0.9667\n",
            "Epoch 7/7\n",
            "232/232 [==============================] - 32s 138ms/step - loss: 0.0576 - binary_accuracy: 0.9833 - val_loss: 0.1635 - val_binary_accuracy: 0.9639\n",
            "Epoch 1/7\n",
            "232/232 [==============================] - 42s 139ms/step - loss: 0.5256 - binary_accuracy: 0.7663 - val_loss: 0.3234 - val_binary_accuracy: 0.8437\n",
            "Epoch 2/7\n",
            "232/232 [==============================] - 30s 131ms/step - loss: 0.2567 - binary_accuracy: 0.8920 - val_loss: 0.1587 - val_binary_accuracy: 0.9404\n",
            "Epoch 3/7\n",
            "232/232 [==============================] - 32s 139ms/step - loss: 0.1465 - binary_accuracy: 0.9473 - val_loss: 0.1080 - val_binary_accuracy: 0.9634\n",
            "Epoch 4/7\n",
            "232/232 [==============================] - 30s 131ms/step - loss: 0.0999 - binary_accuracy: 0.9660 - val_loss: 0.0914 - val_binary_accuracy: 0.9699\n",
            "Epoch 5/7\n",
            "232/232 [==============================] - 33s 141ms/step - loss: 0.0798 - binary_accuracy: 0.9762 - val_loss: 0.0962 - val_binary_accuracy: 0.9770\n",
            "Epoch 6/7\n",
            "232/232 [==============================] - 31s 132ms/step - loss: 0.0650 - binary_accuracy: 0.9805 - val_loss: 0.1142 - val_binary_accuracy: 0.9727\n",
            "Epoch 7/7\n",
            "232/232 [==============================] - 33s 141ms/step - loss: 0.0527 - binary_accuracy: 0.9852 - val_loss: 0.1322 - val_binary_accuracy: 0.9727\n",
            "Epoch 1/7\n",
            "232/232 [==============================] - 41s 137ms/step - loss: 0.8571 - binary_accuracy: 0.6504 - val_loss: 0.3140 - val_binary_accuracy: 0.8557\n",
            "Epoch 2/7\n",
            "232/232 [==============================] - 30s 129ms/step - loss: 0.2489 - binary_accuracy: 0.8940 - val_loss: 0.1668 - val_binary_accuracy: 0.9333\n",
            "Epoch 3/7\n",
            "232/232 [==============================] - 32s 137ms/step - loss: 0.1406 - binary_accuracy: 0.9501 - val_loss: 0.1104 - val_binary_accuracy: 0.9612\n",
            "Epoch 4/7\n",
            "232/232 [==============================] - 30s 129ms/step - loss: 0.1037 - binary_accuracy: 0.9648 - val_loss: 0.1086 - val_binary_accuracy: 0.9699\n",
            "Epoch 5/7\n",
            "232/232 [==============================] - 32s 138ms/step - loss: 0.0850 - binary_accuracy: 0.9723 - val_loss: 0.1272 - val_binary_accuracy: 0.9694\n",
            "Epoch 6/7\n",
            "232/232 [==============================] - 30s 130ms/step - loss: 0.0732 - binary_accuracy: 0.9790 - val_loss: 0.1564 - val_binary_accuracy: 0.9645\n",
            "Epoch 7/7\n",
            "232/232 [==============================] - 32s 137ms/step - loss: 0.0607 - binary_accuracy: 0.9825 - val_loss: 0.1366 - val_binary_accuracy: 0.9694\n",
            "Epoch 1/7\n",
            "232/232 [==============================] - 45s 162ms/step - loss: 0.5445 - binary_accuracy: 0.7587 - val_loss: 0.3410 - val_binary_accuracy: 0.8433\n",
            "Epoch 2/7\n",
            "232/232 [==============================] - 31s 132ms/step - loss: 0.2608 - binary_accuracy: 0.8896 - val_loss: 0.1551 - val_binary_accuracy: 0.9427\n",
            "Epoch 3/7\n",
            "232/232 [==============================] - 32s 138ms/step - loss: 0.1472 - binary_accuracy: 0.9482 - val_loss: 0.1222 - val_binary_accuracy: 0.9618\n",
            "Epoch 4/7\n",
            "232/232 [==============================] - 30s 131ms/step - loss: 0.1008 - binary_accuracy: 0.9679 - val_loss: 0.1330 - val_binary_accuracy: 0.9667\n",
            "Epoch 5/7\n",
            "232/232 [==============================] - 32s 139ms/step - loss: 0.0913 - binary_accuracy: 0.9725 - val_loss: 0.1145 - val_binary_accuracy: 0.9667\n",
            "Epoch 6/7\n",
            "232/232 [==============================] - 30s 129ms/step - loss: 0.0731 - binary_accuracy: 0.9792 - val_loss: 0.1247 - val_binary_accuracy: 0.9667\n",
            "Epoch 7/7\n",
            "232/232 [==============================] - 32s 138ms/step - loss: 0.0652 - binary_accuracy: 0.9810 - val_loss: 0.1433 - val_binary_accuracy: 0.9721\n",
            "[[0.75180983543396, 0.9001502394676208, 0.9471383690834045, 0.9679005742073059, 0.9728178977966309, 0.9778718948364258, 0.9838820099830627], [0.7478964328765869, 0.8956426978111267, 0.9519191384315491, 0.9700860381126404, 0.9758229851722717, 0.9793744087219238, 0.9833356142044067], [0.7662550806999207, 0.8919546604156494, 0.9472749829292297, 0.965988278388977, 0.9762327671051025, 0.9804671406745911, 0.9852479100227356], [0.6504207253456116, 0.8940035700798035, 0.9501433968544006, 0.9647589325904846, 0.9722715616226196, 0.978964626789093, 0.9825160503387451], [0.7586885094642639, 0.8896175026893616, 0.9482240676879883, 0.9678961634635925, 0.9725409746170044, 0.9792349934577942, 0.981010913848877]]\n",
            "[[0.8721311688423157, 0.9497267603874207, 0.965573787689209, 0.9693989157676697, 0.971584677696228, 0.9677595496177673, 0.9699453711509705], [0.8431693911552429, 0.9338797926902771, 0.9612022042274475, 0.9666666388511658, 0.9677595496177673, 0.9666666388511658, 0.9639344215393066], [0.8437158465385437, 0.9404371380805969, 0.9633879661560059, 0.9699453711509705, 0.9770491719245911, 0.9726775884628296, 0.9726775884628296], [0.8557376861572266, 0.9333333373069763, 0.9612022042274475, 0.9699453711509705, 0.9693989157676697, 0.9644808769226074, 0.9693989157676697], [0.8432550430297852, 0.9426543116569519, 0.9617695212364197, 0.966684877872467, 0.966684877872467, 0.966684877872467, 0.9721463918685913]]\n",
            "[[0.5045482516288757, 0.24393735826015472, 0.1458161622285843, 0.10317707806825638, 0.09027868509292603, 0.07143948972225189, 0.0551140122115612], [0.5783088207244873, 0.251831978559494, 0.13779833912849426, 0.09807410836219788, 0.0828380286693573, 0.06974262744188309, 0.05763969570398331], [0.5255506634712219, 0.25670793652534485, 0.1465226709842682, 0.09990590065717697, 0.0798109695315361, 0.06495305150747299, 0.05271480232477188], [0.8571326732635498, 0.24894338846206665, 0.14060184359550476, 0.10369645804166794, 0.08497369289398193, 0.07315859198570251, 0.06066649407148361], [0.5445026159286499, 0.260765016078949, 0.14721381664276123, 0.1008186936378479, 0.09133889526128769, 0.07311467826366425, 0.06524419039487839]]\n",
            "[[0.2938476800918579, 0.1526537984609604, 0.11061881482601166, 0.11494579166173935, 0.11093844473361969, 0.12303280085325241, 0.1252727210521698], [0.3407146632671356, 0.18087367713451385, 0.12906382977962494, 0.12784698605537415, 0.1348402053117752, 0.15051786601543427, 0.16345956921577454], [0.32336321473121643, 0.1587408185005188, 0.10803878307342529, 0.09138096868991852, 0.09618320316076279, 0.11416563391685486, 0.13224101066589355], [0.31400126218795776, 0.16683277487754822, 0.1104033812880516, 0.10859271138906479, 0.12719279527664185, 0.15635845065116882, 0.13657905161380768], [0.34104296565055847, 0.1551247388124466, 0.12215891480445862, 0.13302165269851685, 0.1144970953464508, 0.12466462701559067, 0.14330214262008667]]\n"
          ]
        }
      ],
      "source": [
        "acc_fold = []\n",
        "val_acc_fold = []\n",
        "loss_fold = []\n",
        "val_loss_fold = []\n",
        "\n",
        "init_lr = 0.001\n",
        "epochs = 25\n",
        "\n",
        "steps_per_epoch = train_val_set_size - split_size\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "\n",
        "# fold 1\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model_cv1 = build_classifier_model(0)\n",
        "classifier_model_cv1.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv1 = classifier_model_cv1.fit(x=cv_2.concatenate(cv_3).concatenate(cv_4).concatenate(cv_5),\n",
        "                               validation_data=cv_1,\n",
        "                               epochs=7)\n",
        "history_dict_cv1 = history_cv1.history\n",
        "\n",
        "acc_fold.append(history_dict_cv1['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv1['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv1['loss'])\n",
        "val_loss_fold.append(history_dict_cv1['val_loss'])\n",
        "\n",
        "# fold 2\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model_cv2 = build_classifier_model(0)\n",
        "classifier_model_cv2.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv2 = classifier_model_cv2.fit(x=cv_1.concatenate(cv_3).concatenate(cv_4).concatenate(cv_5),\n",
        "                               validation_data=cv_2,\n",
        "                               epochs=7)\n",
        "history_dict_cv2 = history_cv2.history\n",
        "\n",
        "acc_fold.append(history_dict_cv2['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv2['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv2['loss'])\n",
        "val_loss_fold.append(history_dict_cv2['val_loss'])\n",
        "\n",
        "#fold 3\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model_cv3 = build_classifier_model(0)\n",
        "classifier_model_cv3.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv3 = classifier_model_cv3.fit(x=cv_2.concatenate(cv_1).concatenate(cv_4).concatenate(cv_5),\n",
        "                               validation_data=cv_3,\n",
        "                               epochs=7)\n",
        "history_dict_cv3 = history_cv3.history\n",
        "\n",
        "acc_fold.append(history_dict_cv3['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv3['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv3['loss'])\n",
        "val_loss_fold.append(history_dict_cv3['val_loss'])\n",
        "\n",
        "# fold 4\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model_cv4 = build_classifier_model(0)\n",
        "classifier_model_cv4.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv4 = classifier_model_cv4.fit(x=cv_2.concatenate(cv_3).concatenate(cv_1).concatenate(cv_5),\n",
        "                               validation_data=cv_4,\n",
        "                               epochs=7)\n",
        "history_dict_cv4 = history_cv4.history\n",
        "\n",
        "acc_fold.append(history_dict_cv4['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv4['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv4['loss'])\n",
        "val_loss_fold.append(history_dict_cv4['val_loss'])\n",
        "\n",
        "#fold 5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model_cv5 = build_classifier_model(0)\n",
        "classifier_model_cv5.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history_cv5 = classifier_model_cv5.fit(x=cv_2.concatenate(cv_3).concatenate(cv_4).concatenate(cv_1),\n",
        "                               validation_data=cv_5,\n",
        "                               epochs=7)\n",
        "history_dict_cv5 = history_cv5.history\n",
        "\n",
        "acc_fold.append(history_dict_cv5['binary_accuracy'])\n",
        "val_acc_fold.append(history_dict_cv5['val_binary_accuracy'])\n",
        "loss_fold.append(history_dict_cv5['loss'])\n",
        "val_loss_fold.append(history_dict_cv5['val_loss'])\n",
        "\n",
        "# print\n",
        "print(acc_fold)\n",
        "print(val_acc_fold)\n",
        "print(loss_fold)\n",
        "print(val_loss_fold)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jUqmSrdNBQeG"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yw8TWh6y9h4b",
        "outputId": "a5a29abe-af7f-439f-b7de-53f4b324be0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[1.0706730e-03]\n",
            " [1.6453305e-04]\n",
            " [9.9368566e-01]\n",
            " [6.4237352e-04]], shape=(4, 1), dtype=float32)\n",
            "0.25\n"
          ]
        }
      ],
      "source": [
        "examples = ['For on-the-field cardiac arrests, this \"Courageous Discourse\" gives a key interpretative figure for those who have taken one of the mass mandated products at any time in the past.', 'If you were wise enough and strong enough not to take it, you have to read this piece!', 'Dr. Masanori Fukushima, a prof emeritus at Kyoto University, /25/22, raised concerns involving discrimination against the unvaccinated in Japan as well as an ever-growing list of persons reporting injuries associated with the COVID-19 mRNA vaccines.', 'Exclusive interview podcast with Ms. Jennifer Sharp who produced \"Anecdotals\" Must see documentary for those who had no side effects--gives them a view of the Russian Roulette faced on the next booster.']\n",
        "\n",
        "predictions = tf.sigmoid(model(tf.constant(examples)))\n",
        "\n",
        "print(predictions)\n",
        "\n",
        "info = []\n",
        "\n",
        "# 1 means misinformation and 0 means information\n",
        "\n",
        "for i in predictions.numpy():\n",
        "  if i[0] > 0.5:\n",
        "    info.append(1)\n",
        "  else:\n",
        "    info.append(0)\n",
        "\n",
        "print(sum(info)/len(info))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gMJ5cLUhRLdK"
      },
      "source": [
        "create dataframe with tweets labelled as inf and misinf\n",
        "\n",
        "1 means misinformation and 1 means information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhjCZN4jSfGN",
        "outputId": "3cac423d-f117-42fc-a464-ea99a396c4ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7ff54b1e7f70>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = build_classifier_model(0)\n",
        "model.load_weights('final_classifier8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "4vUIbvRDiUvv",
        "outputId": "f9da194d-3e0b-45d2-a3f4-4b5ae40e0a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "90000\n",
            "100000\n",
            "110000\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_6b5babc4-b34a-451a-8309-b78556d5f571\", \"cleaned_user_text_data_misinf.csv\", 39179114)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "df = pd.read_csv('cleaned_user_text_data.csv')\n",
        "df['Misinformation'] = np.nan\n",
        "\n",
        "for index, tweets in enumerate(df['Cleaned_Tweets']):\n",
        "  \n",
        "  if index % 10000 == 0:\n",
        "    print(index)\n",
        "\n",
        "  tweets = ast.literal_eval(tweets)\n",
        "  predictions = tf.sigmoid(model(tf.constant(tweets)))\n",
        "\n",
        "  info = []\n",
        "  for i in predictions.numpy():\n",
        "    if i[0] > 0.5:\n",
        "      info.append(1)\n",
        "    else:\n",
        "      info.append(0)\n",
        "\n",
        "  df.at[index, 'Misinformation'] = sum(info)/len(info)\n",
        "\n",
        "df.to_csv('cleaned_user_text_data_misinf.csv')\n",
        "\n",
        "files.download('cleaned_user_text_data_misinf.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CGJI3xO7SOmT"
      },
      "source": [
        "could look at how likely you are to be retweeted if you spread misinformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uJLKuyJSUOR",
        "outputId": "6b699c29-e21b-438e-836f-d8bd732cac19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.28282141947858325\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_misinf = pd.read_csv('cleaned_user_text_data_misinf.csv', usecols=['User_ID', 'Cleaned_Tweets', 'Misinformation'])\n",
        "\n",
        "print(df_misinf['Misinformation'].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "C1zQ-NJLLB8D",
        "outputId": "60be4557-5838-4765-eebc-dc283e0eb91d"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1a4f7b8a1b6c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cleaned_user_text_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'User_ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cleaned_Tweets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cleaned_user_text_data.csv'"
          ]
        }
      ],
      "source": [
        "df_cleaned = pd.read_csv('cleaned_user_text_data.csv', usecols=['User_ID', 'Cleaned_Tweets'])\n",
        "print(df_cleaned.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCZjWjPqEdSS"
      },
      "outputs": [],
      "source": [
        "pagerank_users = [1478792963090169860, 472777204, 120526471, 707231479047315456, 705902960967094272, 1465347964252180493]\n",
        "names = ['healthbyjames', 'DrAseemMalhotra', 'Hey_Brian', 'PeterSweden7', 'EricSpracklen', 'P_McCulloughMD']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9wULdCeC7y_"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "def misinf_quotient_users(user_ids):\n",
        "  for user, name in zip(user_ids, names):\n",
        "    index = np.where(df_misinf['User_ID']==user)[0][0]\n",
        "    list_tweets = ast.literal_eval(df_misinf['Cleaned_Tweets'][index])\n",
        "    print(f\"User is {name}, Misinformation quotient is {df_misinf['Misinformation'][index]}\")\n",
        "    #print(df_misinf['Misinformation'][index])\n",
        "    #print(len(list_tweets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6DLgrh2FVkA",
        "outputId": "e0b4138b-3485-43aa-c10f-84f810cfb2bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User is healthbyjames, Misinformation quotient is 0.3574468085106383\n",
            "User is DrAseemMalhotra, Misinformation quotient is 0.7333333333333333\n",
            "User is Hey_Brian, Misinformation quotient is 0.5\n",
            "User is PeterSweden7, Misinformation quotient is 0.3703703703703703\n",
            "User is EricSpracklen, Misinformation quotient is 0.2666666666666666\n",
            "User is P_McCulloughMD, Misinformation quotient is 0.3846153846153846\n"
          ]
        }
      ],
      "source": [
        "misinf_quotient_users(pagerank_users)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vaDyi4jwFSfC"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
